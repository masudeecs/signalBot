## Chapter Time-Domain Analysis

of Continuous-Time Systems

In this book we consider two methods of analysis of linear time-invariant (LTI) systems: the time-domain method and the frequency-domain method. In this chapter we discuss the _time-domain analysis_ of linear, time-invariant, continuous-time (LTIC) systems.

### 11 Introduction

For the purpose of analysis, we shall consider _linear differential systems_. This is the class of LTIC systems introduced in Ch. 1, for which the input \(x(t)\) and the output \(y(t)\) are related by linear differential equations of the form

\[\frac{d^{N}y(t)}{dt^{N}}+a_{1}\frac{d^{N-1}y(t)}{dt^{N-1}}+\cdot \cdot\cdot+a_{N-1}\frac{dy(t)}{dt}+a_{N}y(t)\] \[=b_{N-M}\frac{d^{M}x(t)}{dt^{M}}+b_{N-M+1}\frac{d^{M-1}x(t)}{dt^{M- 1}}+\cdot\cdot\cdot+b_{N-1}\frac{dx(t)}{dt}+b_{N}x(t) \tag{1}\]

where all the coefficients \(a_{i}\) and \(b_{i}\) are constants. Using operator notation \(D\) to represent \(d/dt\), we can express this equation as

\[(D^{N}+a_{1}D^{N-1}+\cdot\cdot\cdot+a_{N-1}D+a_{N})y(t)\] \[=(b_{N-M}D^{M}+b_{N-M+1}D^{M-1}+\cdot\cdot\cdot+b_{N-1}D+b_{N})\,x(t)\]

or

\[Q(D)y(t)=P(D)x(t) \tag{2}\]

where the polynomials \(Q(D)\) and \(P(D)\) are

\[Q(D) =D^{N}+a_{1}D^{N-1}+\cdot\cdot\cdot+a_{N-1}D+a_{N}\] \[P(D) =b_{N-M}D^{M}+b_{N-M+1}D^{M-1}+\cdot\cdot\cdot+b_{N-1}D+b_{N}\]

Theoretically the powers \(M\) and \(N\) in the foregoing equations can take on any value. However, practical considerations make \(M>N\) undesirable for two reasons. In Sec. 11-11, we shall show thatan LTIC system specified by Eq. (2.1) acts as an \((M-N)\)th-order differentiator. A differentiator represents an unstable system because a bounded input like the step input results in an unbounded output, \(\delta(t)\). Second, noise is enhanced by a differentiator. Noise is a wideband signal containing components of all frequencies from 0 to a very high frequency approaching \(\infty\).2 Hence, noise contains a significant amount of rapidly varying components. We know that the derivative of any rapidly varying signal is high. Therefore, any system specified by Eq. (2.1) in which \(M>N\) will magnify the high-frequency components of noise through differentiation. It is entirely possible for noise to be magnified so much that it swamps the desired system output even if the noise signal at the system's input is tolerably small. Hence, practical systems generally use \(M\leq N\). For the rest of this text we assume implicitly that \(M\leq N\). For the sake of generality, we shall assume \(M=N\) in Eq. (2.1).

Footnote 2: Notice is any undesirable signal, natural or manufactured, that interferes with the desired signals in the system. Some of the sources of noise are the electromagnetic radiation from stars, the random motion of electrons in system components, interference from nearby radio and television stations, transients produced by automobile ignition systems, and fluorescent lighting.

In Ch. 1, we demonstrated that a system described by Eq. (2.2) is linear. Therefore, its response can be expressed as the sum of two components: the zero-input response and the zero-state response (decomposition property).3 Therefore,

Footnote 3: We can verify readily that the system described by Eq. (2.2) has the decomposition property. If \(y_{0}(t)\) is the zero-input response, then, by definition,

\[Q(D)y_{0}(t)=0\]

 If \(y(t)\) is the zero-state response, then \(y(t)\) is the solution of

\[Q(D)y(t)=P(D)x(t)\]

 subject to zero initial conditions (zero-state). Adding these two equations, we have

\[Q(D)[y_{0}(t)+y(t)]=P(D)x(t)\]

Clearly, \(y_{0}(t)+y(t)\) is the general solution of Eq. (2.2).
or

\[(D^{N}+a_{1}D^{N-1}+\cdot\cdot\cdot+a_{N-1}D+a_{N})y_{0}(t)=0 \tag{2.3}\]

A solution to this equation can be obtained systematically [1]. However, we will take a shortcut by using heuristic reasoning. Equation (2.3) shows that a linear combination of \(y_{0}(t)\) and its \(N\) successive derivatives is zero, not at _some_ values of \(t\), but for all \(t\). Such a result is possible _if and only if \(y_{0}(t)\)_ and all its \(N\) successive derivatives are of the same form. Otherwise their sum can never add to zero for all values of \(t\). We know that only an exponential function \(e^{\lambda t}\) has this property. So let us assume that

\[y_{0}(t)=ce^{\lambda t}\]

is a solution to Eq. (2.3). Then

\[Dy_{0}(t) =\frac{dy_{0}(t)}{dt}=c\lambda e^{\lambda t}\] \[D^{2}y_{0}(t) =\frac{d^{2}y_{0}(t)}{dt^{2}}=c\lambda^{2}e^{\lambda t}\] \[\vdots\] \[D^{N}y_{0}(t) =\frac{d^{N}y_{0}(t)}{dt^{N}}=c\lambda^{N}e^{\lambda t}\]

Substituting these results in Eq. (2.3), we obtain

\[c(\lambda^{N}+a_{1}\lambda^{N-1}+\cdot\cdot\cdot+a_{N-1}\lambda+a_{N})e^{ \lambda t}=0\]

For a nontrivial solution of this equation,

\[\lambda^{N}+a_{1}\lambda^{N-1}+\cdot\cdot\cdot+a_{N-1}\lambda+a_{N}=0 \tag{2.4}\]

This result means that \(ce^{\lambda t}\) is indeed a solution of Eq. (2.3), provided \(\lambda\) satisfies Eq. (2.4). Note that the polynomial in Eq. (2.4) is identical to the polynomial \(Q(D)\) in Eq. (2.3), with \(\lambda\) replacing \(D\). Therefore, Eq. (2.4) can be expressed as

\[Q(\lambda)=0\]

Expressing \(Q(\lambda)\) in factorized form, we obtain

\[Q(\lambda)=(\lambda-\lambda_{1})(\lambda-\lambda_{2})\cdot\cdot\cdot(\lambda- \lambda_{N})=0 \tag{2.5}\]

Clearly, \(\lambda\) has \(N\) solutions: \(\lambda_{1}\), \(\lambda_{2}\), \(\ldots\), \(\lambda_{N}\), assuming that all \(\lambda_{i}\) are distinct. Consequently, Eq. (2.3) has \(N\) possible solutions: \(c_{1}e^{\lambda_{1}t}\), \(c_{2}e^{\lambda_{2}t}\), \(\ldots\), \(c_{N}e^{\lambda_{N}t}\), with \(c_{1}\), \(c_{2}\),\(\ldots\),\(c_{N}\) as arbitrary constants. Wecan readily show that a general solution is given by the sum of these \(N\) solutions+ so that

Footnote †: \({}^{\ddagger}\)_Eigenvalue_ is German for “characteristic value.”

\[y_{0}(t)=c_{1}e^{\lambda_{1}t}+c_{2}e^{\lambda_{2}t}+\cdot\cdot\cdot+c_{N}e^{ \lambda_{N}t} \tag{6}\]

where \(c_{1}\), \(c_{2}\), \(\ldots\), \(c_{N}\) are arbitrary constants determined by \(N\) constraints (the auxiliary conditions) on the solution.

Observe that the polynomial \(Q(\lambda)\), which is characteristic of the system, has nothing to do with the input. For this reason the polynomial \(Q(\lambda)\) is called the _characteristic polynomial_ of the system. The equation

\[Q(\lambda)=0\]

is called the _characteristic equation_ of the system. Equation (5) clearly indicates that \(\lambda_{1}\), \(\lambda_{2}\), \(\cdot\cdot\cdot\), \(\lambda_{N}\) are the roots of the characteristic equation; consequently, they are called the _characteristic roots_ of the system. The terms _characteristic values, eigenvalues,_ and _natural frequencies_ are also used for characteristic roots.+ The exponentials \(e^{\lambda_{i}t}\) (\(i=1,2,\ldots,n\)) in the zero-input response are the _characteristic modes_ (also known as _natural modes_ or simply as _modes_) of the system. There is a characteristic mode for each characteristic root of the system, and the _zero-input response is a linear combination of the characteristic modes of the system_.

Footnote †: \({}^{\ddagger}\)_Eigenvalue_ is German for “characteristic value.”

An LTIC system's characteristic modes comprise its single most important attribute. Characteristic modes not only determine the zero-input response but also play an important role in determining the zero-state response. In other words, the entire behavior of a system is dictated primarily by its characteristic modes. In the rest of this chapter we shall see the pervasive presence of characteristic modes in every aspect of system behavior.

## Repeated Roots

The solution of Eq. (3) as given in Eq. (6) assumes that the \(N\) characteristic roots \(\lambda_{1}\), \(\lambda_{2}\), \(\ldots\), \(\lambda_{N}\) are distinct. If there are repeated roots (same root occurring more than once), the form of the solution is modified slightly. By direct substitution we can show that the solution of the equation

\[(D-\lambda)^{2}y_{0}(t)=0\]

is given by

\[y_{0}(t)=(c_{1}+c_{2}t)e^{\lambda_{1}t}\]In this case the root \(\lambda\) repeats twice. Observe that the characteristic modes in this case are \(e^{\lambda t}\) and \(te^{\lambda t}\). Continuing this pattern, we can show that for the differential equation

\[(D-\lambda)^{r}y_{0}(t)=0\]

the characteristic modes are \(e^{\lambda t}\), \(te^{\lambda t}\), \(t^{2}e^{\lambda t}\), \(\cdot\cdot\cdot\), \(t^{r-1}e^{\lambda t}\), and that the solution is

\[y_{0}(t)=(c_{1}+c_{2}t+\cdot\cdot\cdot+c_{r}t^{r-1})e^{\lambda t}\]

Consequently, for a system with the characteristic polynomial

\[Q(\lambda)=(\lambda-\lambda_{1})^{r}(\lambda-\lambda_{r+1})\cdot\cdot\cdot( \lambda-\lambda_{N})\]

the characteristic modes are \(e^{\lambda_{1}t}\), \(te^{\lambda_{1}t}\), \(\cdot\cdot\cdot\), \(t^{r-1}e^{\lambda_{1}t}\), \(e^{\lambda_{r+1}t}\), \(\cdot\cdot\cdot\), \(e^{\lambda_{N}t}\) and the solution is

\[y_{0}(t)=(c_{1}+c_{2}t+\cdot\cdot\cdot+c_{r}t^{r-1})e^{\lambda_{1}t}+c_{r+1}e ^{\lambda_{r+1}t}+\cdot\cdot\cdot+c_{N}e^{\lambda_{N}t}\]

### 2.4 Complex Roots

The procedure for handling complex roots is the same as that for real roots. For complex roots, the usual procedure leads to complex characteristic modes and the complex form of solution. However, it is possible to avoid the complex form altogether by selecting a real-form of solution, as described next.

For a real system, complex roots must occur in pairs of conjugates if the coefficients of the characteristic polynomial \(Q(\lambda)\) are to be real. Therefore, if \(\alpha+j\beta\) is a characteristic root, \(\alpha-j\beta\) must also be a characteristic root. The zero-input response corresponding to this pair of complex conjugate roots is

\[y_{0}(t)=c_{1}e^{(\alpha+j\beta)t}+c_{2}e^{(\alpha-j\beta)t} \tag{2.7}\]

For a real system, the response \(y_{0}(t)\) must also be real. This is possible only if \(c_{1}\) and \(c_{2}\) are conjugates. Let

\[c_{1}=\frac{c}{2}e^{j\theta}\qquad\text{and}\qquad c_{2}=\frac{c}{2}e^{-j\theta}\]

This yields

\[y_{0}(t) =\frac{c}{2}e^{j\theta}e^{(\alpha+j\beta)t}+\frac{c}{2}e^{-j \theta}e^{(\alpha-j\beta)t}\] \[=\frac{c}{2}e^{at}\big{[}e^{j(\beta t+\theta)}+e^{-j(\beta t+ \theta)}\big{]}\] \[=ce^{at}\cos{(\beta t+\theta)} \tag{2.8}\]

Therefore, the zero-input response corresponding to complex conjugate roots \(\alpha\pm j\beta\) can be expressed in a complex form [Eq. (2.7)] or a real form [Eq. (2.8)].

### 2.1 Finding the Zero-Input Response

Find \(y_{0}(t)\), the zero-input response of the response for an LTIC system described by

* the simple-root system \((D^{2}+3D+2)y(t)=Dx(t)\) with initial conditions \(y_{0}(0)=0\) and \(\dot{y}_{0}(0)=-5\).
* the repeated-root system \((D^{2}+6D+9)y(t)=(3D+5)x(t)\) with initial conditions \(y_{0}(0)=3\) and \(\dot{y}_{0}(0)=-7\).
* the complex-root system \((D^{2}+4D+40)y(t)=(D+2)x(t)\) with initial conditions \(y_{0}(0)=2\) and \(\dot{y}_{0}(0)=16.78\).

**(a)** Note that \(y_{0}(t)\), being the zero-input response (\(x(t)=0\)), is the solution of \((D^{2}+3D+2)y_{0}(t)=0\). The characteristic polynomial of the system is \(\lambda^{2}+3\lambda+2\). The characteristic equation of the system is therefore \(\lambda^{2}+3\lambda+2=(\lambda+1)(\lambda+2)=0\). The characteristic roots of the system are \(\lambda_{1}=-1\) and \(\lambda_{2}=-2\), and the characteristic modes of the system are \(e^{-t}\) and \(e^{-2t}\). Consequently, the zero-input response is

\[y_{0}(t)=c_{1}e^{-t}+c_{2}e^{-2t}\]

Differentiating this expression, we obtain

\[\dot{y}_{0}(t)=-c_{1}e^{-t}-2c_{2}e^{-2t}\]

To determine the constants \(c_{1}\) and \(c_{2}\), we set \(t=0\) in the equations for \(y_{0}(t)\) and \(\dot{y}_{0}(t)\) and substitute the initial conditions \(y_{0}(0)=0\) and \(\dot{y}_{0}(0)=-5\), yielding

\[0 =c_{1}+c_{2}\] \[-5 =-c_{1}-2c_{2}\]

Solving these two simultaneous equations in two unknowns for \(c_{1}\) and \(c_{2}\) yields

\[c_{1}=-5\qquad\mbox{and}\qquad c_{2}=5\]

Therefore,

\[y_{0}(t)=-5e^{-t}+5e^{-2t} \tag{2.9}\]

This is the zero-input response of \(y(t)\). Because \(y_{0}(t)\) is present at \(t=0^{-}\), we are justified in assuming that it exists for \(t\geq 0\).1

Footnote 1: \(y_{0}(t)\) may be present even before \(t=0^{-}\). However, we can be sure of its presence only from \(t=0^{-}\) onward.

**(b)** The characteristic polynomial is \(\lambda^{2}+6\lambda+9=(\lambda+3)^{2}\), and its characteristic roots are \(\lambda_{1}=-3\), \(\lambda_{2}=-3\) (repeated roots). Consequently, the characteristic modes of the system are \(e^{-3t}\) and \(te^{-3t}\). The zero-input response, being a linear combination of the characteristic modes, is given by

\[y_{0}(t)=(c_{1}+c_{2}t)e^{-3t}\]We can find the arbitrary constants \(c_{1}\) and \(c_{2}\) from the initial conditions \(y_{0}(0)=3\) and \(\dot{y}_{0}(0)=-7\) following the procedure in part (a). The reader can show that \(c_{1}=3\) and \(c_{2}=2\). Hence,

\[y_{0}(t)=(3+2t)e^{-3t}\qquad t\geq 0\]

**(c)** The characteristic polynomial is \(\lambda^{2}+4\lambda+40=(\lambda+2-j6)(\lambda+2+j6)\). The characteristic roots are \(-2\pm j6\).2 The solution can be written either in the complex form [Eq. (2.7)] or in the real form [Eq. (2.8)]. The complex form is \(y_{0}(t)=c_{1}e^{\dot{\lambda}_{1}t}+c_{2}e^{\dot{\lambda}_{2}t}\), where \(\lambda_{1}=-2+j6\) and \(\lambda_{2}=-2-j6\). Since \(\alpha=-2\) and \(\beta=6\), the real-form solution is [see Eq. (2.8)]

Footnote 2: The complex conjugate roots of a second-order polynomial can be determined by using the formula in Sec. B.8-10 or by expressing the polynomial as a sum of two squares. The latter can be accomplished by completing the square with the first two terms, as follows:

\[\lambda^{2}+4\lambda+40=(\lambda^{2}+4\lambda+4)+36=(\lambda+2)^{2}+(6)^{2}=( \lambda+2-j6)(\lambda+2+j6)\]

Differentiating this expression, we obtain

\[\dot{y}_{0}(t)=-2ce^{-2t}\cos{(6t+\theta)}-6ce^{-2t}\sin{(6t+\theta)}\]

To determine the constants \(c\) and \(\theta\), we set \(t=0\) in the equations for \(y_{0}(t)\) and \(\dot{y}_{0}(t)\) and substitute the initial conditions \(y_{0}(0)=2\) and \(\dot{y}_{0}(0)=16.78\), yielding

\[2 =c\cos{\theta}\] \[16.78 =-2c\cos{\theta}-6c\sin{\theta}\]

Solution of these two simultaneous equations in two unknowns \(c\cos{\theta}\) and \(c\sin{\theta}\) yields

\[c\cos{\theta}=2\qquad\text{and}\qquad c\sin{\theta}=-3.463\]

Squaring and then adding these two equations yield

\[c^{2}=(2)^{2}+(-3.464)^{2}=16\Longrightarrow c=4\]

Next, dividing \(c\sin{\theta}=-3.463\) by \(c\cos{\theta}=2\) yields

\[\tan{\theta}=\frac{-3.463}{2}\]

and

\[\theta=\tan^{-1}\left(\frac{-3.463}{2}\right)=-\frac{\pi}{3}\]

Therefore,

\[y_{0}(t)=4e^{-2t}\cos{\left(6t-\frac{\pi}{3}\right)}\]

For the plot of \(y_{0}(t)\), refer again to Fig. B.11c.

### 2.2 Using MATLAB to Find Polynomial Roots

Find the roots \(\lambda_{1}\) and \(\lambda_{2}\) of the polynomial \(\lambda^{2}+4\lambda+k\) for three values of \(k\): **(a)**\(k=3\), **(b)**\(k=4\), and **(c)**\(k=40\).

(a) >> r = roots([1 4 3]).' r = -3 -1 For \(k=3\), the polynomial roots are therefore \(\lambda_{1}=-3\) and \(\lambda_{2}=-1\).

(b) >> r = roots([1 4 4]).' r = -2 -2 For \(k=4\), the polynomial roots are therefore \(\lambda_{1}=\lambda_{2}=-2\).

(c) >> r = roots([1 4 40]).' r = -2.00+6.00i -2.00-6.00i For \(k=40\), the polynomial roots are therefore \(\lambda_{1}=-2+j6\) and \(\lambda_{2}=-2-j6\).

(a) >> y_0 = dsolve('D2y+4*Dy+3*y=0','y(0)=3','Dy(0)=-7','t') y_0 = 1/exp(t) + 2/exp(3*t) For \(k=3\), the zero-input response is therefore \(y_{0}(t)=e^{-t}+2e^{-3t}\).

(b) >> y_0 = dsolve('D2y+4*Dy+4*y=0','y(0)=3','Dy(0)=-7','t') y_0 = 3/exp(2*t) - t/exp(2*t)For \(k=4\), the zero-input response is therefore \(y_{0}(t)=3e^{-2t}-te^{-2t}\).

**(c)**

>> y_0 = dsolve('D2y+4*Dy+40*y=0','y(0)=3','Dy(0)=-7','t') y_0 = (3*cos(6*t))/exp(2*t) - sin(6*t)/(6*exp(2*t)) For \(k=40\), the zero-input response is therefore \(y_{0}(t)=3e^{-2t}\cos{(6t)}-\frac{1}{6}e^{-2t}\sin{(6t)}\).

### 2.1 Finding the Zero-Input Response of a First-Order System

Find the zero-input response of an LTIC system described by \((D+5)y(t)=x(t)\) if the initial condition is \(y(0)=5\).

**ANSWER**

\(y_{0}(t)=5e^{-5t}\)\(t\geq 0\)

### 2.2 Finding the Zero-Input Response of a Second-Order System

Letting \(y_{0}(0)=1\) and \(\dot{y}_{0}(0)=4\), solve

\[(D^{2}+2D)y_{0}(t)=0\]

**ANSWER**

\(y_{0}(t)=3-2e^{-2t}\)\(t\geq 0\)

### 2.3 Practical Initial Conditions and the Meaning of \(0^{-}\) and \(0^{+}\)

In Ex. 2.1 the initial conditions \(y_{0}(0)\) and \(\dot{y}_{0}(0)\) were supplied. In practical problems, we must derive such conditions from the physical situation. For instance, in an \(RLC\) circuit, we may be given the conditions (initial capacitor voltages, initial inductor currents, etc.).

From this information, we need to derive \(y_{0}(0)\), \(\dot{y}_{0}(0)\), \(\ldots\) for the desired variable as demonstrated in the next example.

In much of our discussion, the input is assumed to start at \(t=0\), unless otherwise mentioned. Hence, \(t=0\) is the reference point. The conditions immediately before \(t=0\) (just before the input is applied) are the conditions at \(t=0^{-}\), and those immediately after \(t=0\) (just after the input is applied) are the conditions at \(t=0^{+}\) (compare this with the historical time frames bce and ce). Inpractice, we are likely to know the initial conditions at \(t=0^{-}\) rather than at \(t=0^{+}\). The two sets of conditions are generally different, although in some cases they may be identical.

The total response \(y(t)\) consists of two components: the zero-input response \(y_{0}(t)\) [response due to the initial conditions alone with \(x(t)=0\)] and the zero-state response resulting from the input alone with all initial conditions zero. At \(t=0^{-}\), the total response \(y(t)\) consists solely of the zero-input response \(y_{0}(t)\) because the input has not started yet. Hence the initial conditions on \(y(t)\) are identical to those of \(y_{0}(t)\). Thus, \(y(0^{-})=y_{0}(0^{-})\), \(\dot{y}(0^{-})=\dot{y}_{0}(0^{-})\), and so on. Moreover, \(y_{0}(t)\) is the response due to initial conditions alone and does not depend on the input \(x(t)\). Hence, application of the input at \(t=0\) does not affect \(y_{0}(t)\). This means the initial conditions on \(y_{0}(t)\) at \(t=0^{-}\) and \(0^{+}\) are identical; that is, \(y_{0}(0^{-})\), \(\dot{y}_{0}(0^{-})\), \(\ldots\) are identical to \(y_{0}(0^{+})\), \(\dot{y}_{0}(0^{+})\), \(\ldots\), respectively. It is clear that for \(y_{0}(t)\), there is no distinction between the initial conditions at \(t=0^{-}\), \(0\), and \(0^{+}\). They are all the same. But this is not the case with the total response \(y(t)\), which consists of both the zero-input and zero-state responses. Thus, in general, \(y(0^{-})\neq y(0^{+})\), \(\dot{y}(0^{-})\neq\dot{y}(0^{+})\), and so on.

**EXAMPLE 2.4**: **Consideration of Initial Conditions**

A voltage \(x(t)=10e^{-3t}u(t)\) is applied at the input of the \(RLC\) circuit illustrated in Fig. 2.2a. Find the loop current \(y(t)\) for \(t\geq 0\) if the initial inductor current is zero [\(y(0^{-})=0\)] and the initial capacitor voltage is 5 volts [\(v_{C}(0^{-})=5\)].

The differential (loop) equation relating \(y(t)\) to \(x(t)\) was derived in Eq. (1.29) as

\[(D^{2}+3D+2)y(t)=Dx(t)\]

The zero-state component of \(y(t)\) resulting from the input \(x(t)\), assuming that all initial conditions are zero, that is, \(y(0^{-})=v_{C}(0^{-})=0\), will be obtained later in Ex. 2.9. In this example we shall find the zero-input reponse \(y_{0}(t)\). For this purpose, we need two initial conditions, \(y_{0}(0)\) and \(\dot{y}_{0}(0)\). These conditions can be derived from the given initial conditions, \(y(0^{-})=0\) and \(v_{C}(0^{-})=5\), as follows. Recall that \(y_{0}(t)\) is the loop current when the input terminals are shorted so that the input \(x(t)=0\) (zero-input), as depicted in Fig. 2.2b. We now compute \(y_{0}(0)\) and \(\dot{y}_{0}(0)\), the values of the loop current and its derivative at \(t=0\), from the initial values of the inductor current and the capacitor voltage. Remember that the inductor current cannot change instantaneously in the absence of an impulsive voltage. Similarly, the capacitor voltage cannot change instantaneously in the absence of an impulsive current. Therefore, when the input terminals are shorted at \(t=0\), the inductor current is still zero and the capacitor voltage is still 5 volts. Thus,

\[y_{0}(0)=0\]To determine \(\dot{y}_{0}(0)\), we use the loop equation for the circuit in Fig. 2.2b. Because the voltage across the inductor is \(L(dy_{0}/dt)\) or \(\dot{y}_{0}(t)\), this equation can be written as follows:

\[\dot{y}_{0}(t)+3y_{0}(t)+v_{C}(t)=0\]

Setting \(t=0\), we obtain

\[\dot{y}_{0}(0)+3y_{0}(0)+v_{C}(0)=0\]

But \(y_{0}(0)=0\) and \(v_{C}(0)=5\). Consequently,

\[\dot{y}_{0}(0)=-5\]

Therefore, the desired initial conditions are

\[y_{0}(0)=0\qquad\text{and}\qquad\dot{y}_{0}(0)=-5\]

Thus, the problem reduces to finding \(y_{0}(t)\), the zero-input component of \(y(t)\) of the system specified by the equation \((D^{2}+3D+2)y(t)=Dx(t)\), when the initial conditions are \(y_{0}(0)=0\) and \(\dot{y}_{0}(0)=-5\). We have already solved this problem in Ex. 2.1a, where we found

\[y_{0}(t)=-5e^{-t}+5e^{-2t}\qquad t\geq 0\]

This is the zero-input component of the loop current \(y(t)\).

It is interesting to find the initial conditions at \(t=0^{-}\) and \(0^{+}\) for the total response \(y(t)\). Let us compare \(y(0^{-})\) and \(\dot{y}(0^{-})\) with \(y(0^{+})\) and \(\dot{y}(0^{+})\). The two pairs can be compared by writing the loop equation for the circuit in Fig. 2.2a at \(t=0^{-}\) and \(t=0^{+}\). The only difference between the two situations is that at \(t=0^{-}\), the input \(x(t)=0\), whereas at \(t=0^{+}\), the input \(x(t)=10\) [because \(x(t)=10e^{-3t}\)]. Hence, the two loop equations are

\[\dot{y}(0^{-})+3y(0^{-})+v_{C}(0^{-})=0\] \[\dot{y}(0^{+})+3y(0^{+})+v_{C}(0^{+})=10\]

Figure 2.1: Circuits for Ex. 2.4.

The loop current \(y(0^{+})=y(0^{-})=0\) because it cannot change instantaneously in the absence of impulsive voltage. The same is true of the capacitor voltage. Hence, \(v_{C}(0^{+})=v_{C}(0^{-})=5\). Substituting these values in the foregoing equations, we obtain \(\dot{y}(0^{-})=-5\) and \(\dot{y}(0^{+})=5\). Thus,

\[y(0^{-})=0,\,\dot{y}(0^{-})=-5\quad\mbox{and}\quad y(0^{+})=0,\,\dot{y}(0^{+})=5 \tag{2.10}\]

## 2.3 Zero-Input Response of an RC Circuit

In the circuit in Fig. 2.2a, the inductance \(L=0\) and the initial capacitor voltage \(v_{C}(0)=30\) volts. Show that the zero-input component of the loop current is given by \(y_{0}(t)=-10e^{-2t/3}\) for \(t\geq 0\).

## Independence of the Zero-Input and Zero-State Responses

In Ex. 2.4 we computed the zero-input component without using the input \(x(t)\). The zero-state response can be computed from the knowledge of the input \(x(t)\) alone; the initial conditions are assumed to be zero (system in zero state). The two components of the system response (the zero-input and zero-state responses) are independent of each other. _The two worlds of zero-input response and zero-state response coexist side by side, neither one knowing or caring what the other is doing. For each component, the other is totally irrelevant._

## Role of Auxiliary Conditions in Solution of Differential Equations

The solution of a differential equation requires additional pieces of information (the _auxiliary conditions_). Why? We now show heuristically why a differential equation does not, in general, have a unique solution unless some additional constraints (or conditions) on the solution are known.

Differentiation operation is not invertible unless one piece of information about \(y(t)\) is given. To get back \(y(t)\) from \(dy/dt\), we must know one piece of information, such as \(y(0)\). Thus, differentiation is an irreversible (noninvertible) operation during which certain information is lost. To invert this operation, one piece of information about \(y(t)\) must be provided to restore the original \(y(t)\). Using a similar argument, we can show that, given \(d^{2}y/dt^{2}\), we can determine \(y(t)\) uniquely only if two additional pieces of information (constraints) about \(y(t)\) are given. In general, to determine \(y(t)\) uniquely from its \(N\)th derivative, we need \(N\) additional pieces of information (constraints) about \(y(t)\). These constraints are also called _auxiliary conditions_. When these conditions are given at \(t=0\), they are called _initial conditions_.

### Some Insights into the Zero-Input Behavior of a System

By definition, the zero-input response is the system response to its internal conditions, assuming that its input is zero. Understanding this phenomenon provides interesting insight into system behavior. If a system is disturbed momentarily from its rest position and if the disturbance is then removed, the system will not come back to rest instantaneously. In general, it will come back to rest over a period of time and only through a special type of motion that is characteristic of the system.2 For example, if we press on an automobile fender momentarily and then release it at \(t=0\), there is no external force on the automobile for \(t>0\).3 The auto body will eventually come back to its rest (equilibrium) position, but not through any arbitrary motion. It must do so by using only a form of response that is sustainable by the system on its own without any external source, since the input is zero. Only characteristic modes satisfy this condition. _The system uses a proper combination of characteristic modes to come back to the rest position while satisfying appropriate boundary (or initial) conditions._

Footnote 2: This assumes that the system will eventually come back to its original rest (or equilibrium) position.

Footnote 3: We ignore the force of gravity, which merely causes a constant displacement of the auto body without affecting the other motion.

If the shock absorbers of the automobile are in good condition (high damping coefficient), the characteristic modes will be monotonically decaying exponentials, and the auto body will come to rest rapidly without oscillation. In contrast, for poor shock absorbers (low damping coefficients), the characteristic modes will be exponentially decaying sinusoids, and the body will come to rest through oscillatory motion. When a series \(RC\) circuit with an initial charge on the capacitor is shorted, the capacitor will start to discharge exponentially through the resistor. This response of the \(RC\) circuit is caused entirely by its internal conditions and is sustained by this system without the aid of any external input. The exponential current waveform is therefore the characteristic mode of the \(RC\) circuit.

Mathematically we know that _any combination of characteristic modes can be sustained by the system alone without requiring an external input_. This fact can be readily verified for the series \(RL\) circuit shown in Fig. 2.2. The loop equation for this system is

\[(D+2)y(t)=x(t)\]

It has a single characteristic root \(\lambda=-2\), and the characteristic mode is \(e^{-2t}\). We now verify that a loop current \(y(t)=ce^{-2t}\) can be sustained through this circuit without any input voltage. The input voltage \(x(t)\) required to drive a loop current \(y(t)=ce^{-2t}\) is given by

\[x(t) =L\frac{dy(t)}{dt}+Ry(t)\] \[=\frac{d}{dt}(ce^{-2t})+2ce^{-2t}\] \[=-2ce^{-2t}+2ce^{-2t}=0\]

Figure 2.2: Modes always get a free ride.

Clearly, the loop current \(y(t)=ce^{-2t}\) is sustained by the \(RL\) circuit on its own, without the necessity of an external input.

### The Resonance Phenomenon

We have seen that any signal consisting of a system's characteristic mode is sustained by the system on its own; the system offers no obstacle to such signals. Imagine what would happen if we were to drive the system with an external input that is one of its characteristic modes. This would be like pouring gasoline on a fire in a dry forest or hiring a child to eat ice cream. A child would gladly do the job without pay. Think what would happen if he were paid by the amount of ice cream he ate! He would work overtime. He would work day and night, until he became sick. The same thing happens with a system driven by an input of the form of characteristic mode. The system response grows without limit, until it burns out.2 We call this behavior the _resonance phenomenon_. An intelligent discussion of this important phenomenon requires an understanding of the zero-state response; for this reason we postpone this topic until Sec. 2.6-7.

Footnote 2: In practice, the system in resonance is more likely to go in saturation because of high amplitude levels.

### The Unit Impulse Response \(h(t)\)

In Ch. 1 we explained how a system response to an input \(x(t)\) may be found by breaking this input into narrow rectangular pulses, as illustrated earlier in Fig. 1.27a, and then summing the system response to all the components. The rectangular pulses become impulses in the limit as their widths approach zero. Therefore, the system response is the sum of its responses to various impulse components. This discussion shows that if we know the system response to an impulse input, we can determine the system response to an arbitrary input \(x(t)\). We now discuss a method of determining \(h(t)\), the unit impulse response of an LTIC system described by the \(N\)th-order differential equation [Eq. (2.1)]

\[\frac{d^{N}y(t)}{dt^{N}}+a_{1}\frac{d^{N-1}y(t)}{dt^{N-1}}+\cdot \cdot\cdot+a_{N-1}\frac{dy(t)}{dt}+a_{N}y(t)\] \[\quad=b_{N-M}\frac{d^{M}x(t)}{dt^{M}}+b_{N-M+1}\frac{d^{M-1}x(t)} {dt^{M-1}}+\cdot\cdot\cdot+b_{N-1}\frac{dx(t)}{dt}+b_{N}x(t)\]

Recall that noise considerations restrict practical systems to \(M\leq N\). Under this constraint, the most general case is \(M=N\). Therefore, Eq. (2.1) can be expressed as

\[(D^{N}+a_{1}D^{N-1}+\cdot\cdot\cdot+a_{N-1}D+a_{N})y(t)=(b_{0}D^{N}+b_{1}D^{N- 1}+\cdot\cdot\cdot+b_{N-1}D+b_{N})x(t) \tag{2.11}\]

Before deriving the general expression for the unit impulse response \(h(t)\), it is illuminating to understand qualitatively the nature of \(h(t)\). The impulse response \(h(t)\) is the system response to an impulse input \(\delta(t)\) applied at \(t=0\) with all the initial conditions zero at \(t=0^{-}\). An impulse input \(\delta(t)\) is like lightning, which strikes instantaneously and then vanishes. But in its wake, in that single moment, objects that have been struck are rearranged. Similarly, an impulse input \(\delta(t)\) appears momentarily at \(t=0\), and then it is gone forever. But in that moment it generates energy storages; that is, it creates nonzero initial conditions instantaneously within the system at \(t=0^{+}\). Although the impulse input \(\delta(t)\) vanishes for \(t>0\) so that the system has no input after the impulse has been applied, the system will still have a response generated by these newly created initial conditions. The impulse response \(h(t)\), therefore, must consist of the system's characteristic modes for \(t\geq 0^{+}\). As a result,

\[h(t)=\mbox{characteristic mode terms}\qquad t\geq 0^{+}\]

This response is valid for \(t>0\). But what happens at \(t=0\)? At a single moment \(t=0\), there can at most be an impulse,+ so the form of the complete response \(h(t)\) is

Footnote †: \({}^{\dagger}\) It might be possible for the derivatives of \(\delta(t)\) to appear at the origin. However, if \(M\leq N\), it is impossible for \(h(t)\) to have any derivatives of \(\delta(t)\). This conclusion follows from Eq. (2.11) with \(x(t)=\delta(t)\) and \(y(t)=h(t)\). The coefficients of the impulse and all its derivatives must be matched on both sides of this equation. If \(h(t)\) contains \(\delta^{(1)}(t)\), the first derivative of \(\delta(t)\), the left-hand side of Eq. (2.11) will contain a term \(\delta^{(N+1)}(t)\). But the highest-order derivative term on the right-hand side is \(\delta^{(N)}(t)\). Therefore, the two sides cannot match. Similar arguments can be made against the presence of the impulse’s higher-order derivatives in \(h(t)\).

\[h(t)=A_{0}\delta(t)+\mbox{characteristic mode terms}\qquad t\geq 0 \tag{2.12}\]

because \(h(t)\) is the unit impulse response. Setting \(x(t)=\delta(t)\) and \(y(t)=h(t)\) in Eq. (2.11) yields

\[(D^{N}+a_{1}D^{N-1}+\cdot\cdot\cdot+a_{N-1}D+a_{N})h(t)=(b_{0}D^{N}+b_{1}D^{N- 1}+\cdot\cdot\cdot+b_{N-1}D+b_{N})\delta(t)\]

In this equation we substitute \(h(t)\) from Eq. (2.12) and compare the coefficients of similar impulsive terms on both sides. The highest order of the derivative of impulse on both sides is \(N\), with its coefficient value as \(A_{0}\) on the left-hand side and \(b_{0}\) on the right-hand side. The two values must be matched. Therefore, \(A_{0}=b_{0}\) and

\[h(t)=b_{0}\delta(t)+\mbox{characteristic modes} \tag{2.13}\]

In Eq. (2.11), if \(M<N\), \(b_{0}=0\). Hence, the impulse term \(b_{0}\delta(t)\) exists only if \(M=N\). The unknown coefficients of the \(N\) characteristic modes in \(h(t)\) in Eq. (2.13) can be determined by using the technique of impulse matching, as explained in the following example.

**EXAMPLE 2.5** **Impulse Response via Impulse Matching**

Find the impulse response \(h(t)\) for a system specified by

\[(D^{2}+5D+6)y(t)=(D+1)x(t) \tag{2.14}\]

In this case, \(b_{0}=0\). Hence, \(h(t)\) consists of only the characteristic modes. The characteristic polynomial is \(\lambda^{2}+5\lambda+6=(\lambda+2)(\lambda+3)\). The roots are \(-2\) and \(-3\). Hence, the impulse 

As stated earlier, if the order of \(P(D)\) is less than the order of \(Q(D)\), that is, if \(M<N\), then \(b_{0}=0\), and the impulse term \(b_{0}\delta(t)\) in \(h(t)\) is zero.

**Example 2.6**: **Impulse Response via Simplified Impulse Matching**__

Determine the unit impulse response \(h(t)\) for a system specified by the equation

\[\left(D^{2}+3D+2\right)y(t)=Dx(t) \tag{2.19}\]

This is a second-order system (\(N=2\)) having the characteristic polynomial

\[\left(\lambda^{2}+3\lambda+2\right)=(\lambda+1)(\lambda+2)\]

The characteristic roots of this system are \(\lambda=-1\) and \(\lambda=-2\). Therefore,

\[y_{n}(t)=c_{1}e^{-t}+c_{2}e^{-2t} \tag{2.20}\]

Differentiation of this equation yields

\[\dot{y}_{n}(t)=-c_{1}e^{-t}-2c_{2}e^{-2t} \tag{2.21}\]

The initial conditions are [see Eq. (2.18)]

\[\dot{y}_{n}(0)=1\qquad\mbox{and}\qquad y_{n}(0)=0\]

Setting \(t=0\) in Eqs. (2.20) and (2.21), and substituting the initial conditions just given, we obtain

\[0 =c_{1}+c_{2}\] \[1 =-c_{1}-2c_{2}\]

Solution of these two simultaneous equations yields

\[c_{1}=1\qquad\mbox{and}\qquad c_{2}=-1\]

Therefore,

\[y_{n}(t)=e^{-t}-e^{-2t}\]

Moreover, according to Eq. (2.19), \(P(D)=D\) so that

\[P(D)y_{n}(t)=Dy_{n}(t)=\dot{y}_{n}(t)=-e^{-t}+2e^{-2t}\]

Also in this case, \(b_{0}=0\) [the second-order term is absent in \(P(D)\)]. Therefore,

\[h(t)=[P(D)y_{n}(t)]u(t)=(-e^{-t}+2e^{-2t})u(t)\]

**Comment.** In the above discussion, we have assumed \(M\leq N\), as specified by Eq. (2.11). Section 2.8 shows that the expression for \(h(t)\) applicable to all possible values of \(M\) and \(N\) is given by

\[h(t)=P(D)[y_{n}(t)u(t)]\]

where \(y_{n}(t)\) is a linear combination of the characteristic modes of the system subject to initial conditions [Eq. (2.18)]. This expression reduces to Eq. (2.17) when \(M\leq N\).

Determination of the impulse response \(h(t)\) using the procedures in this section is relatively simple. However, in Ch. 4 we shall discuss another, even simpler method using the Laplace transform. As the next example demonstrates, it is also possible to find \(h(t)\) using functions from MATLAB's symbolic math toolbox.

**Example 2.7**: **Using MATLAB to Find the Impulse Response**

Determine the impulse response \(h(t)\) for an LTIC system specified by the differential equation

\[(D^{2}+3D+2)y(t)=Dx(t)\]

This is a second-order system with \(b_{0}=0\). First we find the zero-input component for initial conditions \(y(0^{-})=0\), and \(\dot{y}(0^{-})=1\). Since \(P(D)=D\), the zero-input response is differentiated and the impulse response immediately follows as \(h(t)=0\delta(t)+[Dy_{n}(t)]u(t)\).

>> y_n = dsolve('D2y+3*Dy+2*y=0','y(0)=0','Dy(0)=1','t'); h = diff(y_n) h = 2/exp(2*t) - 1/exp(t) Therefore, \(h(t)=(2e^{-2t}-e^{-t})u(t)\).

**Drill 2.4**: **Finding the Impulse Response**

Determine the unit impulse response of LTIC systems described by the following equations:

* \((D+2)y(t)=(3D+5)x(t)\)
* \(D(D+2)y(t)=(D+4)x(t)\)
* \((D^{2}+2D+1)y(t)=Dx(t)\)

**Answers**

* \(3\delta(t)-e^{-2t}u(t)\)
* \((2-e^{-2t})u(t)\)
* \((1-t)e^{-t}u(t)\)

### 2.4 System Response to External Input: The Zero-State Response

Figure 2.3: Finding the system response to an arbitrary input \(x(t)\).

## The Distributive Property

According to the distributive property,

\[x_{1}(t)*[x_{2}(t)+x_{3}(t)]=x_{1}(t)*x_{2}(t)+x_{1}(t)*x_{3}(t) \tag{2.26}\]

## The Associative Property

According to the associative property,

\[x_{1}(t)*[x_{2}(t)*x_{3}(t)]=[x_{1}(t)*x_{2}(t)]*x_{3}(t) \tag{2.27}\]

The proofs of Eqs. (2.26) and (2.27) follow directly from the definition of the convolution integral. They are left as an exercise for the reader.

## The Shift Property

If

\[x_{1}(t)*x_{2}(t)=c(t)\]

then

\[x_{1}(t)*x_{2}(t-T)=x_{1}(t-T)*x_{2}(t)=c(t-T)\]

More generally, we see that

\[x_{1}(t-T_{1})*x_{2}(t-T_{2})=c(t-T_{1}-T_{2}) \tag{2.28}\]

Proof.: We are given

\[x_{1}(t)*x_{2}(t)=\int_{-\infty}^{\infty}x_{1}(\tau)x_{2}(t-\tau)\,d\tau=c(t)\]

Therefore,

\[x_{1}(t)*x_{2}(t-T)= \int_{-\infty}^{\infty}x_{1}(\tau)x_{2}(t-T-\tau)\,d\tau\] \[= c(t-T)\]

The equally simple proof of Eq. (2.28) follows a similar approach.

### Convolution with an Impulse

Convolution of a function \(x(t)\) with a unit impulse results in the function \(x(t)\) itself. By definition of convolution,

\[x(t)*\delta(t)=\int_{-\infty}^{\infty}x(\tau)\delta(t-\tau)\,d\tau\]

Because \(\delta(t-\tau)\) is an impulse located at \(\tau=t\), according to the sampling property of the impulse [Eq. (1.11)], the integral here is just the value of \(x(\tau)\) at \(\tau=t\), that is, \(x(t)\). Therefore,

\[x(t)*\delta(t)=x(t)\]

Actually this result was derived earlier [Eq. (2.22)].

## Chapter The Width Property

If the durations (widths) of \(x_{1}(t)\) and \(x_{2}(t)\) are finite, given by \(T_{1}\) and \(T_{2}\), respectively, then the duration (width) of \(x_{1}(t)*x_{2}(t)\) is \(T_{1}+T_{2}\) (Fig. 2.4). The proof of this property follows readily from the graphical considerations discussed later in Sec. 2.4-2.

### 2.4 Zero-State Response and Causality

The (zero-state) response \(y(t)\) of an LTIC system is

\[y(t)=x(t)*h(t)=\int_{-\infty}^{\infty}x(\tau)h(t-\tau)\,d\tau \tag{2.29}\]

In deriving Eq. (2.29), we assumed the system to be linear and time-invariant. There were no other restrictions either on the system or on the input signal \(x(t)\). Since, in practice, most systems are causal, their response cannot begin before the input. Furthermore, most inputs are also causal, which means they start at \(t=0\).

Causality restriction on both signals and systems further simplifies the limits of integration in Eq. (2.29). By definition, the response of a causal system cannot begin before its input begins. Consequently, the causal system's response to a unit impulse \(\delta(t)\) (which is located at \(t=0\)) cannot begin before \(t=0\). Therefore, a _causal system's unit impulse response \(h(t)\) is a causal signal_.

It is important to remember that the integration in Eq. (2.29) is performed with respect to \(\tau\) (not \(t\)). If the input \(x(t)\) is causal, \(x(\tau)=0\) for \(\tau<0\). Therefore, \(x(\tau)=0\) for \(\tau<0\), as illustrated in Fig. 2.5a. Similarly, if \(h(t)\) is causal, \(h(t-\tau)=0\) for \(t-\tau<0\); that is, for \(\tau>t\), as depicted in Fig. 2.5a. Therefore, the product \(x(\tau)h(t-\tau)=0\) everywhere except over the nonshaded interval \(0\leq\tau\leq t\) shown in Fig. 2.5a (assuming \(t\geq 0\)). Observe that if \(t\) is negative, \(x(\tau)h(t-\tau)=0\) for all \(\tau\), as shown in Fig. 2.5b. Therefore, Eq. (2.29) reduces to

\[y(t)=x(t)*h(t)=\begin{cases}\int_{0^{-}}^{t}x(\tau)h(t-\tau)\,d\tau&t\geq 0 \\ 0&t<0\end{cases} \tag{2.30}\]

The lower limit of integration in Eq. (2.30) is taken as \(0^{-}\) to avoid the difficulty in integration that can arise if \(x(t)\) contains an impulse at the origin. This result shows that if \(x(t)\) and \(h(t)\) are both causal, the response \(y(t)\) is also causal.

Figure 2.4: Width property of convolution.

Because of the convolution's commutative property [Eq. (2.25)], we can also express Eq. (2.30) as [assuming causal \(x(t)\) and \(h(t)\)]

\[y(t)=\begin{cases}\int_{0^{-}}^{t}h(\tau)x(t-\tau)\,d\tau&t\geq 0\\ 0&t<0\end{cases}\]

Hereafter, the lower limit of \(0^{-}\) will be implied even when we write it as \(0\). As in Eq. (2.30), this result assumes that both the input and the system are causal.

**Example 2.8**: **Computing the Zero-State Response**

For an LTIC system with the unit impulse response \(h(t)=e^{-2t}u(t)\), determine the response \(y(t)\) for the input

\[x(t)=e^{-t}u(t)\]

Here both \(x(t)\) and \(h(t)\) are causal (Fig. 2.6). Hence, from Eq. (2.30), we obtain

\[y(t)=\int_{0}^{t}x(\tau)h(t-\tau)\,d\tau\qquad t\geq 0\]

Because \(x(t)=e^{-t}u(t)\) and \(h(t)=e^{-2t}u(t)\),

\[x(\tau)=e^{-\tau}u(\tau)\qquad\text{and}\qquad h(t-\tau)=e^{-2(t-\tau)}u(t-\tau)\]

Remember that the integration is performed with respect to \(\tau\) (not \(t\)), and the region of integration is \(0\leq\tau\leq t\). Hence, \(\tau\geq 0\) and \(t-\tau\geq 0\). Therefore, \(u(\tau)=1\) and \(u(t-\tau)=1\); consequently,

\[y(t)=\int_{0}^{t}e^{-\tau}e^{-2(t-\tau)}\,d\tau\qquad t\geq 0\]Because this integration is with respect to \(\tau\), we can pull \(e^{-2t}\) outside the integral, giving us

\[y(t)=e^{-2t}\int_{0}^{t}e^{\tau}\,d\tau=e^{-2t}(e^{t}-1)=e^{-t}-e^{-2t}\qquad t\geq 0\]

Moreover, \(y(t)=0\) when \(t<0\) [see Eq. (2.30)]. Therefore,

\[y(t)=(e^{-t}-e^{-2t})u(t)\]

The response is depicted in Fig. 2.6c.

### 2.5 Computing the Zero-State Response

For an LTIC system with the impulse response \(h(t)=6e^{-t}u(t)\), determine the system response to the input: **(a)**\(2u(t)\) and **(b)**\(3e^{-3t}u(t)\).

#### Answers

* \(12(1-e^{-t})u(t)\)
* \(9(e^{-t}-e^{-3t})u(t)\)

Figure 2.6: Convolution of \(x(t)\) and \(h(t)\).

### 2.6 Zero-State Response with Resonance

Repeat Drill 2.5 for the input \(x(t)=e^{-t}u(t)\).

Answer

\(6t^{-t}u(t)\)

The Convolution Table

The task of convolution is considerably simplified by a ready-made convolution table (Table 2.1). This table, which lists several pairs of signals and their convolution, can conveniently determine \(y(t)\), a system response to an input \(x(t)\), without performing the tedious job of integration. For instance, we could have readily found the convolution in Ex. 2.8 by using pair 4 (with \(\lambda_{1}=-1\) and \(\lambda_{2}=-2\)) to be \((e^{-t}-e^{-2t})u(t)\). The following example demonstrates the utility of this table.

Example 2.9: Convolution by Tables

Use Table 2.1 to compute the loop current \(y(t)\) of the \(RLC\) circuit in Ex. 2.4 for the input \(x(t)=10e^{-3t}u(t)\) when all the initial conditions are zero.

The loop equation for this circuit [see Ex. 1.16 or Eq. (1.29)] is

\[(D^{2}+3D+2)y(t)=Dx(t)\]

The impulse response \(h(t)\) for this system, as obtained in Ex. 2.6, is

\[h(t)=(2e^{-2t}-e^{-t})u(t)\]

The input is \(x(t)=10e^{-3t}u(t)\), and the response \(y(t)\) is

\[y(t)=x(t)*h(t)=10e^{-3t}u(t)*[2e^{-2t}-e^{-t}]u(t)\]

Using the distributive property of the convolution [Eq. (2.26)], we obtain

\[y(t) =10e^{-3t}u(t)*2e^{-2t}u(t)-10e^{-3t}u(t)*e^{-t}u(t)\] \[=20[e^{-3t}u(t)*e^{-2t}u(t)]-10[e^{-3t}u(t)*e^{-t}u(t)]\]

Now the use of pair 4 in Table 2.1 yields

\[y(t) =\frac{20}{-3-(-2)}[e^{-3t}-e^{-2t}]u(t)-\frac{10}{-3-(-1)}[e^{-3t }-e^{-t}]u(t)\] \[=-20(e^{-3t}-e^{-2t})u(t)+5(e^{-3t}-e^{-t})u(t)\] \[=(-5e^{-t}+20e^{-2t}-15e^{-3t})u(t)\]

### 2.7 Convolution by Tables

Use Table 2.1 to show \(e^{-2t}u(t)*(1-e^{-t})u(t)=\left(\frac{1}{2}-e^{-t}+\frac{1}{2}e^{-2t}\right)u(t)\).

### 2.8 Zero-State Response by Convolution Table

### 2.9 Another Zero-State Response by Convolution Table

For an LTIC system with the unit impulse response \(h(t)=e^{-2t}u(t)\), determine the zero-state response \(y(t)\) if the input \(x(t)=\sin 3t\,u(t)\). [_Hint:_ Use pair 12 from Table 2.1.]

### 2.9 Another Zero-State Response by Convolution Table

For an LTIC system with the unit impulse response \(h(t)=e^{-2t}u(t)\), determine the zero-state response \(y(t)\) if the input \(x(t)=\sin 3t\,u(t)\). [_Hint:_ Use pair 12 from Table 2.1.]

### 2.10 Answer

The LTIC system response discussed so far applies to general input signals, real or complex. However, if the system is real, that is, if \(h(t)\) is real, then we shall show that the real part of the input generates the real part of the output, and a similar conclusion applies to the imaginary part.

If the input is \(x(t)=x_{r}(t)+jx_{i}(t)\), where \(x_{r}(t)\) and \(x_{i}(t)\) are the real and imaginary parts of \(x(t)\), then for real \(h(t)\)

\[y(t)=h(t)*[x_{r}(t)+jx_{i}(t)]=h(t)*x_{r}(t)+jh(t)*x_{i}(t)=y_{r}(t)+jy_{i}(t)\]

where \(y_{r}(t)\) and \(y_{i}(t)\) are the real and the imaginary parts of \(y(t)\). Using the right-directed-arrow notation to indicate a pair of the input and the corresponding output, the foregoing result can be expressed as follows. If

\[x(t)=x_{r}(t)+jx_{i}(t)\ \ \ \Longrightarrow\ \ \ y(t)=y_{r}(t)+jy_{i}(t)\]

then

\[x_{r}(t)\ \ \ \Longrightarrow\ \ y_{r}(t)\ \ \ \ \ \ \ \ \text{and}\ \ \ \ \ \ \ x_{i}(t)\ \ \ \Longrightarrow\ \ \ y_{i}(t) \tag{2.31}\]

## Chapter 2 Time-domain analysis of continuous-time systems

### 2.4 Graphical Understanding of Convolution Operation

The convolution operation can be grasped readily through a graphical interpretation of the convolution integral. Such an understanding is helpful in evaluating the convolution integral of more complex signals. In addition, graphical convolution allows us to grasp visually or mentally the convolution integral's result, which can be of great help in sampling, filtering, and many other problems. Finally, many signals have no exact mathematical description, so they can be described only graphically. If two such signals are to be convolved, we have no choice but to perform their convolution graphically.

We shall now explain the convolution operation by convolving the signals \(x(t)\) and \(g(t)\), illustrated in Figs. 2.7a and 2.7b, respectively. If \(c(t)\) is the convolution of \(x(t)\) with \(g(t)\), then

\[c(t)=\int_{-\infty}^{\infty}x(\tau)g(t-\tau)\,d\tau\]

One of the crucial points to remember here is that this integration is performed with respect to \(\tau\) so that \(t\) is just a parameter (like a constant). This consideration is especially important when we sketch the graphical representations of the functions \(x(\tau)\) and \(g(t-\tau)\). Both these functions should be sketched as functions of \(\tau\), not of \(t\).

The function \(x(\tau)\) is identical to \(x(t)\), with \(\tau\) replacing \(t\) (Fig. 2.7c). Therefore, \(x(t)\) and \(x(\tau)\) will have the same graphical representations. Similar remarks apply to \(g(t)\) and \(g(\tau)\) (Fig. 2.7d).

To appreciate what \(g(t-\tau)\) looks like, let us start with the function \(g(\tau)\) (Fig. 2.7d). Time reversal of this function (reflection about the vertical axis \(\tau=0\)) yields \(g(-\tau)\) (Fig. 2.7e). Let us denote this function by \(\phi(\tau)\):

\[\phi(\tau)=g(-\tau)\]

Now \(\phi(\tau)\) shifted by \(t\) seconds is \(\phi(\tau-t)\), given by

\[\phi(\tau-t)=g[-(\tau-t)]=g(t-\tau)\]

Therefore, we first time-reverse \(g(\tau)\) to obtain \(g(-\tau)\) and then time-shift \(g(-\tau)\) by \(t\) to obtain \(g(t-\tau)\). For positive \(t\), the shift is to the right (Fig. 2.7f); for negative \(t\), the shift is to the left (Figs. 2.7g, 2.7h).

The preceding discussion gives us a graphical interpretation of the functions \(x(\tau)\) and \(g(t-\tau)\). The convolution \(c(t)\) is the area under the product of these two functions. Thus, to compute \(c(t)\) at some positive instant \(t=t_{1}\), we first obtain \(g(-\tau)\) by inverting \(g(\tau)\) about the vertical axis. Next, we right-shift or delay \(g(-\tau)\) by \(t_{1}\) to obtain \(g(t_{1}-\tau)\) (Fig. 2.7f), and then we multiply this function by \(x(\tau)\), giving us the product \(x(\tau)g(t_{1}-\tau)\) (shaded portion in Fig. 2.7f). The area \(A_{1}\) under this product is \(c(t_{1})\), the value of \(c(t)\) at \(t=t_{1}\). We can therefore plot \(c(t_{1})=A_{1}\) on a curve describing \(c(t)\), as shown in Fig. 2.7i. The area under the product \(x(\tau)g(-\tau)\) in Fig. 2.7e is \(c(0)\), the value of the convolution for \(t=0\) (at the origin).

### System Response to External Input: The Zero-State Response

Figure 2.7: Graphical explanation of the convolution operation.

A similar procedure is followed in computing the value of \(c(t)\) at \(t=t_{2}\), where \(t_{2}\) is negative (Fig. 2.7g). In this case, the function \(g(-\tau)\) is shifted by a negative amount (that is, left-shifted) to obtain \(g(t_{2}-\tau)\). Multiplication of this function with \(x(\tau)\) yields the product \(x(\tau)g(t_{2}-\tau)\). The area under this product is \(c(t_{2})=A_{2}\), giving us another point on the curve \(c(t)\) at \(t=t_{2}\) (Fig. 2.7i). This procedure can be repeated for all values of \(t\), from \(-\infty\) to \(\infty\). The result will be a curve describing \(c(t)\) for all time \(t\). Note that when \(t\leq-3,x(\tau)\) and \(g(t-\tau)\) do not overlap (see Fig. 2.7h); therefore, \(c(t)=0\) for \(t\leq-3\).

### Summary of the Graphical Procedure

The procedure for graphical convolution can be summarized as follows:

1. Keep the function \(x(\tau)\) fixed.
2. Visualize the function \(g(\tau)\) as a rigid wire frame, and rotate (or invert) this frame about the vertical axis (\(\tau=0\)) to obtain \(g(-\tau)\).
3. Shift the inverted frame along the \(\tau\) axis by \(t_{0}\) seconds. The shifted frame now represents \(g(t_{0}-\tau)\).
4. The area under the product of \(x(\tau)\) and \(g(t_{0}-\tau)\) (the shifted frame) is \(c(t_{0})\), the value of the convolution at \(t=t_{0}\).
5. Repeat this procedure, shifting the frame by different values (positive and negative) to obtain \(c(t)\) for all values of \(t\).

The graphical procedure discussed here appears very complicated and discouraging at first reading. Indeed, some people claim that convolution has driven many electrical engineering undergraduates to contemplate theology either for salvation or as an alternative career (_IEEE Spectrum_, March 1991, p. 60). Actually, the bark of convolution is worse than its bite. In graphical convolution, we need to determine the area under the product \(x(\tau)g(t-\tau)\) for all values of \(t\) from \(-\infty\) to \(\infty\). However, a mathematical description of \(x(\tau)g(t-\tau)\) is generally valid over a range of \(t\). Therefore, repeating the procedure for every value of \(t\) amounts to repeating it only a few times for different ranges of \(t\).

We can also use the commutative property of convolution to our advantage by computing \(x(t)*g(t)\) or \(g(t)*x(t)\), whichever is simpler. As a rule of thumb, _convolution computations are simplified if we choose to invert (time-reverse) the simpler of the two functions_. For example, if the mathematical description of \(g(t)\) is simpler than that of \(x(t)\), then \(x(t)*g(t)\) will be easier to compute than \(g(t)*x(t)\). In contrast, if the mathematical description of \(x(t)\) is simpler, the reverse will be true.

We shall demonstrate graphical convolution with the following examples. Let us start by using this graphical method to rework Ex. 2.8.

**EXAMPLE 2.10** Graphical Convolution of Two Causal Functions

Determine graphically \(y(t)=x(t)*h(t)\) for \(x(t)=e^{-t}u(t)\) and \(h(t)=e^{-2t}u(t)\).

In Figs. 2.8a and 2.8b we have \(x(t)\) and \(h(t)\), respectively; and Fig. 2.8c shows \(x(\tau)\) and \(h(-\tau)\) as functions of \(\tau\). The function \(h(t-\tau)\) is now obtained by shifting \(h(-\tau)\) by \(t\). If \(t\) is positive, the shift is to the right (delay); if \(t\) is negative, the shift is to the left (advance). Figure 2.8d shows that for negative \(t\), \(h(t-\tau)\) [obtained by left-shifting \(h(-\tau)\)] does not overlap \(x(\tau)\), and the product \(x(\tau)h(t-\tau)=0\), so that

\[y(t)=0\qquad t<0\]

Figure 2.8e shows the situation for \(t\geq 0\). Here \(x(\tau)\) and \(h(t-\tau)\) do overlap, but the product is nonzero only over the interval \(0\leq\tau\leq t\) (shaded interval). Therefore,

\[y(t)=\int_{0}^{t}x(\tau)h(t-\tau)\,d\tau\qquad t\geq 0\]

All we need to do now is substitute correct expressions for \(x(\tau)\) and \(h(t-\tau)\) in this integral. From Figs. 2.8a and 2.8b, it is clear that the segments of \(x(t)\) and \(g(t)\) to be used in this convolution (Fig. 2.8e) are described by

\[x(t)=e^{-t}\qquad\mbox{and}\qquad h(t)=e^{-2t}\]

Therefore,

\[x(\tau)=e^{-\tau}\qquad\mbox{and}\qquad h(t-\tau)=e^{-2(t-\tau)}\]

Consequently,

\[y(t)=\int_{0}^{t}e^{-\tau}e^{-2(t-\tau)}\,d\tau=e^{-2t}\int_{0}^{t}e^{\tau}\, d\tau=e^{-t}-e^{-2t}\qquad\qquad t\geq 0\]

Moreover, \(y(t)=0\) for \(t<0\) so that

\[y(t)=(e^{-t}-e^{-2t})u(t)\]

### 2.11 Graphical Convolution: Causal Function and Two-Sided Function

Find \(c(t)=x(t)*g(t)\) for the signals depicted in Figs. 2.9a and 2.9b.

Since \(x(t)\) is simpler than \(g(t)\), it is easier to evaluate \(g(t)*x(t)\) than \(x(t)*g(t)\). However, we shall intentionally take the more difficult route and evaluate \(x(t)*g(t)\).

From \(x(t)\) and \(g(t)\) (Figs. 2.9a and 2.9b, respectively), observe that \(g(t)\) is composed of two segments. As a result, it can be described as

\[g(t)=\begin{cases}2e^{-t}&\text{segment A}\\ -2e^{2t}&\text{segment B}\end{cases}\]

Therefore,

\[g(t-\tau)=\begin{cases}2e^{-(t-\tau)}&\text{segment A}\\ -2e^{2(t-\tau)}&\text{segment B}\end{cases}\]

The segment of \(x(t)\) that is used in convolution is \(x(t)=1\) so that \(x(\tau)=1\). Figure 2.9c shows \(x(\tau)\) and \(g(-\tau)\).

To compute \(c(t)\) for \(t\geq 0\), we right-shift \(g(-\tau)\) to obtain \(g(t-\tau)\), as illustrated in Fig. 2.9d. Clearly, \(g(t-\tau)\) overlaps with \(x(\tau)\) over the shaded interval, that is, over the range \(\tau\geq 0\); segment A overlaps with \(x(\tau)\) over the interval \((0,t)\), while segment B overlaps with \(x(\tau)\) over \((t,\infty)\). Remembering that \(x(\tau)=1\), we have

\[c(t) =\int_{0}^{\infty}x(\tau)g(t-\tau)\,d\tau\] \[=\int_{0}^{t}2e^{-(t-\tau)}\,d\tau+\int_{t}^{\infty}-2e^{2(t- \tau)}\,d\tau\] \[=2(1-e^{-t})-1=1-2e^{-t}\qquad\qquad t\geq 0\]

Figure 2.9e shows the situation for \(t<0\). Here the overlap is over the shaded interval, that is, over the range \(\tau\geq 0\), where only the segment B of \(g(t)\) is involved. Therefore,

\[c(t)=\int_{0}^{\infty}x(\tau)g(t-\tau)\,d\tau=\int_{0}^{\infty}-2e^{2(t-\tau) }\,d\tau=-e^{2t}\qquad\qquad t\leq 0\]

Therefore,

\[c(t)=\begin{cases}1-2e^{-t}&\quad t\geq 0\\ -e^{2t}&\quad t\leq 0\end{cases}\]

Figure 2.9f shows a plot of \(c(t)\).

## Chapter 2 Time-domain analysis of continuous-time systems

Figure 2.9: Convolution of \(x(t)\) and \(g(t)\).

### 2.12 Graphical Convolution of Two Finite-Duration Functions

Find \(x(t)*g(t)\) for the functions \(x(t)\) and \(g(t)\) shown in Figs. 2.10a and 2.10b.

Here, \(x(t)\) has a simpler mathematical description than that of \(g(t)\), so it is preferable to time-reverse \(x(t)\). Hence, we shall determine \(g(t)*x(t)\) rather than \(x(t)*g(t)\). Thus,

\[c(t)=g(t)*x(t)=\int_{-\infty}^{\infty}g(\tau)x(t-\tau)\,d\tau\]

First, we determine the expressions for the segments of \(x(t)\) and \(g(t)\) used in finding \(c(t)\). According to Figs. 2.10a and 2.10b, these segments can be expressed as

\[x(t)=1\qquad\text{and}\qquad g(t)=\tfrac{1}{3}t\]

so that

\[x(t-\tau)=1\qquad\text{and}\qquad g(\tau)=\tfrac{1}{3}\tau\]

Figure 2.10c shows \(g(\tau)\) and \(x(-\tau)\), whereas Fig. 2.10d shows \(g(\tau)\) and \(x(t-\tau)\), which is \(x(-\tau)\) shifted by \(t\). Because the edges of \(x(-\tau)\) are at \(\tau=-1\) and \(1\), the edges of \(x(t-\tau)\) are at \(-1+t\) and \(1+t\). The two functions overlap over the interval \((0,1+t)\) (shaded interval) so that

\[c(t)=\int_{0}^{1+t}g(\tau)x(t-\tau)\,d\tau=\int_{0}^{1+t}\tfrac{1}{3}\tau\,d \tau=\tfrac{1}{6}(t+1)^{2}\qquad\qquad-1\leq t\leq 1 \tag{2.32}\]

This situation, depicted in Fig. 2.10d, is valid only for \(-1\leq t\leq 1\). For \(t\geq 1\) but \(\leq 2\), the situation is as illustrated in Fig. 2.10e. The two functions overlap only over the range \(-1+t\) to \(1+t\) (shaded interval). Note that the expressions for \(g(\tau)\) and \(x(t-\tau)\) do not change; only the range of integration changes. Therefore,

\[c(t)=\int_{-1+t}^{1+t}\tfrac{1}{3}\tau\,d\tau=\tfrac{2}{3}t\qquad\qquad 1\leq t\leq 2 \tag{2.33}\]

Also note that the expressions in Eqs. (2.32) and (2.33) both apply at \(t=1\), the transition point between their respective ranges. We can readily verify that both expressions yield a value of \(2/3\) at \(t=1\) so that \(c(1)=2/3\). The continuity of \(c(t)\) at transition points indicates a high probability of a correct answer. Continuity of \(c(t)\) at transition points is assured as long as \(x(t)\) and \(g(t)\) contain no impulse functions.

For \(t\geq 2\) but \(\leq 4\), the situation is as shown in Fig. 2.10f. The functions \(g(\tau)\) and \(x(t-\tau)\) overlap over the interval from \(-1+t\) to \(3\) (shaded interval) so that

\[c(t)=\int_{-1+t}^{3}\tfrac{1}{3}\tau\,d\tau=-\tfrac{1}{6}(t^{2}-2t-8)\qquad \qquad 2\leq t\leq 4 \tag{2.34}\]Figure 2.10: Convolution of \(x(t)\) and \(g(t)\).

Both Eqs. (2.33) and (2.34) apply at the transition point \(t=2\). We can readily verify that \(c(2)=4/3\) when either of these expressions is used.

For \(t\geq 4\), \(x(t-\tau)\) has been shifted so far to the right that it no longer overlaps with \(g(\tau)\) as depicted in Fig. 2.10g. Consequently,

\[c(t)=0\qquad\qquad t\geq 4\]

We now turn our attention to negative values of \(t\). We have already determined \(c(t)\) up to \(t=-1\). For \(t<-1\), there is no overlap between the two functions, as illustrated in Fig. 2.10h, so that

\[c(t)=0\qquad\qquad t\leq-1\]

Combining our results, we see that

\[c(t)=\begin{cases}\frac{1}{6}(t+1)^{2}&-1\leq t<1\\ \frac{2}{3}t&1\leq t<2\\ -\frac{1}{6}(t^{2}-2t-8)&2\leq t<4\\ 0&\text{otherwise}\end{cases}\]

Figure 2.10i plots \(c(t)\) according to this expression.

The Width of Convolved Functions

The widths (durations) of \(x(t)\), \(g(t)\), and \(c(t)\) in Ex. 2.12 (Fig. 2.10) are 2, 3, and 5, respectively. Note that the width of \(c(t)\) in this case is the sum of the widths of \(x(t)\) and \(g(t)\). This observation is not a coincidence. Using the concept of graphical convolution, we can readily see that if \(x(t)\) and \(g(t)\) have the finite widths of \(T_{1}\) and \(T_{2}\) respectively, then the width of \(c(t)\) is equal to \(T_{1}+T_{2}\). The reason is that the time it takes for a signal of width (duration) \(T_{1}\) to completely pass another signal of width (duration) \(T_{2}\) so that they become non-overlapping is \(T_{1}+T_{2}\). When the two signals become non-overlapping, the convolution goes to zero.

Drill 2.10 Interchanging Convolution Order

Rework Ex. 2.11 by evaluating \(g(t)*x(t)\).

Drill 2.11 Showing Commutability Using Two Causal Signals

Use graphical convolution to show that \(x(t)*g(t)=g(t)*x(t)=c(t)\) in Fig. 2.11.

## Chapter 2 Time-domain analysis of continuous-time systems

### 2.12 Showing Commutability Using a Causal Signal and an Anticausal Signal

Repeat Drill 2.11 for the functions in Fig. 2.12.

### 2.13 Showing Commutability Using Shifted Signals

Repeat Drill 2.11 for the functions in Fig. 2.13.

Figure 2.13: Convolution of shifted signals \(x(t)\) and \(g(t)\).

Figure 2.12: Convolution of causal \(x(t)\) and anticausal \(g(t)\).

Figure 2.11: Convolution of causal signals \(x(t)\) and \(g(t)\).

## The Phantom of the Signals and Systems Opera

In the study of signals and systems we often come across some signals such as an impulse, which cannot be generated in practice and have never been sighted by anyone.2 One wonders why we even consider such idealized signals. The answer should be clear from our discussion so far in this chapter. Even if the impulse function has no physical existence, we can compute the system response \(h(t)\) to this phantom input according to the procedure in Sec. 2.3, and knowing \(h(t)\), we can compute the system response to any arbitrary input. The concept of impulse response, therefore, provides an effective intermediary for computing system response to an arbitrary input. In addition, the impulse response \(h(t)\) itself provides a great deal of information and insight about the system behavior. In Sec. 2.6 we show that the knowledge of impulse response provides much valuable information, such as the response time, pulse dispersion, and filtering properties of the system. Many other useful insights about the system behavior can be obtained by inspection of \(h(t)\).

Footnote 2: The late Prof. S. J. Mason, the inventor of signal flow graph techniques, used to tell a story of a student frustrated with the impulse function. The student said, “The unit impulse is a thing that is so small you can’t see it, except at one place (the origin), where it is so big you can’t see it. In other words, you can’t see it at all; at least I can’t!” [2].

Similarly, in frequency-domain analysis (discussed in later chapters), we use an _everlasting exponential_ (or _sinusoid_) to determine system response. An everlasting exponential (or sinusoid), too, is a phantom, which nobody has ever seen and which has no physical existence. But it provides another effective intermediary for computing the system response to an arbitrary input. Moreover, the system response to everlasting exponential (or sinusoid) provides valuable information and insight regarding the system's behavior. Clearly, idealized impulses and everlasting sinusoids are friendly and helpful spirits.

Interestingly, the unit impulse and the everlasting exponential (or sinusoid) are the dual of each other in the time-frequency duality, to be studied in Ch. 7. Actually, the time-domain and the frequency-domain methods of analysis are the dual of each other.

## Why Convolution? An Intuitive Explanation of System Response

On the surface, it appears rather strange that the response of linear systems (those gentlest of the gentle systems) should be given by such a tortuous operation of convolution, where one signal is fixed and the other is inverted and shifted. To understand this odd behavior, consider a hypothetical impulse response \(h(t)\) that decays linearly with time (Fig. 2.14a). This response is strongest at \(t=0\), the moment the impulse is applied, and it decays linearly at future instants so that one second later (at \(t=1\) and beyond), it ceases to exist. This means that the closer the impulse input is to an instant \(t\), the stronger is its response at \(t\).

Now consider the input \(x(t)\) shown in Fig. 2.14b. To compute the system response, we break the input into rectangular pulses and approximate these pulses with impulses. Generally, the response of a causal system at some instant \(t\) will be determined by all the impulse components of the input before \(t\). Each of these impulse components will have different weight in determining the response at the instant \(t\), depending on its proximity to \(t\). As seen earlier, the closer the impulse is to \(t\), the stronger is its influence at \(t\). The impulse at \(t\) has the greatest weight (unity) in determiningthe response at \(t\). The weight decreases linearly for all impulses before \(t\) until the instant \(t-1\). The input before \(t-1\) has no influence (zero weight). Thus, to determine the system response at \(t\), we must assign a linearly decreasing weight to impulses occurring before \(t\), as shown in Fig. 14b. This weighting function is precisely the function \(h(t-\tau)\). The system response at \(t\) is then determined not by the input \(x(\tau)\) but by the weighted input \(x(\tau)h(t-\tau)\), and the summation of all these weighted inputs is the convolution integral.

### 2.4-3 Interconnected Systems

A larger, more complex system can often be viewed as the interconnection of several smaller subsystems, each of which is easier to characterize. Knowing the characterizations of these subsystems, it becomes simpler to analyze such large systems. We shall consider here two basic interconnections, cascade and parallel. Figure 15a shows \(\mathcal{S}_{1}\) and \(\mathcal{S}_{2}\), two LTIC subsystems connected in parallel, and Fig. 15b shows the same two systems connected in cascade.

In Fig. 15a, the device depicted by the symbol \(\Sigma\) inside a circle represents an adder, which adds signals at its inputs. Also the junction from which two (or more) branches radiate out is called the _pickoff node_. Every branch that radiates out from the pickoff node carries the same signal (the signal at the junction). In Fig. 15a, for instance, the junction at which the input is applied is a pickoff node from which two branches radiate out, each of which carries the input signal at the node.

Let the impulse response of \(\mathcal{S}_{1}\) and \(\mathcal{S}_{2}\) be \(h_{1}(t)\) and \(h_{2}(t)\), respectively. Further assume that interconnecting these systems, as shown in Fig. 15, does not load them. This means that the impulse response of either of these systems remains unchanged whether observed when these systems are unconnected or when they are interconnected.

To find \(h_{p}(t)\), the impulse response of the parallel system \(\mathcal{S}_{p}\) in Fig. 15a, we apply an impulse at the input of \(\mathcal{S}_{p}\). This results in the signal \(\delta(t)\) at the inputs of \(\mathcal{S}_{1}\) and \(\mathcal{S}_{2}\), leading to their outputs \(h_{1}(t)\) and \(h_{2}(t)\), respectively. These signals are added by the adder to yield \(h_{1}(t)+h_{2}(t)\) as the output of \(\mathcal{S}_{p}\):

\[h_{p}(t)=h_{1}(t)+h_{2}(t)\]

To find \(h_{c}(t)\), the impulse response of the cascade system \(\mathcal{S}_{c}\) in Fig. 15b, we apply the input \(\delta(t)\) at the input of \(\mathcal{S}_{c}\), which is also the input to \(\mathcal{S}_{1}\). Hence, the output of \(\mathcal{S}_{1}\) is \(h_{1}(t)\), which now acts

Figure 14: Intuitive explanation of convolution.

as the input to \(\mathcal{S}_{2}\). The response of \(\mathcal{S}_{2}\) to input \(h_{1}(t)\) is \(h_{1}(t)*h_{2}(t)\). Therefore,

\[h_{c}(t)=h_{1}(t)*h_{2}(t)\]

Because of the commutative property of convolution, it follows that interchanging the systems \(\mathcal{S}_{1}\) and \(\mathcal{S}_{2}\), as shown in Fig. 2.15c, results in the same impulse response \(h_{1}(t)*h_{2}(t)\). This means that when several LTIC systems are cascaded, the order of systems does not affect the impulse response of the composite system. In other words, linear operations, performed in cascade, commute. The order in which they are performed is not important, at least theoretically.1

Footnote 1: Change of order, however, could affect performance because of physical limitations and sensitivities to changes in the subsystems involved.

Figure 2.15: Interconnected systems.

We shall give here another interesting application of the commutative property of LTIC systems. Figure 2.15d shows a cascade of two LTIC systems: a system \(\mathcal{S}\) with impulse response \(h(t)\), followed by an ideal integrator. Figure 2.15e shows a cascade of the same two systems in reverse order; an ideal integrator followed by \(\mathcal{S}\). In Fig. 2.15d, if the input \(x(t)\) to \(\mathcal{S}\) yields the output \(y(t)\), then the output of the system of Fig. 2.15d is the integral of \(y(t)\). In Fig. 2.15e, the output of the integrator is the integral of \(x(t)\). The output in Fig. 2.15e is identical to the output in Fig. 2.15d. Hence, it follows that if an LTIC system response to input \(x(t)\) is \(y(t)\), then the response of the same system to the integral of \(x(t)\) is the integral of \(y(t)\). In other words,

\[\text{if }x(t)\Longrightarrow y(t)\qquad\text{then }\int_{-\infty}^{t}x(\tau) \,d\tau\Longrightarrow\int_{-\infty}^{t}y(\tau)\,d\tau\]

Replacing the ideal integrator with an ideal differentiator in Figs. 2.15d and 2.15e, and following a similar argument, we conclude that

\[\text{if }x(t)\Longrightarrow y(t)\qquad\text{then }\frac{dx(t)}{dt} \Longrightarrow\frac{dy(t)}{dt}\]

If we let \(x(t)=\delta(t)\) and \(y(t)=h(t)\) in Fig. 2.15e, we find that \(g(t)\), the unit step response of an LTIC system with impulse \(h(t)\), is given by

\[g(t)=\int_{-\infty}^{t}h(\tau)\,d\tau \tag{2.35}\]

We can also show that the system response to \(\dot{\delta}(t)\) is \(dh(t)/dt\). These results can be extended to other singularity functions. For example, the unit ramp response of an LTIC system is the integral of its unit step response, and so on.

### 2.2 Inverse Systems

In Fig. 2.15b, if \(\mathcal{S}_{1}\) and \(\mathcal{S}_{2}\) are inverse systems with impulse response \(h(t)\) and \(h_{i}(t)\), respectively, then the impulse response of the cascade of these systems is \(h(t)*h_{i}(t)\). But, the cascade of a system with its inverse is an identity system, whose output is the same as the input. In other words, the unit impulse response of the cascade of inverse systems is also an unit impulse \(\delta(t)\). Hence,

\[h(t)*h_{i}(t)=\delta(t) \tag{2.36}\]

We shall give an interesting application of the commutative property. As seen from Eq. (2.36), a cascade of inverse systems is an identity system. Moreover, in a cascade of several LTIC subsystems, changing the order of the subsystems in any manner does not affect the impulse response of the cascade system. Using these facts, we observe that the two systems, shown in Fig. 2.15f, are equivalent. We can compute the response of the cascade system on the right-hand side, by computing the response of the system inside the dotted box to the input \(\dot{x}(t)\). The impulse response of the dotted box is \(g(t)\), the integral of \(h(t)\), as given in Eq. (2.35). Hence, it follows that

\[y(t)=x(t)*h(t)=\dot{x}(t)*g(t) \tag{2.37}\]

Recall that \(g(t)\) is the unit step response of the system. Hence, an LTIC response can also be obtained as a convolution of \(\dot{x}(t)\) (the derivative of the input) with the unit step response of the system. This result can be readily extended to higher derivatives of the input. An LTIC system response is the convolution of the \(n\)th derivative of the input with the \(n\)th integral of the impulse response.

### A Very Special Function for LTIC Systems:

The Everlasting Exponential \(e^{st}\)

There is a very special connection of LTIC systems with the everlasting exponential function \(e^{st}\), where \(s\) is a complex variable, in general. We now show that the LTIC system's (zero-state) response to everlasting exponential input \(e^{st}\) is also the same everlasting exponential (within a multiplicative constant). Moreover, no other function can make the same claim. Such an input for which the system response is also of the same form is called the _characteristic function_ (also _eigenfunction_) of the system. Because a sinusoid is a form of exponential (\(s=\pm jo\)), everlasting sinusoid is also a characteristic function of an LTIC system. Note that we are talking here of an everlasting exponential (or sinusoid), which starts at \(t=-\infty\).

If \(h(t)\) is the system's unit impulse response, then system response \(y(t)\) to an everlasting exponential \(e^{st}\) is given by

\[y(t)=h(t)*e^{st}=\int_{-\infty}^{\infty}h(\tau)e^{s(t-\tau)}\,d\tau=e^{st}\int _{-\infty}^{\infty}h(\tau)e^{-s\tau}\,d\tau\]

The integral on the right-most side is a function of a complex variable \(s\) and a constant with respect to \(t\). Let us denote this term by \(H(s)\), which is also complex, in general. Thus,

\[y(t)=H(s)e^{st} \tag{38}\]

where

\[H(s)=\int_{-\infty}^{\infty}h(\tau)e^{-s\tau}\,d\tau \tag{39}\]

Equation (38) is valid only for the values of \(s\) for which \(H(s)\) exists, that is, if \(\int_{-\infty}^{\infty}h(\tau)e^{-s\tau}\,d\tau\) exists (or converges). The region in the \(s\) plane for which this integral converges is called the _region of convergence_ for \(H(s)\). Further elaboration of the region of convergence is presented in Ch. 4.

For a given \(s\), note that \(H(s)\) is a constant. Thus, the input and the output are the same (within a multiplicative constant) for the everlasting exponential signal.

\(H(s)\), which is called the _transfer function_ of the system, is a function of complex variable \(s\). An alternate definition of the transfer function \(H(s)\) of an LTIC system, as seen from Eq. (38), is

\[H(s)=\frac{\mbox{output signal}}{\mbox{input signal}}\bigg{|}_{\mbox{ input}=\mbox{\scriptsize everlasting exponential }e^{st}} \tag{40}\]

The transfer function is defined for, and is meaningful to, LTIC systems only. It does not exist for nonlinear or time-varying systems, in general.

We repeat again that this discussion is about the everlasting exponential, which starts at \(t=-\infty\), not the causal exponential \(e^{st}u(t)\), which starts at \(t=0\).

For a system specified by Eq. (2.2), the transfer function is given by

\[H(s)=\frac{P(s)}{Q(s)} \tag{2.41}\]

This follows readily by considering an everlasting input \(x(t)=e^{st}\). According to Eq. (2.38), the output is \(y(t)=H(s)e^{st}\). Substitution of this \(x(t)\) and \(y(t)\) in Eq. (2.2) yields

\[H(s)[Q(D)e^{st}]=P(D)e^{st}\]

Moreover,

\[D^{\prime}e^{st}=\frac{d^{\prime}e^{st}}{dt^{\prime}}=s^{\prime}e^{st}\]

Hence,

\[P(D)e^{st}=P(s)e^{st}\qquad\text{and}\qquad Q(D)e^{st}=Q(s)e^{st}\]

Consequently,

\[H(s)=\frac{P(s)}{Q(s)}\]

### 2.14 Ideal Integrator and Differentiator Transfer Functions

Show that the transfer function of an ideal integrator is \(H(s)=1/s\) and that of an ideal differentiator is \(H(s)=s\). Find the answer in two ways: using Eq. (2.39) and using Eq. (2.41).

[_Hint:_ Find \(h(t)\) for the ideal integrator and differentiator. You also may need to use the result in Prob. 1.4-12.]

### 2.15 A Fundamental Property of LTI Systems

We can show that Eq. (2.38) is a fundamental property of LTI systems and it follows directly as a consequence of linearity and time invariance. To show this let us assume that the response of an LTI system to an everlasting exponential \(e^{st}\) is \(y(s,t)\). If we define

\[H(s,t)=\frac{y(s,t)}{e^{st}}\]

then

\[y(s,t)=H(s,t)\,e^{st}\]

Because of the time-invariance property, the system response to input \(e^{s(t-T)}\) is \(H(s,t-T)\,e^{s(t-T)}\), that is,

\[y(s,t-T)=H(s,t-T)\,e^{s(t-T)} \tag{2.42}\]

The delayed input \(e^{st}\) represents the input \(e^{st}\) multiplied by a constant \(e^{-sT}\). Hence, according to the linearity property, the system response to \(e^{s(t-T)}\) must be \(y(s,t)\,e^{-sT}\). Hence,

\[y(s,t-T)=y(s,t)\,e^{-sT}=H(s,t)\,e^{s(t-T)}\]Comparison of this result with Eq. (42) shows that

\[H(s,t)=H(s,t-T)\qquad\text{for all}\,T\]

This means \(H(s,t)\) is independent of \(t\), and we can express \(H(s,t)=H(s)\). Hence,

\[y(s,t)=H(s)\,e^{st}\]

### Total Response

Assuming distinct roots, the total response of a linear system can be expressed as the sum of its zero-input response (ZIR) and its zero-state response (ZSR):

\[\text{total response}=\underbrace{\sum_{k=1}^{N}c_{k}e^{\lambda_{k}t}}_{\text{ZIR }}+\underbrace{x(t)*h(t)}_{\text{ZSR}}\]

For repeated roots, the zero-input component should be appropriately modified.

For the series \(RLC\) circuit in Ex. 4 with the input \(x(t)=10e^{-3t}u(t)\) and the initial conditions \(y(0^{-})=0,v_{C}(0^{-})=5\), we determined the zero-input response in Ex. 1a [Eq. (9)]. We found the zero-state response in Ex. 9. From the results in Exs. 1a and 9, we obtain

\[\text{total current}=\underbrace{(-5e^{-t}+5e^{-2t})}_{\text{zero-input current}}+\underbrace{(-5e^{-t}+20e^{-2t}-15e^{-3t})}_{\text{zero-state current}}\qquad t\geq 0 \tag{43}\]

Figure 16a shows the zero-input, zero-state, and total responses.

Figure 16: Total response and its components.

away from the original state. Thus it is said to be in a _neutral equilibrium_. Clearly, when a system is in stable equilibrium, application of a small disturbance (input) produces a small response. In contrast, when the system is in unstable equilibrium, even a minuscule disturbance (input) produces an unbounded response. The BIBO-stability definition can be understood in the light of this concept. If every bounded input produces bounded output, the system is (BIBO) stable.2 In contrast, if even one bounded input results in unbounded response, the system is (BIBO) unstable.

Footnote 2: The system is assumed to be in zero state.

For an LTIC system,

\[y(t)=h(t)*x(t)=\int_{-\infty}^{\infty}h(\tau)x(t-\tau)\,d\tau\]

Therefore,

\[|y(t)|\leq\int_{-\infty}^{\infty}|h(\tau)||x(t-\tau)|\,d\tau\]

Moreover, if \(x(t)\) is bounded, then \(|x(t-\tau)|<K_{1}<\infty\), and

\[|y(t)|\leq K_{1}\int_{-\infty}^{\infty}|h(\tau)|\,d\tau\]

Hence for BIBO stability,

\[\int_{-\infty}^{\infty}|h(\tau)|\,d\tau<\infty \tag{45}\]

This is a sufficient condition for BIBO stability. We can show that this is also a necessary condition (see Prob. 2.5-7). Therefore, for an LTIC system, if its impulse response \(h(t)\) is absolutely integrable, the system is (BIBO) stable. Otherwise it is (BIBO) unstable. In addition, we shall show in Ch. 4 that a necessary (but not sufficient) condition for an LTIC system described by Eq. (1) to be BIBO-stable is \(M\leq N\). If \(M>N\), the system is unstable. This is one of the reasons to avoid systems with \(M>N\).

Because the BIBO stability of a system can be ascertained by measurements at the external terminals (input and output), this is an external stability criterion. It is no coincidence that the BIBO criterion in Eq. (45) is in terms of the impulse response, which is an external description of the system.

As observed in Sec. 1.9, the internal behavior of a system is not always ascertainable from the external terminals. Therefore, external (BIBO) stability may not be a correct indication of internal stability. Indeed, some systems that appear stable by the BIBO criterion may be internally unstable. This is like a room on fire inside a house: no trace of fire is visible from outside, but the entire house will be burned to ashes.

The BIBO stability is meaningful only for systems in which the internal and the external description are equivalent (controllable and observable systems). Fortunately, most practical systems fall into this category, and whenever we apply this criterion, we implicitly assume that the system, in fact, belongs to this category. Internal stability is all-inclusive, and external stability can always be determined from internal stability. For this reason, we now investigate the internal stability criterion.

### 2.5-2 Internal (Asymptotic) Stability

Because of the great variety of possible system behaviors, there are several definitions of internal stability in the literature. Here we shall consider a definition that is suitable for causal, linear, time-invariant (LTI) systems.

If, in the absence of an external input, a system remains in a particular state (or condition) indefinitely, then that state is said to be an _equilibrium state_ of the system. For an LTI system, zero state, in which all initial conditions are zero, is an equilibrium state. Now suppose an LTI system is in zero state and we change this state by creating small nonzero initial conditions (small disturbance). These initial conditions will generate signals consisting of characteristic modes in the system. By analogy with the cone, if the system is stable, it should eventually return to zero state. In other words, when left to itself, every mode in a stable system arising as a result of nonzero initial conditions should approach \(0\) as \(t\to\infty\). However, if even one of the modes grows with time, the system will never return to zero state, and the system would be identified as unstable. In the borderline case, some modes neither decay to zero nor grow indefinitely, while all the remaining modes decay to zero. This case is like the neutral equilibrium in the cone. Such a system is said to be _marginally_ stable. Internal stability is also called _asymptotic_ stability or stability in the sense of _Lyapunov_.

For a system characterized by Eq. (2.1), we can restate the internal stability criterion in terms of the location of the \(N\) characteristic roots \(\lambda_{1}\), \(\lambda_{2}\), \(\ldots\), \(\lambda_{N}\) of the system in a complex plane. The characteristic modes are of the form \(e^{\lambda_{k}t}\) or \(t^{\prime}e^{\lambda_{k}t}\). The locations of various roots in the complex plane and the corresponding modes are shown in Fig. 2.17. These modes \(\to 0\) as \(t\to\infty\) if Re \(\lambda_{k}<0\). In contrast, the modes \(\to\infty\) as \(t\to\infty\) if Re \(\lambda_{k}>0\).+

Footnote †: This conclusion is also valid for the terms of the form \(t^{\prime}e^{\lambda_{k}t}\).

From Fig. 2.17, we see that a system is (asymptotically) stable if all its characteristic roots lie in the LHP, that is, if Re \(\lambda_{k}<0\) for all \(k\). If even a single characteristic root lies in the RHP, the system is (asymptotically) unstable. Modes due to roots on the imaginary axis (\(\lambda=\pm j\omega_{0}\)) are of the form \(e^{\pm j\omega_{0}t}\). Hence, if some roots are on the imaginary axis, and all the remaining roots are in the LHP, the system is marginally stable (assuming that the roots on the imaginary axis are not repeated). If the imaginary axis roots are repeated, the characteristic modes are of the form \(t^{\prime}e^{\pm j\omega_{0}t}\), which _do_ grow with time indefinitely. Hence, the system is unstable. Figure 2.18 shows stability regions in the complex plane.

To summarize:

1. An LTIC system is asymptotically stable if, and only if, all the characteristic roots are in the LHP. The roots may be simple (unrepeated) or repeated.
2. An LTIC system is unstable if, and only if, one or both of the following conditions exist: (i) at least one root is in the RHP; (ii) there are repeated roots on the imaginary axis.
3. An LTIC system is marginally stable if, and only if, there are no roots in the RHP, and there are some unrepeated roots on the imaginary axis.

### 2.5-3 Relationship Between BIBO and Asymptotic Stability

External stability is determined by applying an external input with zero initial conditions, while internal stability is determined by applying the nonzero initial conditions and no external input. This is why these stabilities are also called the _zero-state stability_ and the _zero-input stability_, respectively.

Recall that \(h(t)\), the impulse response of an LTIC system, is a linear combination of the system characteristic modes. For an LTIC system, specified by Eq. (2.1), we can readily show that when a characteristic root \(\lambda_{k}\) is in the LHP, the corresponding mode \(e^{\lambda_{k}t}\) is absolutely integrable. In

Figure 2.17: Location of characteristic roots and the corresponding characteristic modes.

contrast, if \(\lambda_{k}\) is in the RHP or on the imaginary axis, \(e^{\lambda_{k}t}\) is not absolutely integrable.2 This means that an asymptotically stable system is BIBO-stable. Moreover, a marginally stable or asymptotically unstable system is BIBO-unstable. The converse is not necessarily true; that is, BIBO stability does not necessarily inform us about the internal stability of the system. For instance, if a system is uncontrollable and/or unobservable, some modes of the system are invisible and/or uncontrollable from the external terminals [3]. Hence, the stability picture portrayed by the external description is of questionable value. BIBO (external) stability cannot assure internal (asymptotic) stability, as the following example shows.

Footnote 2: Consider a mode of the form \(e^{\lambda_{t}}\), where \(\lambda=\alpha+j\beta\). Hence, \(e^{\lambda_{t}}=e^{\alpha t}e^{j\beta t}\) and \(|e^{\lambda_{t}}|=e^{\alpha t}\). Therefore,

\[\int_{-\infty}^{\infty}|e^{\lambda_{t}}u(\tau)|\,d\tau=\int_{0}^{\infty}e^{ \alpha\tau}\,d\tau=\begin{cases}-1/\alpha&\alpha<0\\ \infty&\alpha\geq 0\end{cases}\]

 This conclusion is also valid when the integrand is of the form \(|t^{k}e^{\lambda_{t}}u(t)|\).

The composite system impulse response \(h(t)\) is given by

\[h(t)=h_{1}(t)*h_{2}(t)=h_{2}(t)*h_{1}(t) =e^{t}u(t)*[\delta(t)-2e^{-t}u(t)]\] \[=e^{t}u(t)-2\Bigg{[}\frac{e^{t}-e^{-t}}{2}\Bigg{]}u(t)\] \[=e^{-t}u(t)\]

If the composite cascade system were to be enclosed in a black box with only the input and the output terminals accessible, any measurement from these external terminals would show that the impulse response of the system is \(e^{-t}u(t)\), without any hint of the dangerously unstable system the system is harboring within.

The composite system is BIBO-stable because its impulse response, \(e^{-t}u(t)\), is absolutely integrable. Observe, however, the subsystem \(\mathcal{S}_{2}\) has a characteristic root 1, which lies in the RHP. Hence, \(\mathcal{S}_{2}\) is asymptotically unstable. Eventually, \(\mathcal{S}_{2}\) will burn out (or saturate) because of the unbounded characteristic response generated by intended or unintended initial conditions, no matter how small. We shall show in Ex. 10.12 that this composite system is observable, but not controllable. If the positions of \(\mathcal{S}_{1}\) and \(\mathcal{S}_{2}\) were interchanged (\(\mathcal{S}_{2}\) followed by \(\mathcal{S}_{1}\)), the system is still BIBO-stable, but asymptotically unstable. In this case, the analysis in Ex. 10.12 shows that the composite system is controllable, but not observable.

This example shows that BIBO stability does not always imply asymptotic stability. However, asymptotic stability always implies BIBO stability.

Fortunately, uncontrollable and/or unobservable systems are not commonly observed in practice. Henceforth, in determining system stability, we shall assume that unless otherwise mentioned, the internal and the external descriptions of a system are equivalent, implying that the system is controllable and observable.

## Example 2.14 Investigating Asymptotic and BIBO Stability

Investigate the asymptotic and the BIBO stability of LTIC system described by the following equations, assuming that the equations are internal system descriptions:

1. \((D+1)(D^{2}+4D+8)y(t)=(D-3)x(t)\)
2. \((D-1)(D^{2}+4D+8)y(t)=(D+2)x(t)\)
3. \((D+2)(D^{2}+4)y(t)=(D^{2}+D+1)x(t)\)
4. \((D+1)(D^{2}+4)^{2}y(t)=(D^{2}+2D+8)x(t)\)

Figure 2.19: Composite system for Ex. 2.13.

The characteristic polynomials of these systems are

\[\begin{array}{ll}\mbox{\bf(a)}&\mbox{\bf($\lambda+1$)($\lambda^{2}+4\lambda+8$) }=(\lambda+1)(\lambda+2-j2)(\lambda+2+j2)\\ \mbox{\bf($\lambda-1$)($\lambda^{2}+4\lambda+8$)}=(\lambda-1)(\lambda+2-j2)( \lambda+2+j2)\\ \mbox{\bf(c)}&\mbox{\bf($\lambda+2$)($\lambda^{2}+4$)}=(\lambda+2)(\lambda-j2 )(\lambda+j2)\\ \mbox{\bf(d)}&\mbox{\bf($\lambda+1$)($\lambda^{2}+4$)}^{2}=(\lambda+2)(\lambda -j2)^{2}(\lambda+j2)^{2}\end{array}\]

Consequently, the characteristic roots of the systems are (see Fig. 2.20):

\[\begin{array}{ll}\mbox{\bf(a)}&-1,\,-2\pm j2\\ \mbox{\bf(b)}&1,\,-2\pm j2\\ \mbox{\bf(c)}&-2,\,\pm j2\\ \mbox{\bf(d)}&-1,\,\pm j2,\,\pm j2\end{array}\]

System (a) is asymptotically stable (all roots in LHP), system (b) is unstable (one root in RHP), system (c) is marginally stable (unrepeated roots on imaginary axis) and no roots in RHP, and system (d) is unstable (repeated roots on the imaginary axis). BIBO stability is readily determined from the asymptotic stability. System (a) is BIBO-stable, system (b) is BIBO-unstable, system (c) is BIBO-unstable, and system (d) is BIBO-unstable. We have assumed that these systems are controllable and observable.

##### 2.15 Assessing Stability by Characteristic Roots

For each case, plot the characteristic roots and determine asymptotic and BIBO stabilities. Assume the equations reflect internal descriptions.

**(a)**: \(D(D+2)y(t)=3x(t)\)
**(b)**: \(D^{2}(D+3)y(t)=(D+5)x(t)\)
**(c)**: \((D+1)(D+2)y(t)=(2D+3)x(t)\)
**(d)**: \((D^{2}+1)(D^{2}+9)y(t)=(D^{2}+2D+4)x(t)\)
**(e)**: \((D+1)(D^{2}-4D+9)y(t)=(D+7)x(t)\)

Figure 2.20: Characteristic root locations for the systems of Ex. 2.14.

### Implications of Stability

All practical signal-processing systems must be asymptotically stable. Unstable systems are useless from the viewpoint of signal processing because any set of intended or unintended initial conditions leads to an unbounded response that either destroys the system or (more likely) leads it to some saturation conditions that change the nature of the system. Even if the discernible initial conditions are zero, stray voltages or thermal noise signals generated within the system will act as initial conditions. Because of exponential growth of a mode or modes in unstable systems, a stray signal, no matter how small, will eventually cause an unbounded output.

Marginally stable systems, though BIBO unstable, do have one important application in the oscillator, which is a system that generates a signal on its own without the application of an external input. Consequently, the oscillator output is a zero-input response. If such a response is to be a sinusoid of frequency \(\omega_{0}\), the system should be marginally stable with characteristic roots at \(\pm\dot{y}\omega_{0}\). Thus, to design an oscillator of frequency \(\omega_{0}\), we should pick a system with the characteristic polynomial \((\lambda-j\omega_{0})(\lambda+j\omega_{0})=\lambda^{2}+{\omega_{0}}^{2}\). A system described by the differential equation

\[\big{(}D^{2}+{\omega_{0}}^{2}\big{)}y(t)=x(t)\]

will do the job. However, practical oscillators are invariably realized using nonlinear systems.

### 2.6 Intuitive Insights into System Behavior

This section attempts to provide an understanding of what determines system behavior. Because of its intuitive nature, the discussion is more or less qualitative. We shall now show that the most important attributes of a system are its characteristic roots or characteristic modes because they determine not only the zero-input response but also the entire behavior of the system.

#### Dependence of System Behavior on Characteristic Modes

Recall that the zero-input response of a system consists of the system's characteristic modes. For a stable system, these characteristic modes decay exponentially and eventually vanish. This behavior may give the impression that these modes do not substantially affect system behavior in general and system response in particular. This impression is totally wrong! We shall now see that the system's characteristic modes leave their imprint on every aspect of the system behavior. _We may compare the system's characteristic modes (or roots) to a seed that eventually dissolves in theground; however, the plant that springs from it is totally determined by the seed. The imprint of the seed exists on every cell of the plant._

To understand this interesting phenomenon, recall that the characteristic modes of a system are very special to that system because it can sustain these signals without the application of an external input. In other words, the system offers a free ride and ready access to these signals. Now imagine what would happen if we actually drove the system with an input having the form of a characteristic mode! We would expect the system to respond strongly (this is, in fact, the resonance phenomenon discussed later in this section). If the input is not exactly a characteristic mode but is close to such a mode, we would still expect the system response to be strong. However, if the input is very different from any of the characteristic modes, we would expect the system to respond poorly. We shall now show that these intuitive deductions are indeed true.

We have devised a measure of similarity of signals later (see in Ch. 6). Here we shall take a simpler approach. Let us restrict the system's inputs to exponentials of the form \(e^{\zeta t}\), where \(\zeta\) is generally a complex number. The similarity of two exponential signals \(e^{\zeta t}\) and \(e^{\lambda t}\) will then be measured by the closeness of \(\zeta\) and \(\lambda\). If the difference \(\zeta-\lambda\) is small, the signals are similar; if \(\zeta-\lambda\) is large, the signals are dissimilar.

Now consider a first-order system with a single characteristic mode \(e^{\lambda t}\) and the input \(e^{\zeta t}\). The impulse response of this system is then given by \(Ae^{\lambda t}\), where the exact value of \(A\) is not important for this qualitative discussion. The system response \(y(t)\) is given by

\[y(t)=h(t)*x(t)=Ae^{\lambda t}u(t)*e^{\zeta t}u(t)\]

From the convolution table (Table 2.1), we obtain

\[y(t)=\frac{A}{\zeta-\lambda}[e^{\zeta t}-e^{\lambda t}]u(t) \tag{2.46}\]Clearly, if the input \(e^{\xi t}\) is similar to \(e^{\lambda t}\), \(\zeta-\lambda\) is small and the system response is large. _The closer the input \(x(t)\) to the characteristic mode, the stronger the system response._ In contrast, if the input is very different from the natural mode, \(\zeta-\lambda\) is large and the system responds poorly. This is precisely what we set out to prove.

We have proved the foregoing assertion for a single-mode (first-order) system. It can be generalized to an \(N\)th-order system, which has \(N\) characteristic modes. The impulse response \(h(t)\) of such a system is a linear combination of its \(N\) modes. Therefore, if \(x(t)\) is similar to any one of the modes, the corresponding response will be high; if it is similar to none of the modes, the response will be small. Clearly, the characteristic modes are very influential in determining system response to a given input.

It would be tempting to conclude on the basis of Eq. (2.46) that if the input is identical to the characteristic mode, so that \(\zeta=\lambda\), then the response goes to infinity. Remember, however, that if \(\zeta=\lambda\), the numerator on the right-hand side of Eq. (2.46) also goes to zero. We shall study this interesting behavior (resonance phenomenon) later in this section.

We now show that _mere inspection of the impulse response \(h(t)\) (which is composed of characteristic modes) reveals a great deal about the system behavior_.

### Response Time of a System: The System Time Constant

Like human beings, systems have a certain response time. In other words, when an input (stimulus) is applied to a system, a certain amount of time elapses before the system fully responds to that input. This time lag or response time is called the system _time constant_. As we shall see, a system's time constant is equal to the width of its impulse response \(h(t)\).

An input \(\delta(t)\) to a system is instantaneous (zero duration), but its response \(h(t)\) has a duration \(T_{h}\). Therefore, the system requires a time \(T_{h}\) to respond fully to this input, and we are justified in viewing \(T_{h}\) as the system's response time or time constant. We arrive at the same conclusion via another argument. The output is a convolution of the input with \(h(t)\). If an input is a pulse of width \(T_{x}\), then the output pulse width is \(T_{x}+T_{h}\) according to the width property of convolution. This conclusion shows that the system requires \(T_{h}\) seconds to respond fully to any input. _The system time constant indicates how fast the system is. A system with a smaller time constant is a faster system that responds quickly to an input. A system with a relatively large time constant is a sluggish system that cannot respond well to rapidly varying signals._

Strictly speaking, the duration of the impulse response \(h(t)\) is \(\infty\) because the characteristic modes approach zero asymptotically as \(t\to\infty\). However, beyond some value of \(t\), \(h(t)\) becomes negligible. It is therefore necessary to use some suitable measure of the impulse response's effective width.

There is no single satisfactory definition of effective signal duration (or width) applicable to every situation. For the situation depicted in Fig. 2.21, a reasonable definition of the duration \(h(t)\) would be \(T_{h}\), the width of the rectangular pulse \(\hat{h}(t)\). This rectangular pulse \(\hat{h}(t)\) has an area identical to that of \(h(t)\) and a height identical to that of \(h(t)\) at some suitable instant \(t=t_{0}\). In Fig. 2.21, \(t_{0}\) is chosen as the instant at which \(h(t)\) is maximum. According to this definition,1

Footnote 1: This definition is satisfactory when \(h(t)\) is a single, mostly positive (or mostly negative) pulse. Such systems are lowpass systems. This definition should not be applied indiscriminately to all systems.

\[T_{h}h(t_{0})=\int_{-\infty}^{\infty}h(t)\,dt\]or

\[T_{h}=\frac{\int_{-\infty}^{\infty}h(t)\,dt}{h(t_{0})} \tag{2.47}\]

Now if a system has a single mode

\[h(t)=Ae^{\lambda t}u(t)\]

with \(\lambda\) negative and real, then \(h(t)\) is maximum at \(t=0\) with value \(h(0)=A\). Therefore, according to Eq. (2.47),

\[T_{h}=\frac{1}{A}\int_{0}^{\infty}Ae^{\lambda t}\,dt=-\frac{1}{\lambda}\]

Thus, the time constant in this case is simply the (negative of the) reciprocal of the system's characteristic root. For the multimode case, \(h(t)\) is a weighted sum of the system's characteristic modes, and \(T_{h}\) is a weighted average of the time constants associated with the \(N\) modes of the system.

### 2.6-3 Time Constant and Rise Time of a System

Rise time of a system, defined as the time required for the unit step response to rise from 10% to 90% of its steady-state value, is an indication of the speed of response.2 The system time constant may also be viewed from a perspective of rise time. The unit step response \(y(t)\) of a system is the convolution of \(u(t)\) with \(h(t)\). Let the impulse response \(h(t)\) be a rectangular pulse of width \(T_{h}\), as shown in Fig. 2.22. This assumption simplifies the discussion, yet gives satisfactory results for qualitative discussion. The result of this convolution is illustrated in Fig. 2.22. Note that the output does not rise from zero to a final value instantaneously as the input rises; instead, the output takes \(T_{h}\) seconds to accomplish this. Hence, the rise time \(T_{r}\) of the system is equal to the system time constant

Footnote 2: Because of varying definitions of rise time, the reader may find different results in the literature. The qualitative and intuitive nature of this discussion should always be kept in mind.

\[T_{r}=T_{h}\]

This result and Fig. 2.22 show clearly that a system generally does not respond to an input instantaneously. Instead, it takes time \(T_{h}\) for the system to respond fully.

Figure 2.21: Effective duration of an impulse response.

### 2.6.4 Time Constant and Filtering

A larger time constant implies a sluggish system because the system takes longer to respond fully to an input. Such a system cannot respond effectively to rapid variations in the input. In contrast, a smaller time constant indicates that a system is capable of responding to rapid variations in the input. Thus, there is a direct connection between a system's time constant and its filtering properties.

A high-frequency sinusoid varies rapidly with time. A system with a large time constant will not be able to respond well to this input. Therefore, such a system will suppress rapidly varying (high-frequency) sinusoids and other high-frequency signals, thereby acting as a lowpass filter (a filter allowing the transmission of low-frequency signals only). We shall now show that a system

Figure 2.23. Time constant and filtering.

Figure 2.22. Rise time of a system.

with a time constant \(T_{h}\) acts as a lowpass filter having a cutoff frequency of \(f_{c}=1/T_{h}\) hertz, so that sinusoids with frequencies below \(f_{c}\) Hz are transmitted reasonably well, while those with frequencies above \(f_{c}\) Hz are suppressed.

To demonstrate this fact, let us determine the system response to a sinusoidal input \(x(t)\) by convolving this input with the effective impulse response \(h(t)\) in Fig. 23a. From Figs. 23b and 23c we see the process of convolution of \(h(t)\) with the sinusoidal inputs of two different frequencies. The sinusoid in Fig. 23b has a relatively high frequency, while the frequency of the sinusoid in Fig. 23c is low. Recall that the convolution of \(x(t)\) and \(h(t)\) is equal to the area under the product \(x(\tau)h(t-\tau)\). This area is shown shaded in Figs. 23b and 23c for the two cases. For the high-frequency sinusoid, it is clear from Fig. 23b that the area under \(x(\tau)h(t-\tau)\) is very small because its positive and negative areas nearly cancel each other out. In this case the output \(y(t)\) remains periodic but has a rather small amplitude. This happens when the period of the sinusoid is much smaller than the system time constant \(T_{h}\). In contrast, for the low-frequency sinusoid, the period of the sinusoid is larger than \(T_{h}\), rendering the partial cancellation of area under \(x(\tau)h(t-\tau)\) less effective. Consequently, the output \(y(t)\) is much larger, as depicted in Fig. 23c.

Between these two possible extremes in system behavior, a transition point occurs when the period of the sinusoid is equal to the system time constant \(T_{h}\). The frequency at which this transition occurs is known as the _cutoff frequency_\(f_{c}\) of the system. Because \(T_{h}\) is the period of cutoff frequency \(f_{c}\),

\[f_{c}=\frac{1}{T_{h}}\]

The frequency \(f_{c}\) is also known as the bandwidth of the system because the system transmits or passes sinusoidal components with frequencies below \(f_{c}\) while attenuating components with frequencies above \(f_{c}\). Of course, the transition in system behavior is gradual. There is no dramatic change in system behavior at \(f_{c}=1/T_{h}\). Moreover, these results are based on an idealized (rectangular pulse) impulse response; in practice these results will vary somewhat, depending on the exact shape of \(h(t)\). Remember that the "feel" of general system behavior is more important than exact system response for this qualitative discussion.

Since the system time constant is equal to its rise time, we have

\[T_{r}=\frac{1}{f_{c}}\qquad\mbox{or}\qquad f_{c}=\frac{1}{T_{r}} \tag{48}\]

Thus, a system's bandwidth is inversely proportional to its rise time. Although Eq. (48) was derived for an idealized (rectangular) impulse response, its implications are valid for lowpass LTIC systems, in general. For a general case, we can show that [1]

\[f_{c}=\frac{k}{T_{r}}\]

where the exact value of \(k\) depends on the nature of \(h(t)\). An experienced engineer often can estimate quickly the bandwidth of an unknown system by simply observing the system response to a step input on an oscilloscope.

### Time Constant and Pulse Dispersion (Spreading)

In general, the transmission of a pulse through a system causes pulse dispersion (or spreading). Therefore, the output pulse is generally wider than the input pulse. This system behavior can have serious consequences in communication systems in which information is transmitted by pulse amplitudes. Dispersion (or spreading) causes interference or overlap with neighboring pulses, thereby distorting pulse amplitudes and introducing errors in the received information.

Earlier we saw that if an input \(x(t)\) is a pulse of width \(T_{x}\), then \(T_{y}\), the width of the output \(y(t)\), is

\[T_{y}=T_{x}+T_{h}\]

This result shows that an input pulse spreads out (disperses) as it passes through a system. Since \(T_{h}\) is also the system's time constant or rise time, the amount of spread in the pulse is equal to the time constant (or rise time) of the system.

### Time Constant and Rate of Information Transmission

In pulse communications systems, which convey information through pulse amplitudes, the rate of information transmission is proportional to the rate of pulse transmission. We shall demonstrate that to avoid the destruction of information caused by dispersion of pulses during their transmission through the channel (transmission medium), the rate of information transmission should not exceed the bandwidth of the communications channel.

Since an input pulse spreads out by \(T_{h}\) seconds, the consecutive pulses should be spaced \(T_{h}\) seconds apart to avoid interference between pulses. Thus, the rate of pulse transmission should not exceed \(1/T_{h}\) pulses/second. But \(1/T_{h}=f_{c}\), the channel's bandwidth, so that we can transmit pulses through a communications channel at a rate of \(f_{c}\) pulses per second and still avoid significant interference between the pulses. The rate of information transmission is therefore proportional to the channel's bandwidth (or to the reciprocal of its time constant).1

Footnote 1: Theoretically, a channel of bandwidth \(f_{c}\) can transmit correctly up to \(2f_{c}\) pulse amplitudes per second [4]. Our derivation here, being very simple and qualitative, yields only half the theoretical limit. In practice it is not easy to attain the upper theoretical limit.

The discussion of Secs. 2.6-2, 2.6-3, 2.6-4, 2.6-5, and 2.6-6) shows that the system time constant determines much of a system's behavior--its filtering characteristics, rise time, pulse dispersion, and so on. In turn, the time constant is determined by the system's characteristic roots. Clearly the characteristic roots and their relative amounts in the impulse response \(h(t)\) determine the behavior of a system.

## Example 2.15 Intuitive Insights into Lowpass System Behavior

Find the time constant \(T_{h}\), rise time \(T_{r}\), and cutoff frequency \(f_{c}\) for a lowpass system that has impulse response \(h(t)=te^{-t}u(t)\). Determine the maximum rate that pulses of 1 secondduration can be transmitted through the system so that interference is essentially avoided between adjacent pulses at the system output.

The system impulse response \(h(t)=te^{-t}u(t)\), which looks similar to the impulse response of Fig. 2.21, has a peak value of \(e^{-1}=0.3679\) at a time \(t_{0}=1\). According to Eq. (2.47) and using integration by parts, the system time constant is therefore

\[T_{h}=\frac{\int_{0}^{\infty}te^{-t}dt}{e^{-1}}=e^{1}\left(-te^{-t}\big{|}_{0} ^{\infty}+\int_{0}^{\infty}e^{-t}dt\right)=e^{1}\left(0-e^{-t}\big{|}_{0}^{ \infty}\right)=e^{1}(1)=2.7183\]

Thus,

\[T_{h}=2.7183\ \text{s},\qquad T_{r}=T_{h}=2.7183\ \text{s},\qquad\text{and} \qquad f_{c}=\frac{1}{T_{h}}=0.3679\ \text{Hz}\]

Due to its lowpass nature, this system will spread an input pulse of 1 second to an output with width

\[T_{y}=T_{x}+T_{h}=1+2.7183=3.7183\ \text{s}\]

To avoid interference between pulses at the output, the pulse transmission rate should be no more than the reciprocal of the output pulse width. That is,

\[\text{maximum pulse transmission rate}=\frac{1}{3.7183}=0.2689\ \text{pulse/s}\]

By narrowing the input pulses, the pulse transmission rate could increase up to \(f_{c}=0.3679\) pulse/s.

### 2.6-7 The Resonance Phenomenon

Finally, we come to the fascinating phenomenon of resonance. As we have already mentioned several times, this phenomenon is observed when the input signal is identical or is very close to a characteristic mode of the system. For the sake of simplicity and clarity, we consider a first-order system having only a single mode, \(e^{\lambda t}\). Let the impulse response of this system be1

Footnote 1: For convenience, we omit multiplying \(x(t)\) and \(h(t)\) by \(u(t)\). Throughout this discussion, we assume that they are causal.

\[h(t)=Ae^{\lambda t}\]

and let the input be

\[x(t)=e^{(\lambda-\epsilon)t}\]

The system response \(y(t)\) is then given by

\[y(t)=Ae^{\lambda t}*e^{(\lambda-\epsilon)t}\]From the convolution table we obtain

\[y(t)=\frac{A}{\epsilon}\Big{[}e^{\lambda t}-e^{(\lambda-\epsilon)t}\Big{]}=Ae^{ \lambda t}\left(\frac{1-e^{-\epsilon t}}{\epsilon}\right) \tag{2.49}\]

Now, as \(\epsilon\to 0\), both the numerator and the denominator of the term in the parentheses approach zero. Applying L'Hopital's rule to this term yields

\[\lim_{\epsilon\to 0}y(t)=Ate^{\lambda t}\]

Clearly, the response does not go to infinity as \(\epsilon\to 0\), but it acquires a factor \(t\), which approaches \(\infty\) as \(t\to\infty\). If \(\lambda\) has a negative real part (so that it lies in the LHP), \(e^{\lambda t}\) decays faster than \(t\) and \(y(t)\to 0\) as \(t\to\infty\). The resonance phenomenon in this case is present, but its manifestation is aborted by the signal's own exponential decay.

This discussion shows that _resonance is a cumulative phenomenon,_ not instantaneous. It builds up linearly with \(t\).+ When the mode decays exponentially, the signal decays too fast for resonance to counteract the decay; as a result, the signal vanishes before resonance has a chance to build it up. However, if the mode were to decay at a rate less than \(1/t\), we should see the resonance phenomenon clearly. This specific condition would be possible if Re \(\lambda\geq 0\). For instance, when Re \(\lambda=0\) so that \(\lambda\) lies on the imaginary axis of the complex plane (\(\lambda=jo\)), the output becomes

Footnote †: \({}^{\dagger}\) If the characteristic root in question repeats \(r\) times, resonance effect increases as \(t^{r-1}\). However, \(t^{r-1}e^{\lambda t}\to 0\) as \(t\to\infty\) for any value of \(r\), provided Re \(\lambda<0\) (\(\lambda\) in the LHP).

\[y(t)=Ate^{i\omega t}\]

Here, the response does go to infinity linearly with \(t\).

For a real system, if \(\lambda=jo\) is a root, \(\lambda^{*}=-jo\) must also be a root; the impulse response is of the form \(Ae^{i\omega t}+Ae^{-i\omega t}=2A\cos\omega t\). The response of this system to input \(A\cos\omega t\) is \(2A\cos\omega t*\cos\omega t\). The reader can show that this convolution contains a term of the form \(At\cos\omega t\). The resonance phenomenon is clearly visible. The system response to its characteristic mode increases linearly with time, eventually reaching \(\infty\), as indicated in Fig. 2.24.

Recall that when \(\lambda=jo\), the system is marginally stable. As we have indicated, the full effect of resonance cannot be seen for an asymptotically stable system; only in a marginally stable system does the resonance phenomenon boost the system's response to infinity when the system's input

Figure 2.24: Buildup of system response in resonance.

is a characteristic mode. But even in an asymptotically stable system, we see a manifestation of resonance if its characteristic roots are close to the imaginary axis so that Re \(\lambda\) is a small, negative value. We can show that when the characteristic roots of a system are \(\sigma\pm j\omega_{0}\), then the system response to the input \(e^{i\omega_{0}t}\) or the sinusoid \(\cos\omega_{0}t\) is very large for small \(\sigma\).1 The system response drops off rapidly as the input signal frequency moves away from \(\omega_{0}\). This frequency-selective behavior can be studied more profitably after an understanding of frequency-domain analysis has been acquired. For this reason we postpone full discussion of this subject until Ch. 4.

Footnote 1: This follows directly from Eq. (2.49) with \(\lambda=\sigma+j\omega_{0}\) and \(\epsilon=\sigma\).

### Importance of the Resonance Phenomenon

The resonance phenomenon is very important because it allows us to design frequency-selective systems by choosing their characteristic roots properly. Lowpass, bandpass, highpass, and bandstop filters are all examples of frequency-selective networks. In mechanical systems, the inadvertent presence of resonance can cause signals of such tremendous magnitude that the system may fall apart. A musical note (periodic vibrations) of proper frequency can shatter glass if the frequency is matched to the characteristic root of the glass, which acts as a mechanical system. Similarly, a company of soldiers marching in step across a bridge amounts to applying a periodic force to the bridge. If the frequency of this input force happens to be nearer to a characteristic root of the bridge, the bridge may respond (vibrate) violently and collapse, even though it would have been strong enough to carry many soldiers marching out of step. A case in point is the Tacoma Narrows Bridge failure of 1940. This bridge was opened to traffic in July 1940. Within four months of opening (on November 7, 1940), it collapsed in a mild gale, not because of the wind's brute force but because the frequencies of wind-generated vortices, which matched the natural frequencies (characteristic roots) of the bridge, caused resonance.

Because of the great damage that may occur, mechanical resonance is generally to be avoided, especially in structures or vibrating mechanisms. If an engine with periodic force (such as piston motion) is mounted on a platform, the platform with its mass and springs should be designed so that their characteristic roots are not close to the engine's frequency of vibration. Proper design of this platform can not only avoid resonance, but also attenuate vibrations if the system roots are placed far away from the frequency of vibration.

### 2.7 MATLAB: M-Files

M-files are stored sequences of MATLAB commands and help simplify complicated tasks. There are two types of M-file: script and function. Both types are simple text files and require a.m filename extension.

Although M-files can be created by using any text editor, MATLAB's built-in editor is the preferable choice because of its special features. As with any program, comments improve the readability of an M-file. Comments begin with the % character and continue through the end of the line.

An M-file is executed by simply typing the filename (without the.m extension). To execute, M-files need to be located in the current directory or any other directory in the MATLAB path. New directories are easily added to the MATLAB path by using the addpath command.

### 2.7-1 Script M-Files

Script files, the simplest type of M-file, consist of a series of MATLAB commands. Script files record and automate a series of steps, and they are easy to modify. To demonstrate the utility of a script file, consider the operational amplifier circuit shown in Fig. 2.25.

The system's characteristic modes define the circuit's behavior and provide insight regarding system behavior. Using ideal, infinite gain difference amplifier characteristics, we first derive the differential equation that relates output \(y(t)\) to input \(x(t)\). Kirchhoff's current law (KCL) at the node shared by \(R_{1}\) and \(R_{3}\) provides

\[\frac{x(t)-v(t)}{R_{3}}+\frac{y(t)-v(t)}{R_{2}}+\frac{0-v(t)}{R_{1}}-C_{2} \dot{v}(t)=0\]

KCL at the inverting input of the op amp gives

\[\frac{v(t)}{R_{1}}+C_{1}\dot{y}(t)=0\]

Combining and simplifying the KCL equations yield

\[\ddot{y}(t)+\frac{1}{C_{2}}\left\{\frac{1}{R_{1}}+\frac{1}{R_{2}}+\frac{1}{R_{ 3}}\right\}\dot{y}(t)+\frac{1}{R_{1}R_{2}C_{1}C_{2}}y(t)=-\frac{1}{R_{1}R_{3}C _{1}C_{2}}x(t)\]

which is the desired constant coefficient differential equation. Thus, the characteristic equation is given by

\[\dot{\lambda}^{2}+\frac{1}{C_{2}}\left\{\frac{1}{R_{1}}+\frac{1}{R_{2}}+\frac {1}{R_{3}}\right\}\dot{\lambda}+\frac{1}{R_{1}R_{2}C_{1}C_{2}}=(a_{0}\dot{ \lambda}^{2}+a_{1}\dot{\lambda}+a_{2})=0 \tag{2.50}\]

The roots \(\lambda_{1}\) and \(\lambda_{2}\) of Eq. (2.50) establish the nature of the characteristic modes \(e^{\lambda_{1}t}\) and \(e^{\lambda_{2}t}\).

As a first case, assign nominal component values of \(R_{1}=R_{2}=R_{3}=10\) k\(\Omega\) and \(C_{1}=C_{2}=1\) uF. A series of MATLAB commands allows convenient computation of the roots \(\lambda=[\lambda_{1};\lambda_{2}]\). Although \(\lambda\) can be determined using the quadratic equation, MATLAB's roots command is more convenient. The roots command requires an input vector that contains the polynomial coefficients in descending order. Even if a coefficient is zero, it must still be included in the vector.

Figure 2.25: Operation-amplifier circuit.

% CH2MP1.m:Chapter2,MATLABProgram1  %ScriptM-filedeterminescharacteristicrootsofop-ampcircuit.

 %Setcomponentvalues:  R = [1e4, 1e4, 1e4]; C = [1e-6, 1e-6];  %Determinecoefficientsforcharacteristicequation:  A = [1, (1/R(1)+1/R(2)+1/R(3))/C(2), 1/(R(1)*R(2)*C(1)*C(2))];  %Determinecharacteristicroots:  lambda=roots(A); A scriptfileiscreatedbyplacingthesecommandsinastfile,whichinthiscaseisnamedCH2MP1.m.Whilecommentlinesimproprogramclarity,theirremovaldoesnotaffectprogramfunctionality.Theprogramisexecutedbytyping

>>CH2MP1

Afterexecution,alltheresultingvariablesareavailableintheworkspace.Forexample,toviewthecharacteristicroots,type

>>lambda  lambda = -261.8034  -38.1966 Thus,thecharacteristicmodesaresimpledecayingexponentials:\(e^{-261.8034t}\)and\(e^{-38.1966t}\).

Scriptfilespermitsimpleorincrementalchanges,therebysavingsignificanteffort.Considerwhathappenswhencapacitor\(C_{1}\)ischangedfrom1.0\(\mu\)Fto1.0nF.ChangingCH2MP1.msothatC = [1e-9, 1e-6]allowscomputationofthenewcharacteristicroots:

>>CH2MP1 >>lambda  lambda = 1.0e+003*  -0.1500+3.1587i  -0.1500-3.1587i Perhapssurprisingly,thecharacteristicmodesarenowcomplexexponentialscapableofsupportingoscillations.Theimaginaryportionof\(\lambda\)dictatesanoscillationrateof3158.7rad/sorabout503Hz.Therealportiondictatestherateofdecay.Thetimeexpectedtoreducetheamplitudeto25%isapproximately\(t=\ln 0.25/\text{Re}(\lambda)\approx 0.01\)second.

### Function M-Files

Itis inconvenienttomodifyandsaveacscriptfileeachtimeachangeofparametersisdesired.FunctionM-filesprovideasensiblealternative.UnlikescriptM-files,functionM-filescanacceptinputargumentsaswellasreturnoutputs.FunctionstrulyextendtheMATLABlanguageinwaysthatscriptfilescannot.

Syntactically, a function M-file is identical to a script M-file except for the first line. The general form of the first line is

function [_output1,..., outputM_] = filename(_input1,..., inputM_)

For example, consider modification of CH2MP1.m to make function CH2MP2.m. Component values are passed to the function as two separate inputs: a length-3 vector of resistor values and a length-2 vector of capacitor values. The characteristic roots are returned as a \(2\times 1\) complex vector.

function [lambda] = CH2MP2(R,C) % CH2MP2.m : Chapter 2, MATLAB Program 2 % Function M-file finds characteristic roots of op-amp circuit. % INPUTS: R = length-3 vector of resistances % C = length-2 vector of capacitances % OUTPUS: lambda = characteristic roots

% Determine coefficients for characteristic equation: A = [1, (1/R(1)+1/R(2)+1/R(3))/C(2), 1/(R(1)*R(2)*C(1)*C(2))]; % Determine characteristic roots: lambda = roots(A);

As with script M-files, function M-files execute by typing the name at the command prompt. However, inputs must also be included. For example, CH2MP2 easily confirms the oscillatory modes of the preceding example.

>> lambda = CH2MP2([1e4, 1e4, 1e4],[1e-9, 1e-6])  lambda = 1.0e+003 *  -0.1500 + 3.1587i  -0.1500 - 3.1587i

Although scripts and functions have similarities, they also have distinct differences that are worth pointing out. Scripts operate on workspace data; either functions must be supplied data through inputs or they must create their own data. Unless passed as an output, variables and data created by functions remain local to the function; variables or data generated by scripts are global and are added to the workspace. To emphasize this point, consider polynomial coefficient vector A, which is created and used in both CH2MP1.m and CH2MP2.m. Following execution of function CH2MP2, the variable A is not added to the workspace. Following execution of script CH2MP1, however, A is available in the workspace. Recall, the workspace is easily viewed by typing either who or whos.

### For-Loops

Real resistors and capacitors never exactly equal their nominal values. Suppose that the circuit components are measured as \(R_{1}=10.322\) k\(\Omega\), \(R_{2}=9.952\) k\(\Omega\), \(R_{3}=10.115\) k\(\Omega\), \(C_{1}=1.120\) nF, and \(C_{2}=1.320\) nF. These values are consistent with the 10 and 25% tolerance resistor and capacitor values commonly and readily available. CH2MP2.m uses these component values to calculate the new values of \(\lambda\).

* >> lambda = CH2MP2([10322,9592,10115],[1.12e-9, 1.32e-6]) lambda = 1.0e+003
* -0.1136 + 2.6113i  -0.1136 - 2.6113i Now the natural modes oscillate at 2611.3 rad/s or about 416 Hz. Decay to 25% amplitude is expected in \(t=\ln 0.25/(-113.6)\approx 0.012\) second. These values, which differ significantly from the nominal values of 503 Hz and \(t\approx 0.01\) second, warrant a more formal investigation of the effect of component variations on the locations of the characteristic roots.

It is sensible to look at three values for each component: the nominal value, a low value, and a high value. Low and high values are based on component tolerances. For example, a 10% 1 k\(\Omega\) resistor could have an expected low value of \(1000(1-0.1)=900\)\(\Omega\) and an expected high value of \(1000(1+0.1)=1100\)\(\Omega\). For the five passive components in the design, \(3^{5}=243\) permutations are possible.

Using either CH2MP1.m or CH2MP2.m to solve each of the 243 cases would be very tedious and boring. For-loops help automate repetitive tasks such as this. In MATLAB, the general structure of a for statement is

 for _variable = expression, statement,..., statement,_ end Five nested for-loops, one for each passive component, are required for the present example.

 % CH2MP3.m : Chapter 2, MATLAB Program 3  % Script M-file determines characteristic roots over a range of component values.

 % Pre-allocate memory for all computed roots:  lambda = zeros(2,243);  % Initialize index to identify each permutation:  p=0;  for R1 = 1e4*[0.9,1.0,1.1],  for R2 = 1e4*[0.9,1.0,1.1],  for R3 = 1e4*[0.9,1.0,1.1],  for C1 = 1e-9*[0.75,1.0,1.25],  for C2 = 1e-6*[0.75,1.0,1.25],  p = p+1;  lambda(:,p) = CH2MP2([R1 R2 R3],[C1 C2]);  end  end  end end

 plot(real(lambda(:))),imag(lambda(:)),'kx',...  real(lambda(:,1)),imag(lambda(:,1)),'kv',...  real(lambda(:,end)),imag(lambda(:,end)),'k-')  xlabel('Real'),ylabel('Imaginary')  legend('Char. Roots','Min. Val. Roots','Max. Val. Roots','Location','West');The command lambda = zeros(2,243) preallocates a \(2\times 243\) array to store the computed roots. When necessary, MATLAB performs dynamic memory allocation, so this command is not strictly necessary. However, preallocation significantly improves script execution speed. Notice also that it would be nearly useless to call script CH2MP1 from within the nested loop; script file parameters cannot be changed during execution.

The plot instruction is quite long. Long commands can be broken across several lines by terminating intermediate lines with three dots (...). The three dots tell MATLAB to continue the present command to the next line. Black x's locate roots of each permutation. The command lambda(:) vectorizes the \(2\times 243\) matrix lambda into a \(486\times 1\) vector. This is necessary in this case to ensure that a proper legend is generated. Because of loop order, permutation \(p=1\) corresponds to the case of all components at the smallest values and permutation \(p=243\) corresponds to the case of all components at the largest values. This information is used to separately highlight the minimum and maximum cases using down-triangles (\(\bigtriangledown\)) and up-triangles (\(\bigtriangleup\)), respectively. In addition to terminating each for loop, end is used to indicate the final index along a particular dimension, which eliminates the need to remember the particular size of a variable. An overloaded function, such as end, serves multiple uses and is typically interpreted based on context.

The graphical results provided by CH2MP3 are shown in Fig. 26. Between extremes, root oscillations vary from 365 to 745 Hz and decay times to 25% amplitude vary from 6.2 to 12.7 ms. Clearly, this circuit's behavior is quite sensitive to ordinary component variations.

### Graphical Understanding of Convolution

MATLAB graphics effectively illustrate the convolution process. Consider the case of \(y(t)=x(t)*h(t)\), where \(x(t)=1.5\sin\left(\pi t\right)(u(t)-u(t-1))\) and \(h(t)=1.5(u(t)-u(t-1.5))-u(t-2)+u(t-2.5)\). Program CH2MP4 steps through the convolution over the time interval (\(-0.25\leq t\leq 3.75\)).

Figure 26: Effect of component values on characteristic root locations.

% CH2MP4.m: Chapter 2, MATLAB Program 4  % Script M-file graphically demonstrates the convolution process.

 figure(1) % Create figure window and make visible on screen  u = @(t) 1.0*(t=0);  x = @(t) 1.5*sin(pi*t).*(u(t)-u(t-1));  h = @(t) 1.5*(u(t)-u(t-1.5))-u(t-2)+u(t-2.5);  dtau = 0.005; tau = -1:dtau:4;  ti = 0; tvec = -.25::1:3.75;  y = NaN*zeros(1,length(tvec)); % Pre-allocate memory  for t = tvec,  ti = ti+1; % Time index  xh = x(t-tau).*h(tau); lxh = length(xh);  y(ti) = sum(xh.*dtau); % Trapezoidal approximation of convolution integral  subplot(2,1,1),plot(tau,h(tau),'k-',tau,x(t-tau),'k-',t,0,'ok');  axis([tau(1) tau(end) -2.0 2.5]);  patch([tau(1:end-1);tau(1:end-1);tau(2:end);tau(2:end)],...  [zeros(1,lxh-1);xh(1:end-1);xh(2:end);zeros(1,lxh-1)],...  [.8.8.8],'edgecolor','none');  xlabel('\tau'); title('h(\tau)[solid], x(t-\tau)[dashed], h(\tau)x(t-\tau)[gray]');  c = get(gca,'children'); set(gca,'children',[c(2);c(3);c(4);c(1)]);  subplot(2,1,2),plot(tvec,y,'k',tvec(ti),y(ti),'ok');  xlabel('t'); ylabel('y(t) = \(\backslash\)int h(\tau)x(t-\tau)d\tau');  axis([tau(1) tau(end) -1.0 2.0]); grid;  drawnow;  end At each step, the program plots \(h(\tau)\), \(x(t-\tau)\), and shades the area \(h(\tau)x(t-\tau)\) gray. This gray area, which reflects the integral of \(h(\tau)x(t-\tau)\), is also the desired result, \(y(t)\). Figures 2.27, 2.28, and 2.29 display the convolution process at times \(t\) of 0.75, 2.25, and 2.85 seconds, respectively. These figures help illustrate how the regions of integration change with time. Figure 2.27 has limits of integration from 0 to \((t=0.75)\). Figure 2.28 has two regions of integration, with limits \((t-1=1.25)\) to 1.5 and 2.0 to \((t=2.25)\). The last plot, Fig. 2.29, has limits from 2.0 to 2.5.

Several comments regarding CH2MP4 are in order. The command figure(1) opens the first figure window and, more important, makes sure it is visible. Anonymous functions are used to represent the functions \(u(t)\), \(x(t)\), and \(h(t)\). NaN, standing for not-a-number, usually results from operations such as \(0/0\) or \(\infty-\infty\). MATLAB refuses to plot NaN values, so preallocating \(y(t)\) with NaNs ensures that MATLAB displays only values of \(y(t)\) that have been computed. As its name suggests, length returns the length of the input vector. The subplot(a,b,c) command partitions the current figure window into an a-by-b matrix of axes and selects axes c for use. Subplots facilitate graphical comparison by allowing multiple axes in a single figure window. The patch command is used to create the gray-shaded area for \(h(\tau)x(t-\tau)\). In CH2MP4, the get and set commands are used to reorder plot objects so that the gray area does not obscure other lines. Details of the patch, get, and set commands, as used in CH2MP4, are somewhat advanced and are not pursued here.2 MATLAB also prints most Greek letters if the Greek name is preceded by a backslash (\(\backslash\)) character. For example, \tautau in the xlabel command produces the symbol \(\tau\) in the plot's axis label. Similarly, an integral sign is produced by \(\backslash\)int. Finally, the drawn command forces MATLAB to update the graphics window for each loop iteration. Although slow, this creates an animation-like effect. Replacing drawnow with the pause command allows users to manually step through the convolution process. The pause command still forces the graphics window to update, but the program will not continue until a key is pressed.

Figure 28: Graphical convolution at step \(t=2.25\) seconds.

Figure 27: Graphical convolution at step \(t=0.75\) second.

## Chapter 2 Time-domain analysis of continuous-time systems

### 2.8 Appendix: Determining the Impulse Response

In Eq. (2.13), we showed that for an LTIC system \(S\) specified by Eq. (2.11), the unit impulse response \(h(t)\) can be expressed as

\[h(t)=b_{0}\delta(t)+\text{characteristic modes} \tag{2.51}\]

To determine the characteristic mode terms in Eq. (2.51), let us consider a system \(S_{0}\) whose input \(x(t)\) and the corresponding output \(w(t)\) are related by

\[Q(D)w(t)=x(t) \tag{2.52}\]

Observe that both the systems \(S\) and \(S_{0}\) have the same characteristic polynomial; namely, \(Q(\lambda)\), and, consequently, the same characteristic modes. Moreover, \(S_{0}\) is the same as \(S\) with \(P(D)=1\), that is, \(b_{0}=0\). Therefore, according to Eq. (2.51), the impulse response of \(S_{0}\) consists of characteristic mode terms only without an impulse at \(t=0\). Let us denote this impulse response of \(S_{0}\) by \(y_{n}(t)\). Observe that \(y_{n}(t)\) consists of characteristic modes of \(S\) and therefore may be viewed as a zero-input response of \(S\). Now \(y_{n}(t)\) is the response of \(S_{0}\) to input \(\delta(t)\). Therefore, according to Eq. (2.52),

\[Q(D)y_{n}(t)=\delta(t)\]

or

\[(D^{N}+a_{1}D^{N-1}+\cdot\cdot\cdot+a_{N1}D+a_{N})y_{n}(t)=\delta(t)\]

or

\[y_{n}^{(N)}(t)+a_{1}y_{n}^{(N-1)}(t)+\cdot\cdot\cdot+a_{N-1}y_{n}^{(1)}(t)+a_{ N}y_{n}(t)=\delta(t)\]

Figure 2.29: Graphical convolution at step \(t=2.85\) seconds.

where \(y_{n}^{(k)}(t)\) represents the \(k\)th derivative of \(y_{n}(t)\). The right-hand side contains a single impulse term, \(\delta(t)\). This is possible only if \(y_{n}^{(N-1)}(t)\) has a unit jump discontinuity at \(t=0\), so that \(y_{n}^{(N)}(t)=\delta(t)\). Moreover, the lower-order terms cannot have any jump discontinuity because this would mean the presence of the derivatives of \(\delta(t)\). Therefore \(y_{n}(0)=y_{n}^{(1)}(0)=\cdot\cdot\cdot=y_{n}^{(N-2)}(0)=0\) (no discontinuity at \(t=0\)), and the \(N\) initial conditions on \(y_{n}(t)\) are

\[y_{n}(0)=y_{n}^{(1)}(0)=\cdot\cdot\cdot=y_{n}^{(N-2)}(0)=0\qquad\text{and} \qquad y_{n}^{(N-1)}(0)=1 \tag{53}\]

This discussion means that \(y_{n}(t)\) is the zero-input response of the system \(S\) subject to initial conditions [Eq. (53)].

We now show that for the same input \(x(t)\) to both systems, \(S\) and \(S_{0}\), their respective outputs \(y(t)\) and \(w(t)\) are related by

\[y(t)=P(D)w(t) \tag{54}\]

To prove this result, we operate on both sides of Eq. (52) by \(P(D)\) to obtain

\[Q(D)P(D)w(t)=P(D)x(t)\]

Comparison of this equation with Eq. (2) leads immediately to Eq. (54).

Now if the input \(x(t)=\delta(t)\), the output of \(S_{0}\) is \(y_{n}(t)\), and the output of \(S\), according to Eq. (54), is \(P(D)y_{n}(t)\). This output is \(h(t)\), the unit impulse response of \(S\). Note, however, that because it is an impulse response of a causal system \(S_{0}\), the function \(y_{n}(t)\) is causal. To incorporate this fact we must represent this function as \(y_{n}(t)u(t)\). Now it follows that \(h(t)\), the unit impulse response of the system \(S\), is given by

\[h(t)=P(D)[y_{n}(t)u(t)] \tag{55}\]

where \(y_{n}(t)\) is a linear combination of the characteristic modes of the system subject to initial conditions (53).

The right-hand side of Eq. (55) is a linear combination of the derivatives of \(y_{n}(t)u(t)\). Evaluating these derivatives is clumsy and inconvenient because of the presence of \(u(t)\). The derivatives will generate an impulse and its derivatives at the origin. Fortunately when \(M\leq N\) [Eq. (11)], we can avoid this difficulty by using the observation in Eq. (51), which asserts that at \(t=0\) (the origin), \(h(t)=b_{0}\delta(t)\). Therefore, we need not bother to find \(h(t)\) at the origin. This simplification means that instead of deriving \(P(D)[y_{n}(t)u(t)]\), we can derive \(P(D)y_{n}(t)\) and add to it the term \(b_{0}\delta(t)\) so that

\[h(t) =b_{0}\delta(t)+P(D)y_{n}(t)\qquad t\geq 0\] \[=b_{0}\delta(t)+[P(D)y_{n}(t)]u(t)\]

This expression is valid when \(M\leq N\) [the form given in Eq. (11)]. When \(M>N\), Eq. (55) should be used.

## 9 Summary

This chapter discusses time-domain analysis of LTIC systems. The total response of a linear system is a sum of the zero-input response and zero-state response. The zero-input response is the system response generated only by the internal conditions (initial conditions) of the system, assuming that the external input is zero; hence the adjective "zero-input." The zero-state response is the system response generated by the external input, assuming that all initial conditions are zero, that is, when the system is in zero state.

Every system can sustain certain forms of response on its own with no external input (zero input). These forms are intrinsic characteristics of the system; that is, they do not depend on any external input. For this reason they are called characteristic modes of the system. Needless to say, the zero-input response is made up of characteristic modes chosen in a combination required to satisfy the initial conditions of the system. For an \(N\)th-order system, there are \(N\) distinct modes.

The unit impulse function is an idealized mathematical model of a signal that cannot be generated in practice.1 Nevertheless, introduction of such a signal as an intermediary is very helpful in analysis of signals and systems. The unit impulse response of a system is a combination of the characteristic modes of the system2 because the impulse \(\delta(t)=0\) for \(t>0\). Therefore, the system response for \(t>0\) must necessarily be a zero-input response, which, as seen earlier, is a combination of characteristic modes.

Footnote 1: However, it can be closely approximated by a narrow pulse of unit area and having a width that is much smaller than the time constant of an LTIC system in which it is used.

Footnote 2: There is the possibility of an impulse in addition to the characteristic modes.

The zero-state response (response due to external input) of a linear system can be obtained by breaking the input into simpler components and then adding the responses to all the components. In this chapter we represent an arbitrary input \(x(t)\) as a sum of narrow rectangular pulses [staircase approximation of \(x(t)\)]. In the limit as the pulse width \(\to 0\), the rectangular pulse components approach impulses. Knowing the impulse response of the system, we can find the system response to all the impulse components and add them to yield the system response to the input \(x(t)\). The sum of the responses to the impulse components is in the form of an integral, known as the convolution integral. The system response is obtained as the convolution of the input \(x(t)\) with the system's impulse response \(h(t)\). Therefore, the knowledge of the system's impulse response allows us to determine the system response to any arbitrary input.

LTIC systems have a very special relationship to the everlasting exponential signal \(e^{st}\) because the response of an LTIC system to such an input signal is the same signal within a multiplicative constant. The response of an LTIC system to the everlasting exponential input \(e^{st}\) is \(H(s)e^{st}\), where \(H(s)\) is the transfer function of the system.

If every bounded input results in a bounded output, the system is stable in the bounded-input/bounded-output (BIBO) sense. An LTIC system is BIBO-stable if and only if its impulse response is absolutely integrable. Otherwise, it is BIBO-unstable. BIBO stability is a stability seen from external terminals of the system. Hence, it is also called external stability or zero-state stability.

In contrast, internal stability (or the zero-input stability) examines the system stability from inside. When some initial conditions are applied to a system in zero state, then, if the system eventually returns to zero state, the system is said to be stable in the asymptotic or Lyapunov sense. If the system's response increases without bound, it is unstable. If the system does not go to zero state and the response does not increase indefinitely, the system is marginally stable. The internal stability criterion, in terms of the location of a system's characteristic roots, can be summarized as follows:1. An LTIC system is asymptotically stable if, and only if, all the characteristic roots are in the LHP. The roots may be repeated or unrepeated.
2. An LTIC system is unstable if, and only if, either one or both of the following conditions exist: (i) at least one root is in the RHP; (ii) there are repeated roots on the imaginary axis.
3. An LTIC system is marginally stable if, and only if, there are no roots in the RHP, and there are some unrepeated roots on the imaginary axis.

It is possible for a system to be externally (BIBO) stable but internally unstable. When a system is controllable and observable, its external and internal descriptions are equivalent. Hence, external (BIBO) and internal (asymptotic) stabilities are equivalent and provide the same information. Such a BIBO-stable system is also asymptotically stable, and vice versa. Similarly, a BIBO-unstable system is either marginally stable or asymptotically unstable system.

The characteristic behavior of a system is extremely important because it determines not only the system response to internal conditions (zero-input behavior), but also the system response to external inputs (zero-state behavior) and the system stability. The system response to external inputs is determined by the impulse response, which itself is made up of characteristic modes. The width of the impulse response is called the time constant of the system, which indicates how fast the system can respond to an input. The time constant plays an important role in determining such diverse system behaviors as the response time and filtering properties of the system, dispersion of pulses, and the rate of pulse transmission through the system.

## References

* [1] Lathi, B. P., _Signals and Systems_. Berkeley-Cambridge Press, Carmichael, CA, 1987.
* [2] Mason, S. J., _Electronic Circuits, Signals, and Systems_. Wiley, New York, 1960.
* [3] Kailath, T., _Linear System_. Prentice-Hall, Englewood Cliffs, NJ, 1980.
* [4] Lathi, B. P., _Modern Digital and Analog Communication Systems,_ 3rd ed. Oxford University Press, New York, 1998.

## References

* [1] Lathi, B. P., _Signals and Systems_. Berkeley-Cambridge Press, Carmichael, CA, 1987.
* [2] Mason, S. J., _Electronic Circuits, Signals, and Systems_. Wiley, New York, 1960.
* [3] Kailath, T., _Linear System_. Prentice-Hall, Englewood Cliffs, NJ, 1980.
* [4] Lathi, B. P., _Modern Digital and Analog Communication Systems,_ 3rd ed. Oxford University Press, New York, 1998.

## References

* [1] Lathi, B. P., _Signals and Systems_. Berkeley-Cambridge Press, Carmichael, CA, 1987.
* [2] Mason, S. J., _Electronic Circuits, Signals, and Systems_. Wiley, New York, 1960.
* [3] Kailath, T., _Linear System_. Prentice-Hall, Englewood Cliffs, NJ, 1980.
* [4] Lathi, B. P., _Modern Digital and Analog Communication Systems,_ 3rd ed. Oxford University Press, New York, 1998.

## References

* [1] Lathi, B. P., _Signals and Systems_. Berkeley-Cambridge Press, Carmichael, CA, 1987.
* [2] Mason, S. J., _Electronic Circuits, Signals, and Systems_. Wiley, New York, 1960.
* [3] Kailath, T., _Linear System_. Prentice-Hall, Englewood Cliffs, NJ, 1980.
* [4] Lathi, B. P., _Modern Digital and Analog Communication Systems,_ 3rd ed. Oxford University Press, New York, 1998.

## References

* [1] Lathi, B. P., _Signals and Systems_. Berkeley-Cambridge Press, Carmichael, CA, 1987.
* [2] Mason, S. J., _Electronic Circuits, Signals, and Systems_. Wiley, New York, 1960.
* [3] Kailath, T., _Linear System_. Prentice-Hall, Englewood Cliffs, NJ, 1980.
* [4] Lathi, B. P., _Modern Digital and Analog Communication Systems,_ 3rd ed. Oxford University Press, New York, 1998.

## References

* [1] Lathi, B. P., _Signals and Systems_. Berkeley-Cambridge Press, Carmichael, CA, 1987.
* [2] Mason, S. J., _Electronic Circuits, Signals, and Systems_. Wiley, New York, 1960.
* [3] Kailath, T., _Linear System_. Prentice-Hall, Englewood Cliffs, NJ, 1980.
* [4] Lathi, B. P., _Modern Digital and Analog Communication Systems,_ 3rd ed. Oxford University Press, New York, 1998.

## References

* [1] Lathi, B. P., _Signals and Systems_. Berkeley-Cambridge Press, Carmichael, CA, 1987.
* [2] Mason, S. J., _Electronic Circuits, Signals, and Systems_. Wiley, New York, 1960.
* [3] Kailath, T., _Linear System_. Prentice-Hall, Englewood Cliffs, NJ, 1980.
* [4] Lathi, B. P., _Modern Digital and Analog Communication Systems,_ 3rd ed. Oxford University Press, New York, 1998.

## References

* [1] Lathi, B. P., _Signals and Systems_. Berkeley-Cambridge Press, Carmichael, CA, 1987.
* [2] Mason, S. J., _Electronic Circuits, Signals, and Systems_. Wiley, New York, 1960.
* [3] Kailath, T., _Linear System_. Prentice-Hall, Englewood Cliffs, NJ, 1980.
* [4] Lathi, B. P., _Modern Digital and Analog Communication Systems,_ 3rd ed. Oxford University Press, New York, 1998.

## References

* [1] Lathi, B. P., _Signals and Systems_. Berkeley-Cambridge Press, Carmichael, CA, 1987.
* [2] Mason, S. J., _Electronic Circuits, Signals, and Systems_. Wiley, New York, 1960.
* [3] Kailath, T., _Linear System_. Prentice-Hall, Englewood Cliffs, NJ, 1980.
* [4] Lathi, B. P., _Modern Digital and Analog Communication Systems,_ 3rd ed. Oxford University Press, New York, 1998.

## References

* [1] Lathi, B. P., _Signals and Systems_. Berkeley-Cambridge Press, Carmichael, CA, 1987.
* [2] Mason, S. J., _Electronic Circuits, Signals, and Systems_. Wiley, New York, 1960.
* [3] Kailath, T., _Linear System_. Prentice-Hall, Englewood Cliffs, NJ, 1980.
* [4] Lathi, B. P., _Modern Digital and Analog Communication Systems,_ 3rd ed. Oxford University Press, New York, 1998.

## References

* [1] Lathi, B. P., _Signals and Systems_. Berkeley-Cambridge Press, Carmichael, CA, 1987.
* [2] Mason, S. J., _Electronic Circuits, Signals, and Systems_. Wiley, New York, 1960.
* [3] Kailath, T., _Linear System_. Prentice-Hall, Englewood Cliffs, NJ, 1980.
* [4] Lathi, B. P., _Modern Digital and Analog Communication Systems,_ 3rd ed. Oxford University Press, New York, 1998.

## References

* [1] Lathi, B. P., _Signals and Systems_. Berkeley-Cambridge Press, Carmichael, CA, 1987.
* [2] Mason, S. J., _Electronic Circuits, Signals, and Systems_. Wiley, New York, 1960.
* [3] Kailath, T., _Linear System_. Prentice-Hall, Englewood Cliffs, NJ, 1980.
* [4] Lathi, B. P., _Modern Digital and Analog Communication Systems,_ 3rd ed. Oxford University Press, New York, 1998.

## References

* [1] Lathi, B. P., _Signals and Systems_. Berkeley-Cambridge Press, Carmichael, CA, 1987.
* [2] Mason, S. J., _Electronic Circuits, Signals, and Systems_. Wiley, New York, 1960.
* [3] Kailath, T., _Linear System_. Prentice-Hall, Englewood Cliffs, NJ, 1980.
* [4] Lathi, B. P., _Modern Digital and Analog Communication Systems,_ 3rd ed. Oxford University Press, New York, 1998.

## References

* [1] Lathi, B. P., _Signals and Systems_. Berkeley-Cambridge Press, Carmichael, CA, 1987.
* [2] Mason, S. J., _Electronic Circuits, Signals, and Systems_. Wiley, New York, 1960.
* [3] Kailath, T., _Linear System_. Prentice-Hall, Englewood Cliffs, NJ, 1980.
* [4] Lathi, B. P., _Modern Digital and Analog Communication Systems,_ 3rd ed. Oxford University Press, New York, 1998.

## References

* [1] Lathi, B. P., _Signals and Systems_. Berkeley-Cambridge Press, Carmichael, CA, 1987.
* [2] Mason, S. J., _Electronic Circuits, Signals, and Systems_. Wiley, New York, 1960.
* [3] Kailath, T., _Linear System_. Prentice-Hall, Englewood Cliffs, NJ, 1980.
* [4] Lathi, B. P., _Modern Digital and Analog Communication Systems,_ 3rd ed. Oxford University Press, New York, 1998.

## References

* [2] Lathi, B. P., _Signals and Systems_. Berkeley-Cambridge Press, Carmichael, CA, 1987.
* [3] Mason, S. J., _Electronic Circuits, Signals, and Systems_. Wiley, New York, 1960.
* [4] Kailath, T., _Linear System_. Prentice-Hall, Englewood Cliffs, NJ, 1980.
* [5] Lathi, B. P., _Modern Digital and Analog Communication Systems,_ 3rd ed. Oxford University Press, New York, 1998.

## References

* [