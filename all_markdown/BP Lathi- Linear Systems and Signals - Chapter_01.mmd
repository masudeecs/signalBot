## Signals and Systems

In this chapter we shall discuss basic aspects of signals and systems. We shall also introduce fundamental concepts and qualitative explanations of the hows and whys of systems theory, thus building a solid foundation for understanding the quantitative analysis in the remainder of the book. For simplicity, the focus of this chapter is on continuous-time signals and systems. Chapter 3 presents the same ideas for discrete-time signals and systems.

### Signals

A _signal_ is a set of data or information. Examples include a telephone or a television signal, monthly sales of a corporation, or daily closing prices of a stock market (e.g., the Dow Jones averages). In all these examples, the signals are functions of the independent variable _time_. This is not always the case, however. When an electrical charge is distributed over a body, for instance, the signal is the charge density, a function of _space_ rather than time. In this book we deal almost exclusively with signals that are functions of time. The discussion, however, applies equally well to other independent variables.

### Systems

Signals may be processed further by _systems,_ which may modify them or extract additional information from them. For example, an anti-aircraft gun operator may want to know the future location of a hostile moving target that is being tracked by his radar. Knowing the radar signal, he knows the past location and velocity of the target. By properly processing the radar signal (the input), he can approximately estimate the future location of the target. Thus, a system is an entity that _processes_ a set of signals (_inputs_) to yield another set of signals (_outputs_). A system may be made up of physical components, as in electrical, mechanical, or hydraulic systems (hardware realization), or it may be an algorithm that computes an output from an input signal (software realization).

### Size of a Signal

The size of any entity is a number that indicates the largeness or strength of that entity. Generally speaking, the signal amplitude varies with time. How can a signal that exists over a certain timeinterval with varying amplitude be measured by one number that will indicate the signal size or signal strength? Such a measure must consider not only the signal amplitude, but also its duration. For instance, if we are to devise a single number \(V\) as a measure of the size of a human being, we must consider not only his or her width (girth), but also the height. If we make a simplifying assumption that the shape of a person is a cylinder of variable radius \(r\) (which varies with the height \(h\)), then one possible measure of the size of a person of height \(H\) is the person's volume \(V\), given by

\[V=\pi\int_{0}^{H}r^{2}(h)\,dh\]

#### Signal Energy

Arguing in this manner, we may consider the area under a signal \(x(t)\) as a possible measure of its size, because it takes account not only of the amplitude but also of the duration. However, this will be a defective measure because even for a large signal \(x(t)\), its positive and negative areas could cancel each other, indicating a signal of small size. This difficulty can be corrected by defining the signal size as the area under \(|x(t)|^{2}\), which is always positive. We call this measure the _signal energy_\(E_{x}\), defined as

\[E_{x}=\int_{-\infty}^{\infty}|x(t)|^{2}\,dt \tag{1}\]

This definition simplifies for a real-valued signal \(x(t)\) to \(E_{x}=\int_{-\infty}^{\infty}x^{2}(t)\,dt\). There are also other possible measures of signal size, such as the area under \(|x(t)|\). The energy measure, however, is not only more tractable mathematically but is also more meaningful (as shown later) in the sense that it is indicative of the energy that can be extracted from the signal.

#### Signal Power

Signal energy must be finite for it to be a meaningful measure of signal size. A necessary condition for the energy to be finite is that the signal amplitude \(\to 0\) as \(|t|\to\infty\) (Fig. 1a). Otherwise the integral in Eq. (1) will not converge.

When the amplitude of \(x(t)\) does not \(\to 0\) as \(|t|\to\infty\) (Fig. 1b), the signal energy is infinite. A more meaningful measure of the signal size in such a case would be the time average of the energy, if it exists. This measure is called the _power_ of the signal. For a signal \(x(t)\), we define its power \(P_{x}\) as

\[P_{x}=\lim_{T\to\infty}\frac{1}{T}\int_{-T/2}^{T/2}|x(t)|^{2}\,dt \tag{2}\]

This definition simplifies for a real-valued signal \(x(t)\) to \(P_{x}=\lim_{T\to\infty}\frac{1}{T}\int_{-T/2}^{T/2}x^{2}(t)\,dt\). Observe that the signal power \(P_{x}\) is the time average (mean) of the signal magnitude squared, that is, the _mean-square_ value of \(|x(t)|\). Indeed, the square root of \(P_{x}\) is the familiar _rms_ (root-mean-square) value of \(x(t)\).

Generally, the mean of an entity averaged over a large time interval approaching infinity exists if the entity either is periodic or has a statistical regularity. If such a condition is not satisfied, the average may not exist. For instance, a ramp signal \(x(t)=t\) increases indefinitely as \(|t|\to\infty\), and neither the energy nor the power exists for this signal. However, the unit step function, which is not periodic nor has statistical regularity, does have a finite power.

When \(x(t)\) is periodic, \(|x(t)|^{2}\) is also periodic. Hence, the power of \(x(t)\) can be computed from Eq. (1.2) by averaging \(|x(t)|^{2}\) over one period.

#### 1.3.3 Comments.

The signal energy as defined in Eq. (1.1) does not indicate the actual energy (in the conventional sense) of the signal because the signal energy depends not only on the signal, but also on the load. It can, however, be interpreted as the energy dissipated in a normalized load of a 1 ohm resistor if a voltage \(x(t)\) were to be applied across the 1 ohm resistor [or if a current \(x(t)\) were to be passed through the 1 ohm resistor]. The measure of "energy" is therefore indicative of the energy capability of the signal, not the actual energy. For this reason the concepts of conservation of energy should not be applied to this "signal energy." Parallel observation applies to "signal power" defined in Eq. (1.2). These measures are but convenient indicators of the signal size, which prove useful in many applications. For instance, if we approximate a signal \(x(t)\) by another signal \(g(t)\), the error in the approximation is \(e(t)=x(t)-g(t)\). The energy (or power) of \(e(t)\) is a convenient indicator of the goodness of the approximation. It provides us with a quantitative measure of determining the closeness of the approximation. In communication systems, during transmission over a channel, message signals are corrupted by unwanted signals (noise). The quality of the received signal is judged by the relative sizes of the desired signal and the unwanted signal (noise). In this case the ratio of the message signal and noise signal powers (signal-to-noise power ratio) is a good indication of the received signal quality.

#### 1.3.4 Units of Energy and Power.

Equation (1.1) is not correct dimensionally. This is because here we are using the term _energy_ not in its conventional sense, but to indicate the signal size. The same observation applies to Eq. (1.2) for power. The units of energy and power, as defined here, depend on the nature of the signal \(x(t)\). If \(x(t)\) is a voltage signal, its energy \(E_{x}\) has units of volts squared-seconds (V\({}^{2}\) s), and its power \(P_{x}\) has units of volts squared. If \(x(t)\) is a current signal, these units will be amperes squared-seconds (A\({}^{2}\) s) and amperes squared, respectively.

Figure 1.1: Examples of signals: **(a)** a signal with finite energy and **(b)** a signal with finite power.

In Fig. 1.2a, the signal amplitude \(\to 0\) as \(|t|\to\infty\). Therefore the suitable measure for this signal is its energy \(E_{x}\) given by

\[E_{x}=\int_{-\infty}^{\infty}|x(t)|^{2}\,dt=\int_{-1}^{0}(2)^{2}\,dt+\int_{0}^{ \infty}4e^{-t}\,dt=4+4=8\]

In Fig. 1.2b, the signal magnitude does not \(\to 0\) as \(|t|\to\infty\). However, it is periodic, and therefore its power exists. We can use Eq. (1.2) to determine its power. We can simplify the procedure for periodic signals by observing that a periodic signal repeats regularly each period (2 seconds in this case). Therefore, averaging \(|x(t)|^{2}\) over an infinitely large interval is identical to averaging this quantity over one period (2 seconds in this case). Thus

\[P_{x}=\tfrac{1}{2}\int_{-1}^{1}|x(t)|^{2}\,dt=\tfrac{1}{2}\int_{-1}^{1}t^{2}\, dt=\tfrac{1}{3}\]

Recall that the signal power is the square of its rms value. Therefore, the rms value of this signal is \(1/\sqrt{3}\).

Figure 1.2: Signals for Ex. 1.1

## Chapter 1 Signals and Systems

### 1.1 Determining Power and RMS Value

The power and the rms value of

\[\begin{split}&\text{(a) This is a periodic signal with period $T_{0}=2\pi/\omega_{0}$. The suitable measure of this signal is its power. Because it is a periodic signal, we may compute its power by averaging its energy over one period \(T_{0}=2\pi/\omega_{0}\). However, for the sake of demonstration, we shall use Eq. (1.2) to solve this problem by averaging over an infinitely large time interval.

\[\begin{split} P_{x}&=\lim_{T\to\infty}\frac{1}{T} \int_{-T/2}^{T/2}C^{2}\cos^{2}\left(\omega_{0}t+\theta\right)dt=\lim_{T\to \infty}\frac{C^{2}}{2T}\int_{-T/2}^{T/2}\left[1+\cos\left(2\omega_{0}t+2\theta \right)\right]dt\\ &=\lim_{T\to\infty}\frac{C^{2}}{2T}\int_{-T/2}^{T/2}dt+\lim_{T \to\infty}\frac{C^{2}}{2T}\int_{-T/2}^{T/2}\cos\left(2\omega_{0}t+2\theta \right)dt\end{split}\]

The first term on the right-hand side is equal to \(C^{2}/2\). The second term, however, is zero because the integral appearing in this term represents the area under a sinusoid over a very large time interval \(T\) with \(T\to\infty\). This area is at most equal to the area of half the cycle because of cancellations of the positive and negative areas of a sinusoid. The second term is this area multiplied by \(C^{2}/2T\) with \(T\to\infty\). Clearly this term is zero, and

\[P_{x}=\frac{C^{2}}{2}\]

This shows that a sinusoid of amplitude \(C\) has a power \(C^{2}/2\) regardless of the value of its frequency \(\omega_{0}\left(\omega_{0}\neq 0\right)\) and phase \(\theta\). The rms value is \(C/\sqrt{2}\). If the signal frequency is zero (dc or a constant signal of amplitude \(C\)), the reader can show that the power is \(C^{2}\).

**(b)** In Ch. 6, we shall show that a sum of two sinusoids may or may not be periodic, depending on whether the ratio \(\omega_{1}/\omega_{2}\) is a rational number. Therefore, the period of this signal is not known. Hence, its power will be determined by averaging its energy over \(T\) seconds with \(T\to\infty\). Thus,

\[\begin{split} P_{x}&=\lim_{T\to\infty}\frac{1}{T} \int_{-T/2}^{T/2}[C_{1}\cos\left(\omega_{1}t+\theta_{1}\right)+C_{2}\cos\left( \omega_{2}t+\theta_{2}\right)]^{2}dt\\ &=\lim_{T\to\infty}\frac{1}{T}\int_{-T/2}^{T/2}{C_{1}}^{2}\cos^{ 2}\left(\omega_{1}t+\theta_{1}\right)dt+\lim_{T\to\infty}\frac{1}{T}\int_{-T/ 2}^{T/2}{C_{2}}^{2}\cos^{2}\left(\omega_{2}t+\theta_{2}\right)dt\\ &\quad+\lim_{T\to\infty}\frac{2C_{1}C_{2}}{T}\int_{-T/2}^{T/2} \cos\left(\omega_{1}t+\theta_{1}\right)\cos\left(\omega_{2}t+\theta_{2}\right) dt\end{split}\]The first and second integrals on the right-hand side are the powers of the two sinusoids, which are \({C_{1}}^{2}/2\) and \({C_{2}}^{2}/2\), as found in part (a). The third term, the product of two sinusoids, can be expressed as a sum of two sinusoids \(\cos\left[(\omega_{1}+\omega_{2})t+(\theta_{1}+\theta_{2})\right]\) and \(\cos\left[(\omega_{1}-\omega_{2})t+(\theta_{1}-\theta_{2})\right]\), respectively. Now, arguing as in part (a), we see that the third term is zero. Hence, we have+

Footnote †: This is true only if \(\omega_{1}\neq\omega_{2}\). If \(\omega_{1}=\omega_{2}\), the integrand of the third term contains a constant \(\cos\left(\theta_{1}-\theta_{2}\right)\), and the third term \(\to 2C_{1}C_{2}\cos\left(\theta_{1}-\theta_{2}\right)\) as \(T\to\infty\).

\[P_{x}=\frac{{C_{1}}^{2}}{2}+\frac{{C_{2}}^{2}}{2}\]

and the rms value is \(\sqrt{({C_{1}}^{2}+{C_{2}}^{2})/2}\).

We can readily extend this result to a sum of any number of sinusoids with distinct frequencies. Thus, if

\[x(t)=\sum_{n=1}^{\infty}C_{n}\cos\left(\omega_{n}t+\theta_{n}\right)\]

assuming that none of the two sinusoids have identical frequencies and \(\omega_{n}\neq 0\), then

\[P_{x}=\tfrac{1}{2}\sum_{n=1}^{\infty}{C_{n}}^{2}\]

If \(x(t)\) also has a dc term, as

\[x(t)=C_{0}+\sum_{n=1}^{\infty}C_{n}\cos\left(\omega_{n}t+\theta_{n}\right)\]

then

\[P_{x}=C_{0}^{2}+\tfrac{1}{2}\sum_{n=1}^{\infty}{C_{n}}^{2} \tag{1.3}\]

**(c)** In this case the signal is complex, and we use Eq. (1.2) to compute the power.

\[P_{x}=\lim_{T\to\infty}\frac{1}{T}\int_{-T/2}^{T/2}|De^{i\omega_{0}t}|^{2}\,dt\]

Recall that \(|e^{i\omega_{0}t}|=1\) so that \(|De^{i\omega_{0}t}|^{2}=|D|^{2}\), and

\[P_{x}=|D|^{2} \tag{1.4}\]

The rms value is \(|D|\).

**Comment.** In part (b) of Ex. 1.2, we have shown that the power of the sum of two sinusoids is equal to the sum of the powers of the sinusoids. It may appear that the power of \(x_{1}(t)+x_{2}(t)\)is \(P_{x_{1}}+P_{x_{2}}\). Unfortunately, this conclusion is not true in general. It is true only under a certain condition (orthogonality), discussed later (Sec. 6.5-3).

### 1.1 Computing Energy, Power, and RMS Value

Show that the energies of the signals in Figs. 1.3a, 1.3b, 1.3c, and 1.3d are 4, 1, 4/3, and 4/3, respectively. Observe that doubling a signal quadruples the energy, and time-shifting a signal has no effect on the energy. Show also that the power of the signal in Fig. 1.3e is 0.4323. What is the rms value of signal in Fig. 1.3e?

### 1.2 Computing Power over a Period

Redo Ex. 1.1a to find the power of a sinusoid \(C\cos\left(\omega_{0}t+\theta\right)\) by averaging the signal energy over one period \(T_{0}=2\pi/\omega_{0}\) (rather than averaging over the infinitely large interval). Show also that the power of a dc signal \(x(t)=C_{0}\) is \(C_{0}^{2}\), and its rms value is \(C_{0}\).

### 1.3 Power of a Sum of Two Equal-Frequency Sinusoids

Show that if \(\omega_{1}=\omega_{2}\), the power of \(x(t)=C_{1}\,\cos\left(\omega_{1}t+\theta_{1}\right)+C_{2}\,\cos\left(\omega_{ 2}t+\theta_{2}\right)\) is \([{C_{1}}^{2}+{C_{2}}^{2}+2C_{1}C_{2}\,\cos\left(\theta_{1}-\theta_{2}\right) ]/2\), which is not equal to the Ex. 1.2b result of \(({C_{1}}^{2}+{C_{2}}^{2})/2\).

Figure 1.3: Signals for Drill 1.1

## Chapter 1 Introduction

### 1.1 Some Useful Signal Operations

The use of

## Chapter 1 Signals and Systems

### 1.3 Time Shifting

An exponential function \(x(t)=e^{-2t}\) shown in Fig. 1.5a is delayed by 1 second. Sketch and mathematically describe the delayed function. Repeat the problem with \(x(t)\) advanced by 1 second.

The function \(x(t)\) can be described mathematically as

\[x(t)=\begin{cases}e^{-2t}&t\geq 0\\ 0&t<0\end{cases} \tag{1.5}\]

Let \(x_{d}(t)\) represent the function \(x(t)\) delayed (right-shifted) by 1 second, as illustrated in Fig. 1.5b. This function is \(x(t-1)\); its mathematical description can be obtained from \(x(t)\)

Figure 1.5: **(a)** Signal \(x(t)\). **(b)** Signal \(x(t)\) delayed by 1 second. **(c)** Signal \(x(t)\) advanced by 1 second.

by replacing \(t\) with \(t-1\) in Eq. (1.5). Thus,

\[x_{d}(t)=x(t-1)=\begin{cases}e^{-2(t-1)}&t-1\geq 0\quad\text{or}\quad t\geq 1 \\ 0&t-1<0\quad\text{or}\quad t<1\end{cases}\]

Let \(x_{a}(t)\) represent the function \(x(t)\) advanced (left-shifted) by \(1\) second, as depicted in Fig. 1.5c. This function is \(x(t+1)\); its mathematical description can be obtained from \(x(t)\) by replacing \(t\) with \(t+1\) in Eq. (1.5). Thus,

\[x_{a}(t)=x(t+1)=\begin{cases}e^{-2(t+1)}&t+1\geq 0\quad\text{or}\quad t\geq-1 \\ 0&t+1<0\quad\text{or}\quad t<-1\end{cases}\]

### 1.4 Working with Time Delay and Time Advance

Write a mathematical description of the signal \(x_{3}(t)\) in Fig. 1.3c. Next, delay this signal by \(2\) seconds. Sketch the delayed signal. Show that this delayed signal \(x_{d}(t)\) can be described mathematically as \(x_{d}(t)=2(t-2)\) for \(2\leq t\leq 3\), and equal to \(0\) otherwise. Now repeat the procedure with the signal advanced (left-shifted) by \(1\) second. Show that this advanced signal \(x_{a}(t)\) can be described as \(x_{a}(t)=2(t+1)\) for \(-1\leq t\leq 0\), and \(0\) otherwise.

### 1.2-2 Time Scaling

The compression or expansion of a signal in time is known as _time scaling_. Consider the signal \(x(t)\) of Fig. 1.6a. The signal \(\phi(t)\) in Fig. 1.6b is \(x(t)\) compressed in time by a factor of \(2\). Therefore, whatever happens in \(x(t)\) at some instant \(t\) also happens to \(\phi(t)\) at the instant \(t/2\) so that

\[\phi\Big{(}\frac{t}{2}\Big{)}=x(t)\qquad\text{and}\qquad\phi(t)=x(2t)\]

Observe that because \(x(t)=0\) at \(t=T_{1}\) and \(T_{2}\), we must have \(\phi(t)=0\) at \(t=T_{1}/2\) and \(T_{2}/2\), as shown in Fig. 1.6b. If \(x(t)\) were recorded on a tape and played back at twice the normal recording speed, we would obtain \(x(2t)\). In general, if \(x(t)\) is compressed in time by a factor \(a\) (\(a>1\)), the resulting signal \(\phi(t)\) is given by

\[\phi(t)=x(at)\]

Using a similar argument, we can show that \(x(t)\) expanded (slowed down) in time by a factor \(a\) (\(a>1\)) is given by

\[\phi(t)=x\bigg{(}\frac{t}{a}\bigg{)}\]

Figure 1.6c shows \(x(t/2)\), which is \(x(t)\) expanded in time by a factor of \(2\). Observe that in a time-scaling operation, the origin \(t=0\) is the anchor point, which remains unchanged under the scaling operation because at \(t=0\), \(x(t)=x(at)=x(0)\).

In summary, to time-scale a signal by a factor \(a\), we replace \(t\) with \(at\). If \(a>1\), the scaling results in compression, and if \(a<1\), the scaling results in expansion.

**Example 1.4**: **Continuous Time-Scaling Operation**__

Figure 1.7a shows a signal \(x(t)\). Sketch and describe mathematically this signal time-compressed by factor 3. Repeat the problem for the same signal time-expanded by factor 2.

\[x(t)=\begin{cases}2&-1.5\leq t<0\\ 2\,e^{-t/2}&0\leq t<3\\ 0&\text{otherwise}\end{cases} \tag{1.6}\]

Figure 1.7b shows \(x_{c}(t)\), which is \(x(t)\) time-compressed by factor 3; consequently, it can be described mathematically as \(x(3t)\), which is obtained by replacing \(t\) with \(3t\) in the right-hand side of Eq. (1.6). Thus,

\[x_{c}(t)=x(3t)=\begin{cases}2&-1.5\leq 3t<0\quad\text{or}\quad-0.5\leq t<0\\ 2\,e^{-3t/2}&0\leq 3t<3\quad\text{or}\quad 0\leq t<1\\ 0&\text{otherwise}\end{cases}\]Observe that the instants \(t=-1.5\) and \(3\) in \(x(t)\) correspond to the instants \(t=-0.5\), and \(1\) in the compressed signal \(x(3t)\).

Figure 7c shows \(x_{e}(t)\), which is \(x(t)\) time-expanded by factor \(2\); consequently, it can be described mathematically as \(x(t/2)\), which is obtained by replacing \(t\) with \(t/2\) in \(x(t)\). Thus,

\[x_{e}(t)=x\left(\frac{t}{2}\right)=\begin{cases}2&-1.5\leq\frac{t}{2}<0\quad \text{or}\quad-3\leq t<0\\ 2e^{-t/4}&0\leq\frac{t}{2}<3\quad\text{or}\quad 0\leq t<6\\ 0&\text{otherwise}\end{cases}\]

Observe that the instants \(t=-1.5\) and \(3\) in \(x(t)\) correspond to the instants \(t=-3\) and \(6\) in the expanded signal \(x(t/2)\).

## Chapter 1 Signals and Systems

### 1.2 Time Reversal

Consider the signal \(x(t)\) in Fig. 1.8a. We can view \(x(t)\) as a rigid wire frame hinged at the vertical axis. To time-reverse \(x(t)\), we rotate this frame \(180^{\circ}\) about the vertical axis. This time reversal [the reflection of \(x(t)\) about the vertical axis] gives us the signal \(\phi(t)\) (Fig. 1.8b). Observe that whatever happens in Fig. 1.8a at some instant \(t\) also happens in Fig. 1.8b at the instant \(-t\), and vice versa. Therefore,

\[\phi(t)=x(-t)\]

Thus, to time-reverse a signal we replace \(t\) with \(-t\), and the time reversal of signal \(x(t)\) results in a signal \(x(-t)\). We must remember that the reversal is performed about the vertical axis, which acts as an anchor or a hinge. Recall also that the reversal of \(x(t)\) about the horizontal axis results in \(-x(t)\).

Figure 1.8: Time reversal of a signal.

### 1.2 Some Useful Signal Operations

**Example 1.5**: Time Reversal of a Signal

For the signal \(x(t)\) illustrated in Fig. 1.9a, sketch \(x(-t)\), which is time-reversed \(x(t)\).

The instants \(-1\) and \(-5\) in \(x(t)\) are mapped into instants \(1\) and \(5\) in \(x(-t)\). Because \(x(t)=e^{t/2}\), we have \(x(-t)=e^{-t/2}\). The signal \(x(-t)\) is depicted in Fig. 1.9b. We can describe \(x(t)\) and \(x(-t)\) as

\[x(t)=\begin{cases}e^{t/2}&-1\geq t>-5\\ 0&\text{otherwise}\end{cases}\]

and its time-reversed version \(x(-t)\) is obtained by replacing \(t\) with \(-t\) in \(x(t)\) as

\[x(-t)=\begin{cases}e^{-t/2}&-1\geq-t>-5\quad\text{or}\quad 1\leq t<5\\ 0&\text{otherwise}\end{cases}\]

### 1.2-4 Combined Operations

Certain complex operations require simultaneous use of more than one of the operations just described. The most general operation involving all the three operations is \(x(at-b)\), which is realized in two possible sequences of operation:

1. Time-shift \(x(t)\) by \(b\) to obtain \(x(t-b)\). Now time-scale the shifted signal \(x(t-b)\) by \(a\) [i.e., replace \(t\) with \(at\)] to obtain \(x(at-b)\).
2. Time-scale \(x(t)\) by \(a\) to obtain \(x(at)\). Now time-shift \(x(at)\) by \(b/a\) [i.e., replace \(t\) with \(t-(b/a)\)] to obtain \(x[a(t-b/a)\)] = \(x(at-b)\). In either case, if \(a\) is negative, time scaling involves time reversal.

For example, the signal \(x(2t-6)\) can be obtained in two ways. We can delay \(x(t)\) by \(6\) to obtain \(x(t-6)\), and then time-compress this signal by factor \(2\) (replace \(t\) with \(2t\)) to obtain \(x(2t-6)\)

Figure 1.9: Example of time reversal.

Alternatively, we can first time-compress \(x(t)\) by factor 2 to obtain \(x(2t)\), then delay this signal by 3 (replace \(t\) with \(t-3\)) to obtain \(x(2t-6)\).

### 1.3 Classification of Signals

Classification helps us better understand and utilize the items around us. Cars, for example, are classified as sports, offroad, family, and so forth. Knowing you have a sports car is useful in deciding whether to drive on a highway or on a dirt road. Knowing you want to drive up a mountain, you would probably choose an offroad vehicle over a family sedan. Similarly, there are several classes of signals. Some signal classes are more suitable for certain applications than others. Further, different signal classes often require different mathematical tools. Here we shall consider only the following classes of signals, which are suitable for the scope of this book:

1. Continuous-time and discrete-time signals
2. Analog and digital signals
3. Periodic and aperiodic signals
4. Energy and power signals
5. Deterministic and probabilistic signals

### 1.3-1 Continuous-Time and Discrete-Time Signals

A signal that is specified for a continuum of values of time \(t\) (Fig. 1.10a) is a _continuous-time signal_, and a signal that is specified only at discrete values of \(t\) (Fig. 1.10b) is a _discrete-time signal_. Telephone and video camera outputs are continuous-time signals, whereas the quarterly gross national product (GNP), monthly sales of a corporation, and stock market daily averages are discrete-time signals.

### 1.3-2 Analog and Digital Signals

The concept of continuous time is often confused with that of analog. The two are not the same. The same is true of the concepts of discrete time and digital. A signal whose amplitude can take on any value in a continuous range is an _analog signal_. This means that an analog signal amplitude can take on an infinite number of values. A _digital signal,_ on the other hand, is one whose amplitude can take on only a finite number of values. Signals associated with a digital computer are digital because they take on only two values (binary signals). A digital signal whose amplitudes can take on \(M\) values is an \(M\)-ary signal of which binary (\(M=2\)) is a special case. The terms _continuous time_ and _discrete time_ qualify the nature of a signal along the time (horizontal) axis. The terms _analog_ and _digital,_ on the other hand, qualify the nature of the signal amplitude (vertical axis). Figure 1.11 shows examples of signals of various types. It is clear that analog is not necessarily continuous-time and digital need not be discrete-time. Figure 1.11c shows an example of an analog discrete-time signal. An analog signal can be converted into a digital signal [analog-to-digital (A/D) conversion] through quantization (rounding off ), as explained in Sec. 8.3.

### 1.3.1 Periodic and Aperiodic Signals

A signal \(x(t)\) is said to be _periodic_ if for some positive constant \(T_{0}\)

\[x(t)=x(t+T_{0})\qquad\mbox{for all $t$} \tag{1.7}\]

The _smallest_ value of \(T_{0}\) that satisfies the periodicity condition of Eq. (1.7) is the _fundamental period_ of \(x(t)\). The signals in Figs. 1.2b and 1.3e are periodic signals with periods 2 and 1, respectively. A signal is _aperiodic_ if it is not periodic. Signals in Figs. 1.2a, 1.3a, 1.3b, 1.3c, and 1.3d are all aperiodic.

By definition, a periodic signal \(x(t)\) remains unchanged when time-shifted by one period. For this reason, a periodic signal must start at \(t=-\infty\): if it started at some finite instant, say, \(t=0\), the time-shifted signal \(x(t+T_{0})\) would start at \(t=-T_{0}\) and \(x(t+T_{0})\) would not be the same as

Figure 1.10: **(a)** Continuous-time and **(b)** discrete-time signals.

\(x(t)\). Therefore, a _periodic signal, by definition, must start at \(t=-\infty\) and continue forever, as illustrated in Fig. 12._

Another important property of a periodic signal \(x(t)\) is that \(x(t)\) can be generated by _periodic extension_ of any segment of \(x(t)\) of duration \(T_{0}\) (the period). As a result, we can generate \(x(t)\) from any segment of \(x(t)\) having a duration of one period by placing this segment and the reproduction thereof end to end ad infinitum on either side. Figure 13 shows a periodic signal \(x(t)\) of period \(T_{0}=6\). The shaded portion of Fig. 13a shows a segment of \(x(t)\) starting at \(t=-1\) and having a duration of one period (6 seconds). This segment, when repeated forever in either direction, results in the periodic signal \(x(t)\). Figure 13b shows another shaded segment of \(x(t)\) of duration \(T_{0}\) starting at \(t=0\). Again, we see that this segment, when repeated forever on either side, results in \(x(t)\). The reader can verify that this construction is possible with any segment of \(x(t)\) starting at any instant as long as the segment duration is one period.

Figure 11: Examples of signals: **(a)** analog, continuous time; **(b)** digital, continuous time; **(c)** analog, discrete time; and **(d)** digital, discrete time.

Figure 12: A periodic signal of period \(T_{0}\).

An additional useful property of a periodic signal \(x(t)\) of period \(T_{0}\) is that the area under \(x(t)\) over any interval of duration \(T_{0}\) is the same; that is, for any real numbers \(a\) and \(b\),

\[\int_{a}^{a+T_{0}}x(t)\,dt=\int_{b}^{b+T_{0}}x(t)\,dt\]

This result follows from the fact that a periodic signal takes the same values at the intervals of \(T_{0}\). Hence, the values over any segment of duration \(T_{0}\) are repeated in any other interval of the same duration. For convenience, the area under \(x(t)\) over any interval of duration \(T_{0}\) will be denoted by

\[\int_{T_{0}}x(t)\,dt\]

It is helpful to label signals that start at \(t=-\infty\) and continue forever as _everlasting_ signals. Thus, an everlasting signal exists over the entire interval \(-\infty<t<\infty\). The signals in Figs. (b)b and (b)b are examples of everlasting signals. Clearly, a periodic signal, by definition, is an everlasting signal.

A signal that does not start before \(t=0\) is a _causal_ signal. In other words, \(x(t)\) is a causal signal if

\[x(t)=0\qquad t<0\]

The signals in Figs. (a)a-(c)c are causal signals. A signal that starts before \(t=0\) is a _noncausal_ signal. All the signals in Figs. (d)d and (e)e are noncausal. Observe that an everlasting signal is always noncausal but a noncausal signal is not necessarily everlasting. The everlasting signal in Fig. (b)b is noncausal; however, the noncausal signal in Fig. (a)a is not everlasting. A signal that is zero for all \(t\geq 0\) is called an _anti-causal_ signal.

**Comment.** A true everlasting signal cannot be generated in practice for obvious reasons. Why should we bother to postulate such a signal? In later chapters we shall see that certain signals

Figure 13: Generation of a periodic signal by periodic extension of its segment of one-period duration.

(e.g., an impulse and an everlasting sinusoid) that cannot be generated in practice _do_ serve a very useful purpose in the study of signals and systems.

### 1.3-4 Energy and Power Signals

A signal with finite energy is an _energy signal_, and a signal with finite and nonzero power is a _power signal_. The signals in Figs. 1.2a and 1.2b are examples of energy and power signals, respectively. Observe that power is the time average of energy. Since the averaging is over an infinitely large interval, a signal with finite energy has zero power, and a signal with finite power has infinite energy. Therefore, a signal cannot be both an energy signal and a power signal. If it is one, it cannot be the other. On the other hand, there are signals that are neither energy nor power signals. The ramp signal is one such case.

**Comments.** All practical signals have finite energies and are therefore energy signals. A power signal must necessarily have infinite duration; otherwise, its power, which is its energy averaged over an infinitely large interval, will not approach a (nonzero) limit. Clearly, it is impossible to generate a true power signal in practice because such a signal has infinite duration and infinite energy.

Also, because of periodic repetition, periodic signals for which the area under \(|x(t)|^{2}\) over one period is finite are power signals; however, not all power signals are periodic.

**Drill 1.6**: **Neither Energy nor Power**

Show that an everlasting exponential \(e^{-at}\) is neither an energy nor a power signal for any real value of \(a\). However, if \(a\) is imaginary, it is a power signal with power \(P_{x}=1\) regardless of the value of \(a\).

### 1.3-5 Deterministic and Random Signals

A signal whose physical description is known completely, in either a mathematical form or a graphical form, is a _deterministic signal_. A signal whose values cannot be predicted precisely but are known only in terms of probabilistic description, such as mean value or mean-squared value, is a _random signal_. In this book we shall exclusively deal with deterministic signals. Random signals are beyond the scope of this study.

## 1.4 Some Useful Signal Models

In the area of signals and systems, the step, the impulse, and the exponential functions play very important roles. Not only do they serve as a basis for representing other signals, but their use can simplify many aspects of the signals and systems.

### 1.4 Some Useful Signal Models

#### The Unit Step Function \(u(t)\)

In much of our discussion, the signals begin at \(t=0\) (causal signals). Such signals can be conveniently described in terms of unit step function \(u(t)\) shown in Fig. 1.14a. This function is defined by

\[u(t)=\left\{\begin{array}{ll}1&t\geq 0\\ 0&t<0\end{array}\right. \tag{1.8}\]

If we want a signal to start at \(t=0\) (so that it has a value of zero for \(t<0\)), we need only multiply the signal by \(u(t)\). For instance, the signal \(e^{-at}\) represents an everlasting exponential that starts at \(t=-\infty\). The causal form of this exponential (Fig. 1.14b) can be described as \(e^{-at}u(t)\).

The unit step function also proves very useful in specifying a function with different mathematical descriptions over different intervals. Examples of such functions appear in Fig. 1.7. These functions have different mathematical descriptions over different segments of time, as seen from Eqs. (1.5) and (1.6). Such a description often proves clumsy and inconvenient in mathematical treatment. We can use the unit step function to describe such functions by a single expression that is valid for all \(t\).

Consider, for example, the rectangular pulse depicted in Fig. 1.15a. We can express such a pulse in terms of familiar step functions by observing that the pulse \(x(t)\) can be expressed as the sum of the two delayed unit step functions, as shown in Fig. 1.15b. The unit step function \(u(t)\) delayed by \(T\) seconds is \(u(t-T)\). From Fig. 1.15b, it is clear that

\[x(t)=u(t-2)-u(t-4)\]

Figure 1.14: **(a)** Unit step function \(u(t)\). **(b)** Exponential \(e^{-at}u(t)\).

Figure 1.15: Representation of a rectangular pulse by step functions.

## Chapter 1 Signals and Systems

### 1.6 Describing a Triangle Function with the Unit Step

Use the unit step function to describe the signal in Fig. 1.16a.

The signal illustrated in Fig. 1.16a can be conveniently handled by breaking it up into the two components \(x_{1}(t)\) and \(x_{2}(t)\), depicted in Figs. 1.16b and 1.16c, respectively. Here, \(x_{1}(t)\) can be obtained by multiplying the ramp \(t\) by the gate pulse \(u(t)-u(t-2)\), as shown in Fig. 1.16b. Therefore,

\[x_{1}(t)=t[u(t)-u(t-2)]\]

The signal \(x_{2}(t)\) can be obtained by multiplying another ramp by the gate pulse illustrated in Fig. 1.16c. This ramp has a slope \(-2\); hence it can be described by \(-2t+c\). Now, because the ramp has a zero value at \(t=3\), the constant \(c=6\), and the ramp can be described by \(-2(t-3)\). Also, the gate pulse in Fig. 1.16c is \(u(t-2)-u(t-3)\). Therefore,

\[x_{2}(t)=-2(t-3)[u(t-2)-u(t-3)]\]

Figure 1.16: Representation of a signal defined interval by interval.

\[x(t) =x_{1}(t)+x_{2}(t)\] \[=t[u(t)-u(t-2)]-2(t-3)\left[u(t-2)-u(t-3)\right]\] \[=tu(t)-3(t-2)u(t-2)+2(t-3)u(t-3)\]

**Example 1.7**: **Describing a Piecewise Function with the Unit Step**

Describe the signal in Fig. 1.7a by a single expression valid for all \(t\).

Over the interval from \(-1.5\) to \(0\), the signal can be described by a constant \(2\), and over the interval from \(0\) to \(3\), it can be described by \(2\,e^{-t/2}\). Therefore,

\[x(t) =\underbrace{2[u(t+1.5)-u(t)]}_{\text{constant part}}+ \underbrace{2e^{-t/2}[u(t)-u(t-3)]}_{\text{exponential part}}\] \[=2u(t+1.5)-2(1-e^{-t/2})u(t)-2e^{-t/2}u(t-3)\]

Compare this expression with the expression for the same function found in Eq. (1.6).

**Drill 1.7**: **Using Reflected Unit Step Functions**

Show that the signals depicted in Figs. 1.17a and 1.17b can be described as \(u(-t)\) and \(e^{-at}u(-t)\), respectively.

Figure 1.17: Signals for Drill 1.7.

### 1.8 Describing a Piecewise Function with the Unit Step

Show that the signal shown in Fig. 1.18 can be described as

\[x(t)=(t-1)u(t-1)-(t-2)u(t-2)-u(t-4)\]

### 1.4 The Unit Impulse Function \(\delta(t)\)

The unit impulse function \(\delta(t)\) is one of the most important functions in the study of signals and systems. This function was first defined in two parts by P. A. M. Dirac as

\[\delta(t)=0\quad t\neq 0\qquad\text{and}\qquad\int_{-\infty}^{\infty}\delta(t) \,dt=1 \tag{1.9}\]

We can visualize an impulse as a tall, narrow, rectangular pulse of unit area, as illustrated in Fig. 1.19b. The width of this rectangular pulse is a very small value \(\epsilon\to 0\). Consequently, its height is a very large value \(1/\epsilon\to\infty\). The unit impulse therefore can be regarded as a rectangular pulse with a width that has become infinitesimally small, a height that has become infinitely large, and an overall area that has been maintained at unity. Thus \(\delta(t)=0\) everywhere except at \(t=0\), where it is undefined. For this reason, a unit impulse is represented by the separlike symbol in Fig. 1.19a.

Other pulses, such as the exponential, triangular, or Gaussian types, may also be used in impulse approximation. The important feature of the unit impulse function is not its shape but the fact that its effective duration (pulse width) approaches zero while its area remains at unity. For example, the exponential pulse \(\alpha e^{-\alpha t}u(t)\) in Fig. 1.20a becomes taller and narrower as \(\alpha\) increases.

Figure 1.19: A unit impulse and its approximation.

Figure 1.18: Signal for Drill 1.8.

In the limit as \(\alpha\rightarrow\infty\), the pulse height \(\rightarrow\infty\), and its width or duration \(\to 0\). Yet, the area under the pulse is unity regardless of the value of \(\alpha\) because

\[\int_{0}^{\infty}\alpha e^{-\alpha t}dt=1\]

The pulses in Figs. 1.20b and 1.20c behave in a similar fashion. Clearly, the exact impulse function cannot be generated in practice; it can only be approached.

From Eq. (9), it follows that the function \(k\delta(t)=0\) for all \(t\neq 0\), and its area is \(k\). Thus, \(k\delta(t)\) is an impulse function whose area is \(k\) (in contrast to the unit impulse function, whose area is \(1\)).

Multiplication of a Function by an Impulse

Let us now consider what happens when we multiply the unit impulse \(\delta(t)\) by a function \(\phi(t)\) that is known to be continuous at \(t=0\). Since the impulse has nonzero value only at \(t=0\), and the value of \(\phi(t)\) at \(t=0\) is \(\phi(0)\), we obtain

\[\phi(t)\delta(t)=\phi(0)\delta(t)\]

Thus, multiplication of a continuous-time function \(\phi(t)\) with an unit impulse located at \(t=0\) results in an impulse, which is located at \(t=0\) and has strength \(\phi(0)\) [the value of \(\phi(t)\) at the location of the impulse]. Use of exactly the same argument leads to the generalization of this result, stating that provided \(\phi(t)\) is continuous at \(t=T,\phi(t)\) multiplied by an impulse \(\delta(t-T)\) (impulse located at \(t=T\)) results in an impulse located at \(t=T\) and having strength \(\phi(T)\) [the value of \(\phi(t)\) at the location of the impulse].

\[\phi(t)\delta(t-T)=\phi(T)\delta(t-T) \tag{10}\]

Sampling Property of the Unit Impulse Function

From Eq. (10) it follows that

\[\int_{-\infty}^{\infty}\phi(t)\delta(t-T)\,dt=\phi(T)\int_{-\infty}^{\infty} \delta(t)\,dt=\phi(T) \tag{11}\]

provided \(\phi(t)\) is continuous at \(t=T\). This result means that _the area under the product of a function with an impulse \(\delta(t-T)\) is equal to the value of that function at the instant at which the unit impulse is located._ This property is very important and useful and is known as the _sampling_ or _sifting property_ of the unit impulse.

Figure 10: Other possible approximations to a unit impulse.

## Chapter 1 Signals and Systems

### 1.1 Unit Impulse as a Generalized Function

The definition of the unit impulse function given in Eq. (1.9) is not mathematically rigorous, which leads to serious difficulties. First, the impulse function does not define a unique function: for example, it can be shown that \(\delta(t)+\dot{\delta}(t)\) also satisfies Eq. (1.9) [1]. Moreover, \(\delta(t)\) is not even a true function in the ordinary sense. An ordinary function is specified by its values for all time \(t\). The impulse function is zero everywhere except at \(t=0\), and at this, the only interesting part of its range, it is undefined. These difficulties are resolved by defining the impulse as a generalized function rather than an ordinary function. A _generalized function_ is defined by its effect on other functions instead of by its value at every instant of time.

In this approach the impulse function is defined by the sampling property [Eq. (1.11)]. We say nothing about what the impulse function is or what it looks like. Instead, the impulse function is defined in terms of its effect on a test function \(\phi(t)\). We define a unit impulse as a function for which the area under its product with a function \(\phi(t)\) is equal to the value of the function \(\phi(t)\) at the instant at which the impulse is located. It is assumed that \(\phi(t)\) is continuous at the location of the impulse. Recall that the sampling property [Eq. (1.11)] is the consequence of the classical (Dirac) definition of the unit impulse in Eq. (1.9). In contrast, _the sampling property [Eq. (1.11)] defines the impulse function in the generalized function approach._

We now present an interesting application of the generalized function definition of an impulse. Because the unit step function \(u(t)\) is discontinuous at \(t=0\), its derivative \(du/dt\) does not exist at \(t=0\) in the ordinary sense. We now show that this derivative _does_ exist in the generalized sense, and it is, in fact, \(\delta(t)\). As a proof, let us evaluate the integral of \((du/dt)\phi(t)\), using integration by parts:

\[\int_{-\infty}^{\infty}\frac{du(t)}{dt}\phi(t)\,dt =u(t)\phi(t)\bigg{|}_{-\infty}^{\infty}-\int_{-\infty}^{\infty}u(t )\dot{\phi}(t)\,dt\] \[=\phi(\infty)-0-\int_{0}^{\infty}\dot{\phi}(t)\,dt\] \[=\phi(\infty)-\phi(t)|_{0}^{\infty}=\phi(0)\]

This result shows that \(du/dt\) satisfies the sampling property of \(\delta(t)\). Therefore it is an impulse \(\delta(t)\) in the generalized sense--that is,

\[\frac{du(t)}{dt}=\delta(t) \tag{1.12}\]

Consequently,

\[\int_{-\infty}^{t}\delta(\tau)\,d\tau=u(t)\]

These results can also be obtained graphically from Fig. 1.19b. We observe that the area from \(-\infty\) to \(t\) under the limiting form of \(\delta(t)\) in Fig. 1.19b is zero if \(t<-\epsilon/2\) and unity if \(t\geq\epsilon/2\) with \(\epsilon\to 0\). Consequently,

\[\int_{-\infty}^{t}\delta(\tau)\,d\tau =\begin{cases}0&t<0\\ 1&t\geq 0\end{cases}\] \[=u(t)\]This result shows that the unit step function can be obtained by integrating the unit impulse function. Similarly the unit ramp function \(x(t)=tu(t)\) can be obtained by integrating the unit step function. We may continue with unit parabolic function \(t^{2}/2\) obtained by integrating the unit ramp, and so on. On the other side, we have derivatives of impulse function, which can be defined as generalized functions (see Prob. 1.4-12). All these functions, derived from the unit impulse function (successive derivatives and integrals), are called _singularity functions.+_

Footnote †: Singularity functions were defined by late Prof. S. J. Mason as follows. A singularity is a point at which a function does not possess a derivative. Each of the singularity functions (or if not the function itself, then the function differentiated a finite number of times) has a singular point at the origin and is zero elsewhere [2].

Therefore,

\[e^{st}=e^{(\sigma+j\omega)t}=e^{\sigma t}e^{j\omega t}=e^{\sigma t}(\cos\omega t +j\sin\omega t) \tag{1.13}\]

Since \(s^{*}=\sigma-j\omega\) (the conjugate of \(s\)), then

\[e^{s^{*}t}=e^{(\sigma-j\omega)t}=e^{\sigma t}e^{-j\omega t}=e^{\sigma t}(\cos \omega t-j\sin\omega t)\]

and

\[e^{\sigma t}\cos\omega t=\tfrac{1}{2}(e^{st}+e^{s^{*}t}) \tag{1.14}\]

A comparison of Eq. (1.13) with Euler's formula shows that \(e^{st}\) is a generalization of the function \(e^{j\omega t}\), where the frequency variable \(j\omega\) is generalized to a complex variable \(s=\sigma+j\omega\). For this reason, we designate the variable \(s\) as the _complex frequency_. In fact, function \(e^{st}\) encompasses a large class of functions. The following functions are either special cases of or can be expressed in terms of \(e^{st}\):

1. A constant \(k=ke^{\sigma t}\qquad(s=0)\)
2. A monotonic exponential \(e^{\sigma t}\qquad(\omega=0,\,s=\sigma)\)
3. A sinusoid \(\cos\omega t\qquad(\sigma=0,\,s=\pm j\omega)\)
4. An exponentially varying sinusoid \(e^{\sigma t}\cos\omega t\qquad(s=\sigma\pm j\omega)\)

These functions are illustrated in Fig. 1.21.

The complex frequency \(s\) can be conveniently represented on a _complex frequency plane_ (\(s\) plane), as depicted in Fig. 1.22. The horizontal axis is the real axis (\(\sigma\) axis), and the vertical axis is the imaginary axis (\(\omega\) axis). The absolute value of the imaginary part of \(s\) is \(|\omega|\) (the

Figure 1.21: Sinusoids of complex frequency \(\sigma+j\omega\).

_radian_ frequency), which indicates the frequency of oscillation of \(e^{st}\); the real part \(\sigma\) (the _neper_ frequency) gives information about the rate of increase or decrease of the amplitude of \(e^{st}\). For signals whose complex frequencies lie on the real axis (\(\sigma\) axis, where \(\omega=0\)), the frequency of oscillation is zero. Consequently these signals are monotonically increasing or decreasing exponentials (Fig. 21a). For signals whose frequencies lie on the imaginary axis (\(\omega\) axis, where \(\sigma=0\)), \(e^{\sigma t}=1\). Therefore, these signals are conventional sinusoids with constant amplitude (Fig. 21b). The case \(s=0\) (\(\sigma=\omega=0\)) corresponds to a constant (dc) signal because \(e^{0t}=1\). For the signals illustrated in Figs. 21c and 21d, both \(\sigma\) and \(\omega\) are nonzero; the frequency \(s\) is complex and does not lie on either axis. The signal in Fig. 21c decays exponentially. Therefore, \(\sigma\) is negative, and \(s\) lies to the left of the imaginary axis. In contrast, the signal in Fig. 21d _grows_ exponentially. Therefore, \(\sigma\) is positive, and \(s\) lies to the right of the imaginary axis. Thus the \(s\) plane (Fig. 21) can be separated into two parts: the _left half-plane_ (LHP) corresponding to exponentially decaying signals and the _right half-plane_ (RHP) corresponding to exponentially growing signals. The imaginary axis separates the two regions and corresponds to signals of constant amplitude.

An exponentially growing sinusoid \(e^{2t}\cos 5t\), for example, can be expressed as a linear combination of exponentials \(e^{(2+j5)t}\) and \(e^{(2-j5)t}\) with complex frequencies \(2+j5\) and \(2-j5\), respectively, which lie in the RHP. An exponentially decaying sinusoid \(e^{-2t}\cos 5t\) can be expressed as a linear combination of exponentials \(e^{(-2+j5)t}\) and \(e^{(-2-j5)t}\) with complex frequencies \(-2+j5\) and \(-2-j5\), respectively, which lie in the LHP. A constant-amplitude sinusoid \(\cos 5t\) can be expressed as a linear combination of exponentials \(e^{j5t}\) and \(e^{-j5t}\) with complex frequencies \(\pm j5\), which lie on the imaginary axis. Observe that the monotonic exponentials \(e^{\pm 2t}\) are also generalized sinusoids with complex frequencies \(\pm 2\).

Figure 22: Complex frequency plane.

## Chapter 1 Signals and Systems

### 1.5 Even and Odd Functions

A function \(x_{e}(t)\) is said to be an _even function_ of \(t\) if it is symmetrical about the vertical axis. A function \(x_{o}(t)\) is said to be an _odd function_ of \(t\) if it is antisymmetrical about the vertical axis. Mathematically expressed, these symmetry conditions require

\[x_{e}(t)=x_{e}(-t)\qquad\text{and}\qquad x_{o}(t)=-x_{o}(-t) \tag{1.15}\]

An even function has the same value at the instants \(t\) and \(-t\) for all values of \(t\). On the other hand, the value of an odd function at the instant \(t\) is the negative of its value at the instant \(-t\). An example even signal and an example odd signal are shown in Figs. 1.23a and 1.23b, respectively.

### 1.5-1 Some Properties of Even and Odd Functions

Even and odd functions have the following properties:

\[\text{even function}\times\text{odd function}=\text{odd function}\] \[\text{odd function}\times\text{odd function}=\text{even function}\] \[\text{even function}\times\text{even function}=\text{even function}\]

The proofs are trivial and follow directly from the definition of odd and even functions [Eq. (1.15)].

Figure 1.23: Functions of \(t\): **(a)** even and **(b)** odd.

### Area

Because of the symmetries of even and odd functions about the vertical axis, it follows from Eq. (1.15) [or Fig. 1.23] that

\[\int_{-a}^{a}x_{e}(t)\,dt=2\int_{0}^{a}x_{e}(t)\,dt\qquad\text{and}\qquad\int_{ -a}^{a}x_{o}(t)\,dt=0 \tag{1.16}\]

These results are valid under the assumption that there is no impulse (or its derivatives) at the origin. The proof of these statements is obvious from the plots of even and odd functions. Formal proofs, left as an exercise for the reader, can be accomplished by using the definitions in Eq. (1.15).

Because of their properties, study of odd and even functions proves useful in many applications, as will become evident in later chapters.

### Even and Odd Components of a Signal

Every signal \(x(t)\) can be expressed as a sum of even and odd components because

\[x(t)=\underbrace{\tfrac{1}{2}[x(t)+x(-t)]}_{\text{even}}+\underbrace{\tfrac{1 }{2}[x(t)-x(-t)]}_{\text{odd}} \tag{1.17}\]

From the definitions in Eq. (1.15), we can clearly see that the first component on the right-hand side is an even function, while the second component is odd. This is apparent from the fact that replacing \(t\) by \(-t\) in the first component yields the same function. The same maneuver in the second component yields the negative of that component.

**Example 1.8**: **Finding the Even and Odd Components of a Signal**

Find and sketch the even and odd components of \(x(t)=e^{-at}u(t)\).

Based on Eq. (1.17), we can express \(x(t)\) as a sum of the even component \(x_{e}(t)\) and the odd component \(x_{o}(t)\) as

\[x(t)=x_{e}(t)+x_{o}(t)\]

where

\[x_{e}(t)=\tfrac{1}{2}[e^{-at}u(t)+e^{at}u(-t)]\qquad\text{and}\qquad x_{o}(t)= \tfrac{1}{2}[e^{-at}u(t)-e^{at}u(-t)]\]

The function \(e^{-at}u(t)\) and its even and odd components are illustrated in Fig. 1.24.

**Example 1.9**: **Finding the Even and Odd Components of a Complex Signal**

Find the even and odd components of \(e^{it}\).

From Eq. (1.17),

\[e^{it}=x_{e}(t)+x_{o}(t)\]

where

\[x_{e}(t)=\tfrac{1}{2}[e^{it}+e^{-jt}]=\cos t\qquad\text{and}\qquad x_{o}(t)= \tfrac{1}{2}[e^{it}-e^{-jt}]=j\sin t\]

Figure 1.24: Finding even and odd components of a signal.

## Chapter A Modification for Complex Signals

While a complex signal can be decomposed into even and odd components, it is more common to decompose complex signals using conjugate symmetries. A complex signal \(x(t)\) is said to be _conjugate-symmetric_ if \(x(t)=x^{*}(-t)\). A conjugate-symmetric signal is even in the real part and odd in the imaginary part. Thus, a real conjugate-symmetric signal is an even signal. A signal is _conjugate-antisymmetric_ if \(x(t)=-x^{*}(-t)\). A conjugate-antisymmetric signal is odd in the real part and even in the imaginary part. A real conjugate-antisymmetric signal is an odd signal. Any signal \(x(t)\) can be decomposed into a conjugate-symmetric portion \(x_{cs}(t)\) plus a conjugate-antisymmetric portion \(x_{ca}(t)\). That is,

\[x(t)=x_{cs}(t)+x_{ca}(t)\]

where

\[x_{cs}(t)=\frac{x(t)+x^{*}(-t)}{2}\qquad\text{and}\qquad x_{ca}(t)=\frac{x(t)-x ^{*}(-t)}{2}\]

The proof is similar to the one for decomposing a signal into even and odd components. As we shall see in later chapters, conjugate symmetries commonly occur in real-world signals and their transforms.

### 1.6 Systems

As mentioned in Sec. 1.1, systems are used to process signals to allow modification or extraction of additional information from the signals. A system may consist of physical components (hardware realization) or of an algorithm that computes the output signal from the input signal (software realization).

Roughly speaking, a physical system consists of interconnected components, which are characterized by their terminal (input-output) relationships. In addition, a system is governed by laws of interconnection. For example, in electrical systems, the terminal relationships are the familiar voltage-current relationships for the resistors, capacitors, inductors, transformers, transistors, and so on, as well as the laws of interconnection (i.e., Kirchhoff's laws). We use these laws to derive mathematical equations relating the outputs to the inputs. These equations then represent a _mathematical model_ of the system.

A system can be conveniently illustrated by a "black box" with one set of accessible terminals where the input variables \(x_{1}(t)\), \(x_{2}(t)\), \(\ldots\),\(x_{j}(t)\) are applied and another set of accessible terminals where the output variables \(y_{1}(t)\), \(y_{2}(t)\),\(\ldots\),\(y_{k}(t)\) are observed (Fig. 1.25).

The study of systems consists of three major areas: mathematical modeling, analysis, and design. Although we shall be dealing with mathematical modeling, our main concern is with

Figure 1.25: Representation of a system.

analysis and design. The major portion of this book is devoted to the analysis problem--how to determine the system outputs for the given inputs and a given mathematical model of the system (or rules governing the system). To a lesser extent, we will also consider the problem of design or synthesis--how to construct a system that will produce a desired set of outputs for the given inputs.

##### Data Needed to Compute System Response

To understand what data we need to compute a system response, consider a simple \(RC\) circuit with a current source \(x(t)\) as its input (Fig. 1.26).

The output voltage \(y(t)\) is given by

\[y(t)=Rx(t)+\frac{1}{C}\int_{-\infty}^{t}x(\tau)\,d\tau \tag{1.18}\]

The limits of the integral on the right-hand side are from \(-\infty\) to \(t\) because this integral represents the capacitor charge due to the current \(x(t)\) flowing in the capacitor, and this charge is the result of the current flowing in the capacitor from \(-\infty\). Now, Eq. (1.18) can be expressed as

\[y(t)=Rx(t)+\frac{1}{C}\int_{-\infty}^{0}x(\tau)\,d\tau+\frac{1}{C}\int_{0}^{t} x(\tau)\,d\tau\]

The middle term on the right-hand side is \(v_{C}(0)\), the capacitor voltage at \(t=0\). Therefore,

\[y(t)=v_{C}(0)+Rx(t)+\frac{1}{C}\int_{0}^{t}x(\tau)\,d\tau\qquad t\geq 0\]

This equation can be readily generalized as

\[y(t)=v_{C}(t_{0})+Rx(t)+\frac{1}{C}\int_{t_{0}}^{t}x(\tau)\,d\tau\qquad t\geq t _{0} \tag{1.19}\]

From Eq. (1.18), the output voltage \(y(t)\) at an instant \(t\) can be computed if we know the input current flowing in the capacitor throughout its entire past (\(-\infty\) to \(t\)). Alternatively, if we know the input current \(x(t)\) from some moment \(t_{0}\) onward, then, using Eq. (1.19), we can still calculate \(y(t)\) for \(t\geq t_{0}\) from a knowledge of the input current, provided we know \(v_{C}(t_{0})\), the initial capacitor voltage (voltage at \(t_{0}\)). Thus \(v_{C}(t_{0})\) contains all the relevant information about the circuit's entire

Figure 1.26: Example of a simple electrical system.

past (\(-\infty\) to \(t_{0}\)) that we need to compute \(y(t)\) for \(t\geq t_{0}\). Therefore, the response of a system at \(t\geq t_{0}\) can be determined from its input(s) during the interval \(t_{0}\) to \(t\) and from certain _initial conditions_ at \(t=t_{0}\).

In the preceding example, we needed only one initial condition. However, in more complex systems, several initial conditions may be necessary. We know, for example, that in passive _RLC_ networks, the initial values of all inductor currents and all capacitor voltages1 are needed to determine the outputs at any instant \(t\geq 0\) if the inputs are given over the interval \([0,t]\).

Footnote 1: Strictly speaking, this means independent inductor currents and capacitor voltages.

### 7 Classification of Systems

Systems may be classified broadly in the following categories:

1. Linear and nonlinear systems
2. Constant-parameter and time-varying-parameter systems
3. Instantaneous (memoryless) and dynamic (with memory) systems
4. Causal and noncausal systems
5. Continuous-time and discrete-time systems
6. Analog and digital systems
7. Invertible and noninvertible systems
8. Stable and unstable systems

Other classifications, such as deterministic and probabilistic systems, are beyond the scope of this text and are not considered.

### 7-1 Linear and Nonlinear Systems

The Concept of Linearity

A system whose output is proportional to its input is an _example_ of a linear system. But linearity implies more than this; it also implies the _additivity property:_ that is, if several inputs are acting on a system, then the total effect on the system due to all these inputs can be determined by considering one input at a time while assuming all the other inputs to be zero. The total effect is then the sum of all the component effects. This property may be expressed as follows: for a linear system, if an input \(x_{1}\) acting alone has an effect \(y_{1}\), and if another input \(x_{2}\), also acting alone, has an effect \(y_{2}\), then, with both inputs acting on the system, the total effect will be \(y_{1}+y_{2}\). Thus, if

\[x_{1}\longrightarrow y_{1}\qquad\mbox{and}\qquad x_{2}\longrightarrow y_{2}\]

then for all \(x_{1}\) and \(x_{2}\)

\[x_{1}+x_{2}\longrightarrow y_{1}+y_{2} \tag{20}\]

In addition, a linear system must satisfy the _homogeneity_ or scaling property, which states that for arbitrary real or imaginary number \(k\), if an input is increased \(k\)-fold, the effect also increases \(k\)-fold. Thus, if

\[x\longrightarrow y\]then for all real or imaginary \(k\)

\[kx\longrightarrow ky \tag{1.21}\]

Thus, linearity implies two properties: homogeneity (scaling) and additivity.2 Both these properties can be combined into one property (_superposition_), which is expressed as follows: If

Footnote 2: A linear system must also satisfy the additional condition of _smoothness,_ where small changes in the system’s inputs must result in small changes in its outputs [3].

\[x_{1}\longrightarrow y_{1}\qquad\mbox{and}\qquad x_{2}\longrightarrow y_{2}\]

then for all inputs \(x_{1}\) and \(x_{2}\) and all constants \(k_{1}\) and \(k_{2}\),

\[k_{1}x_{1}+k_{2}x_{2}\longrightarrow k_{1}y_{1}+k_{2}y_{2} \tag{1.22}\]

There is another useful way to view the linearity condition described in Eq. (1.22): the response of a linear system is unchanged whether the operations of summing and scaling precede the system (sum and scale act on inputs) or follow the system (sum and scale act on outputs). _Thus, linearity implies commutability between a system and the operations of summing and scaling._ It may appear that additivity implies homogeneity. Unfortunately, homogeneity does not always follow from additivity. Drill 1.11 demonstrates such a case.

**Drill 1.11** **Additivity but Not Homogeneity**

Show that a system with the input \(x(t)\) and the output \(y(t)\) related by \(y(t)=\mbox{Re}\{x(t)\}\) satisfies the additivity property but violates the homogeneity property. Hence, such a system is not linear.

[_Hint:_ Show that Eq. (1.21) is not satisfied when \(k\) is complex.]

Response of a Linear System

For the sake of simplicity, we discuss only _single-input, single-output_ (_SISO_) systems. But the discussion can be readily extended to _multiple-input, multiple-output_ (_MIMO_) systems.

A system's output for \(t\geq 0\) is the result of two independent causes: the initial conditions of the system (or the system state) at \(t=0\) and the input \(x(t)\) for \(t\geq 0\). If a system is to be linear, the output must be the sum of the two components resulting from these two causes: first, the _zero-input response_ (ZIR) that results only from the initial conditions at \(t=0\) with the input \(x(t)=0\) for \(t\geq 0\), and then the _zero-state response_ (ZSR) that results only from the input \(x(t)\) for \(t\geq 0\) when the initial conditions (at \(t=0\)) are assumed to be zero. When all the appropriate initial conditions are zero, the system is said to be in _zero state_. The system output is zero when the input is zero only if the system is in zero state.

In summary, a linear system response can be expressed as the sum of the zero-input and zero-state responses:

total response = zero-input response + zero-state response This property of linear systems, which permits the separation of an output into components resulting from the initial conditions and from the input, is called the _decomposition property_. For the \(RC\) circuit of Fig. 1.26, the response \(y(t)\) was found to be [see Eq. (19) with \(t_{0}=0\)]

\[y(t)=\underbrace{v_{C}(0)}_{\text{ZIR}}+\underbrace{Rx(t)+\frac{1}{C}\int_{0}^ {t}x(\tau)\,d\tau}_{\text{ZSR}} \tag{23}\]

From Eq. (23), it is clear that if the input \(x(t)=0\) for \(t\geq 0\), the output \(y(t)=v_{C}(0)\). Hence \(v_{C}(0)\) is the zero-input response of the response \(y(t)\). Similarly, if the system state (the voltage \(v_{C}\) in this case) is zero at \(t=0\), the output is given by the second component on the right-hand side of Eq. (23). Clearly this is the zero-state response of the response \(y(t)\).

In addition to the decomposition property, linearity implies that both the zero-input and zero-state components must obey the principle of superposition with respect to each of their respective causes. For example, if we increase the initial condition \(k\)-fold, the zero-input response must also increase \(k\)-fold. Similarly, if we increase the input \(k\)-fold, the zero-state response must also increase \(k\)-fold. These facts can be readily verified from Eq. (23) for the \(RC\) circuit in Fig. 1.26. For instance, if we double the initial condition \(v_{C}(0)\), the zero-input response doubles; if we double the input \(x(t)\), the zero-state response doubles.

**Example 1.10**: **Linearity of Constant-Coefficient Linear Differential Equations**

Show that the system described by the equation

\[\frac{dy(t)}{dt}+3y(t)=x(t) \tag{24}\]

is linear.

Let the system response to the inputs \(x_{1}(t)\) and \(x_{2}(t)\) be \(y_{1}(t)\) and \(y_{2}(t)\), respectively. Then

\[\frac{dy_{1}(t)}{dt}+3y_{1}(t)=x_{1}(t)\qquad\text{and}\qquad\frac{dy_{2}(t)}{ dt}+3y_{2}(t)=x_{2}(t)\]

Multiplying the first equation by \(k_{1}\), the second by \(k_{2}\), and adding them yield

\[\frac{d}{dt}[k_{1}y_{1}(t)+k_{2}y_{2}(t)]+3[k_{1}y_{1}(t)+k_{2}y_{2}(t)]=k_{1 }x_{1}(t)+k_{2}x_{2}(t)\]

But this equation is the system equation [Eq. (24)] with

\[x(t)=k_{1}x_{1}(t)+k_{2}x_{2}(t)\qquad\text{and}\qquad y(t)=k_{1}y_{1}(t)+k_{2 }y_{2}(t)\]

Therefore, when the input is \(k_{1}x_{1}(t)+k_{2}x_{2}(t)\), the system response is \(k_{1}y_{1}(t)+k_{2}y_{2}(t)\). Consequently, the system is linear. Using this argument, we can readily generalize the result to show that a system described by a differential equation of the form

\[a_{0}\frac{d^{N}y(t)}{dt^{N}}+a_{1}\frac{d^{N-1}y(t)}{dt^{N-1}}+\cdot\cdot\cdot+a _{N}y(t)=b_{N-M}\frac{d^{M}x(t)}{dt^{M}}+\cdot\cdot\cdot+b_{N-1}\frac{dx(t)}{dt} +b_{N}x(t) \tag{1.25}\]

is a linear system. The coefficients \(a_{i}\) and \(b_{i}\) in this equation can be constants or functions of time. Although here we proved only zero-state linearity, it can be shown that such systems are also zero-input linear and have the decomposition property.

**Drill 1.12**: **Linearity of a Differential Equation with Time-Varying Parameters**

Show that the system described by the following equation is linear:

\[\frac{dy(t)}{dt}+t^{2}y(t)=(2t+3)x(t)\]

**Drill 1.13**: **A Nonlinear Differential Equation**

Show that the system described by the following equation is nonlinear:

\[y(t)\frac{dy(t)}{dt}+3y(t)=x(t)\]

## More Comments on Linear Systems

Almost all systems observed in practice become nonlinear when large enough signals are applied to them. However, it is possible to approximate most of the nonlinear systems by linear systems for small-signal analysis. The analysis of nonlinear systems is generally difficult. Nonlinearities can arise in so many ways that describing them with a common mathematical form is impossible. Not only is each system a category in itself, but even for a given system, changes in initial conditions or input amplitudes may change the nature of the problem. On the other hand, the superposition property of linear systems is a powerful unifying principle that allows for a general solution. The superposition property (linearity) greatly simplifies the analysis of linear systems. Because of the decomposition property, we can evaluate separately the two components of the output. The zero-input response can be computed by assuming the input to be zero, and the zero-state response can be computed by assuming zero initial conditions. Moreover, if we express an input \(x(t)\) as a sum of simpler functions,

\[x(t)=a_{1}x_{1}(t)+a_{2}x_{2}(t)+\cdot\cdot\cdot+a_{m}x_{m}(t)\]

then, by virtue of linearity, the response \(y(t)\) is given by

\[y(t)=a_{1}y_{1}(t)+a_{2}y_{2}(t)+\cdot\cdot\cdot+a_{m}y_{m}(t)\]

where \(y_{k}(t)\) is the zero-state response to an input \(x_{k}(t)\). This apparently trivial observation has profound implications. As we shall see repeatedly in later chapters, it proves extremely useful and opens new avenues for analyzing linear systems.

For example, consider an arbitrary input \(x(t)\) such as the one shown in Fig. 1.27a. We can approximate \(x(t)\) with a sum of rectangular pulses of width \(\Delta t\) and of varying heights. The approximation improves as \(\Delta t\to 0\), when the rectangular pulses become impulses spaced \(\Delta t\) seconds apart (with \(\Delta t\to 0\)).2 Thus, an arbitrary input can be replaced by a weighted sum of impulses spaced \(\Delta t\) (\(\Delta t\to 0\)) seconds apart. Therefore, if we know the system response to a unit impulse, we can immediately determine the system response to an arbitrary input \(x(t)\) by adding the system response to each impulse component of \(x(t)\). A similar situation is depicted in Fig. 1.27b, where \(x(t)\) is approximated by a sum of step functions of varying magnitude and spaced \(\Delta t\) seconds apart. The approximation improves as \(\Delta t\) becomes smaller. Therefore, if we know the system response to a unit step input, we can compute the system response to any arbitrary input \(x(t)\) with relative ease. Time-domain analysis of linear systems (discussed in Ch. 2) uses this approach.

Footnote 2: Here, the discussion of a rectangular pulse approaching an impulse at \(\Delta t\to 0\) is somewhat imprecise. It is explained in Sec. 2.4 with more rigor.

Chapters 4, 5, 6, and 7 employ the same approach but instead use sinusoids or exponentials as the basic signal components. We show that any arbitrary input signal can be expressed as a weighted sum of sinusoids (or exponentials) having various frequencies. Thus a knowledge of the system response to a sinusoid enables us to determine the system response to an arbitrary input \(x(t)\).

Figure 1.27: Signal representation in terms of impulse and step components.

### 1.7-2 Time-Invariant and Time-Varying Systems

Systems whose parameters do not change with time are _time-invariant_ (also _constant-parameter_) systems. For such a system, if the input is delayed by \(T\) seconds, the output is the same as before but delayed by \(T\) (assuming initial conditions are also delayed by \(T\)). This property is expressed graphically in Fig. 1.28. We can also illustrate this property, as shown in Fig. 1.29. We can delay the output \(y(t)\) of a system \(\mathcal{S}\) by applying the output \(y(t)\) to a \(T\) second delay (Fig. 1.29a). If the system is time invariant, then the delayed output \(y(t-T)\) can also be obtained by first delaying the input \(x(t)\) before applying it to the system, as shown in Fig. 1.29b. In other words, the system \(\mathcal{S}\) and the time delay commute if the system \(\mathcal{S}\) is time invariant. This would not be true for time-varying systems. Consider, for instance, a time-varying system specified by \(y(t)=e^{-t}x(t)\). The output for such a system in Fig. 1.29a is \(e^{-(t-T)}x(t-T)\). In contrast, the output for the system in Fig. 1.29b is \(e^{-t}x(t-T)\).

Figure 1.28: Time-invariance property.

Figure 1.29: Illustration of time-invariance property.

It is possible to verify that the system in Fig. 26 is a time-invariant system. Networks composed of \(RLC\) elements and other commonly used active elements such as transistors are time-invariant systems. A system with an input-output relationship described by a linear differential equation of the form given in Ex. 10 [Eq. (25)] is a linear time-invariant (LTI) system when the coefficients \(a_{i}\) and \(b_{i}\) of such equation are constants. If these coefficients are functions of time, then the system is a linear _time-varying_ system.

The system described in Drill 12 is linear time varying. Another familiar example of a time-varying system is the carbon microphone, in which the resistance \(R\) is a function of the mechanical pressure generated by sound waves on the carbon granules of the microphone. The output current from the microphone is thus modulated by the sound waves, as desired.

**EXAMPLE 1.11 Assessing System Time Invariance**

Determine the time invariance of the following systems: **(a)**\(y(t)=x(t)u(t)\) and **(b)**\(y(t)=\frac{d}{dt}x(t)\).

**(a)** In this case, the output equals the input for \(t\geq 0\) and is otherwise zero. Clearly, the input is being modified by a time-dependent function, so the system is likely time variant. We can prove that the system is not time invariant through a counterexample. Letting \(x_{1}(t)=\delta(t+1)\), we see that \(y_{1}(t)=0\). However, \(x_{2}(t)=x_{1}(t-2)=\delta(t-1)\) produces an output of \(y_{2}(t)=\delta(t-1)\), which does equal \(y_{1}(t-2)=0\) as time-invariance would require. Thus, \(y(t)=x(t)u(t)\) is a time variant system.

**(b)** Although it appears that \(x(t)\) is being modified by a time-dependent function, this is not the case. The output of this system is simply the slope of the input. If the input is delayed, so too is the output. Applying input \(x(t)\) to the system produces output \(y(t)=\frac{d}{dt}x(t)\); delaying this output by \(T\) produces \(y(t-T)=\frac{d}{dt(t-T)}x(t-T)=\frac{d}{dt}x(t-T)\). This is just the output of the system to a delayed input \(x(t-T)\). Since the \(T\)-delayed output of the system to input \(x(t)\) equals the output of the system to the \(T\)-delayed input \(x(t-T)\), the system is time invariant.

**DRILL 1.14 A Time-Variant System**

Show that a system described by the following equation is a time-varying-parameter system:

\[y(t)=(\sin t)x(t-2)\]

[_Hint:_ Show that the system fails to satisfy the time-invariance property.]

**1.7-3 Instantaneous and Dynamic Systems**

As observed earlier, a system's output at any instant \(t\) generally depends on the entire past input. However, in a special class of systems, the output at any instant \(t\) depends only on its input at that instant. In resistive networks, for example, any output of the network at some instant \(t\) depends only on the input at the instant \(t\). In these systems, past history is irrelevant in determining the response. Such systems are said to be _instantaneous_ or _memoryless_ systems. More precisely, a system is said to be instantaneous (or memoryless) if its output at any instant \(t\) depends, at most, on the strength of its input(s) at the same instant \(t\), and not on any past or future values of the input(s). Otherwise, the system is said to be _dynamic_ (or a system with memory). A system whose response at \(t\) is completely determined by the input signals over the past \(T\) seconds [interval from \((t-T)\) to \(t\)] is a _finite-memory system_ with a memory of \(T\) seconds. Networks containing inductive and capacitive elements generally have infinite memory because the response of such networks at any instant \(t\) is determined by their inputs over the entire past \((-\infty,t)\). This is true for the \(RC\) circuit of Fig. 1.26.

**Example 1.12**: **Assessing System Memory**

Determine whether the following systems are memoryless: **(a)**\(y(t-1)=2x(t-1)\), **(b)**\(y(t)=\frac{d}{dt}x(t)\), and **(c)**\(y(t)=(t-1)x(t)\).

**(a)** In this case, the output at time \(t-1\) is just twice the input at the same time \(t-1\). Since the output at a particular time depends only on the strength of the input at the same time, the system is memoryless.

**(b)** Although it appears that the output \(y(t)\) at time \(t\) depends on the input \(x(t)\) at the same time \(t\), we know that the slope (derivative) of \(x(t)\) cannot be determined solely from a single point. There must be some memory, even if infinitesimally small, involved. This is confirmed by using the fundamental theorem of calculus to express the system as

\[y(t)=\lim_{T\to 0}\frac{x(t)-x(t-T)}{T}\]

Since the output at a particular time depends on more than just the input at the same time, the system is not memoryless.

**(c)** The output \(y(t)\) at time \(t\) is just the input \(x(t)\) at the same time \(t\) multiplied by the (time-dependent) coefficient \(t-1\). Since the output at a particular time depends only on the strength of the input at the same time, the system is memoryless.

### 1.7-4 Causal and Noncausal Systems

A _causal_ (also known as a _physical_ or _nonanticipative_) system is one for which the output at any instant \(t_{0}\) depends only on the value of the input \(x(t)\) for \(t\leq t_{0}\). In other words, the value of the output at the present instant depends only on the past and present values of the input \(x(t)\), not on its future values. To put it simply, in a causal system the output cannot start before the input is applied. If the response starts before the input, it means that the system knows the input in the future and acts on this knowledge before the input is applied. A system that violates the condition of causality is called a _noncausal_ (or _anticipative_) system.

Any practical system that operates in real time+ must necessarily be causal. We do not yet know how to build a system that can respond to future inputs (inputs not yet applied). A noncausal system is a prophetic system that knows the future input and acts on it in the present. Thus, if we apply an input starting at \(t=0\) to a noncausal system, the output would begin even before \(t=0\). For example, consider the system specified by

Footnote †: margin: \(\diamondsuit\)

\[y(t)=x(t-2)+x(t+2) \tag{26}\]

For the input \(x(t)\) illustrated in Fig. 30a, the output \(y(t)\), as computed from Eq. (26) (shown in Fig. 30b), starts even before the input is applied. Equation (26) shows that \(y(t)\), the output at \(t\), is given by the sum of the input values 2 seconds before and 2 seconds after \(t\) (at \(t-2\) and \(t+2\), respectively). But if we are operating the system in real time at \(t\), we do not know what the value of the input will be 2 seconds later. Thus it is impossible to implement this system in real time. For this reason, noncausal systems are unrealizable in _real time_.

## 13 Assessing System Causality

Determine whether the following systems are causal: **(a)**\(y(t)=x(-t)\), **(b)**\(y(t)=x(t+1)\), and **(c)**\(y(t+1)=x(t)\).

Figure 30: Input–output of a noncausal system and the causal output achieved by delay.

**(a)** Here, the output is a reflection of the input. We can easily use a counterexample to disprove the causality of this system. The input \(x(t)=\delta(t-1)\), which is nonzero at \(t=1\), produces an output \(y(t)=\delta(t+1)\), which is nonzero at \(t=-1\), a time \(2\) seconds earlier than the input! Clearly the system is not causal.

**(b)** In this case, the output at time \(t\) depends on the input at future time of \(t+1\). Clearly the system is not causal.

**(c)** In this case, the output at time \(t+1\) depends on the input one second in the past, at time \(t\). Since the output does not depend on future values of the input, the system is causal.

### Why Study Noncausal Systems?

The foregoing discussion may suggest that noncausal systems have no practical purpose. This is not the case; they are valuable in the study of systems for several reasons. First, noncausal systems _are_ realizable when the independent variable is other than "time" (e.g., _space_). Consider, for example, an electric charge of density \(q(x)\) placed along the \(x\) axis for \(x\geq 0\). This charge density produces an electric field \(E(x)\) that is present at every point on the \(x\) axis from \(x=-\infty\) to \(\infty\). In this case the input [i.e., the charge density \(q(x)\)] starts at \(x=0\), but its output [the electric field \(E(x)\)] begins before \(x=0\). Clearly, this space-charge system is noncausal. This discussion shows that only temporal systems (systems with time as independent variable) must be causal to be realizable. The terms "before" and "after" have a special connection to causality only when the independent variable is time. This connection is lost for variables other than time. Nontemporal systems, such as those occurring in optics, can be noncausal and still realizable.

Moreover, even for temporal systems, such as those used for signal processing, the study of noncausal systems is important. In such systems we may have all input data prerecorded. This often happens with speech, geophysical, and meteorological signals, and with space probes. In such cases, the input's future values are available to us. For example, suppose we had a set of input signal records available for the system described by Eq. (26). We can then compute \(y(t)\) since, for any \(t\), we need only refer to the records to find the input's value \(2\) seconds before and \(2\) seconds after \(t\). Thus, noncausal systems can be realized, although not in real time. We may therefore be able to realize a noncausal system, provided we are willing to accept a time delay in the output. Consider a system whose output \(\hat{y}(t)\) is the same as \(y(t)\) in Eq. (26) delayed by \(2\) seconds (Fig. 30c), so that

\[\hat{y}(t)=y(t-2)=x(t-4)+x(t)\]

Here the value of the output \(\hat{y}\) at any instant \(t\) is the sum of the values of the input \(x\) at \(t\) and at the instant \(4\) seconds earlier [at \((t-4)\)]. In this case, the output at any instant \(t\) does not depend on future values of the input, and the system is causal. The output of this system, which is \(\hat{y}(t)\), is identical to that in Eq. (26) or Fig. 30b except for a delay of \(2\) seconds. Thus, a noncausal system may be realized or satisfactorily approximated in real time by using a causal system with a delay.

A third reason for studying noncausal systems is that they provide an upper bound on the performance of causal systems. For example, if we wish to design a filter for separating a signal from noise, then the optimum filter is invariably a noncausal system. Although unrealizable, thisnoncausal system's performance acts as the upper limit on what can be achieved and gives us a standard for evaluating the performance of causal filters.

At first glance, noncausal systems may seem to be inscrutable. Actually, there is nothing mysterious about these systems and their approximate realization through physical systems with delay. If we want to know what will happen one year from now, we have two choices: go to a prophet (an unrealizable person) who can give the answers instantly, or go to a wise man and allow him a delay of one year to give us the answer! If the wise man is truly wise, he may even be able, by studying trends, to shrewdly guess the future very closely with a delay of less than a year. Such is the case with noncausal systems--nothing more and nothing less.

**DRIL 1.15**: **A Noncausal System**

Show that a system described by the following equation is noncausal:

\[y(t)=\int_{t-5}^{t+5}x(\tau)\,d\tau\]

Show that this system can be realized physically if we accept a delay of 5 seconds in the output.

### 1.7-5 Continuous-Time and Discrete-Time Systems

Signals defined or specified over a continuous range of time are _continuous-time signals,_ denoted by symbols \(x(t)\), \(y(t)\), and so on. Systems whose inputs and outputs are continuous-time signals are _continuous-time systems_. On the other hand, signals defined only at discrete instants of time \(t_{0},t_{1},t_{2},\ldots,t_{n},\ldots\) are _discrete-time signals,_ denoted by the symbols \(x(t_{n})\), \(y(t_{n})\), and so on, where \(n\) is some integer. Systems whose inputs and outputs are discrete-time signals are _discrete-time systems_. A digital computer is a familiar example of this type of system. In practice, discrete-time signals can arise from sampling continuous-time signals. For example, when the sampling is uniform, the discrete instants \(t_{0},t_{1},\)\(t_{2},\)\(\ldots\) are uniformly spaced so that

\[t_{k+1}-t_{k}=T\qquad\text{for all }k\]

In such case, the discrete-time signals represented by the samples of continuous-time signals \(x(t),y(t),\) and so on can be expressed as \(x(nT),\)\(y(nT),\) and so on; for convenience, we further simplify this notation to \(x[n],\)\(y[n],\)\(\ldots,\) where it is understood that \(x[n]=x(nT)\) and that \(n\) is some integer. A typical discrete-time signal is shown in Fig. 1.31. A discrete-time signal may also be viewed as a sequence of numbers \(\ldots,\)\(x[-1],\)\(x[0],\)\(x[1],\)\(x[2],\)\(\ldots.\) Thus, a discrete-time system may be seen as processing a sequence of numbers \(x[n]\) and yielding as an output another sequence of numbers \(y[n].\)

Discrete-time signals arise naturally in situations that are inherently discrete time, such as population studies, amortization problems, national income models, and radar tracking. They may also arise as a result of sampling continuous-time signals in sampled data systems, digital filtering, and the like. Digital filtering is a particularly interesting application in which continuous-time signals are processed by using discrete-time systems, as shown in Fig. 1.32. A continuous-time signal \(x(t)\) is first sampled to convert it into a discrete-time signal \(x[n],\) which then is processed by the discrete-time system to yield a discrete-time output \(y[n].\) A continuous-time signal \(y(t)\) is finally constructed from \(y[n].\) In this manner, we can process a continuous-time signal with an appropriate discrete-time system such as a digital computer. Because discrete-time systems have several significant advantages over continuous-time systems, there is an accelerating trend toward processing continuous-time signals with discrete-time systems.

Figure 1.32: Processing continuous-time signals by discrete-time systems.

Figure 1.31: A discrete-time signal.

### 1.7-6 Analog and Digital Systems

Analog and digital signals are discussed in Sec. 1.3-2. A system whose input and output signals are analog is an _analog system;_ a system whose input and output signals are digital is a _digital system_. A digital computer is an example of a digital (binary) system. Observe that a digital computer is a digital as well as a discrete-time system.

### 1.7-7 Invertible and Noninvertible Systems

A system \(\mathcal{S}\) performs certain operation(s) on input signal(s). If we can obtain the input \(x(t)\) back from the corresponding output \(y(t)\) by some operation, the system \(\mathcal{S}\) is said to be _invertible_. When several different inputs result in the same output (as in a rectifier), it is impossible to obtain the input from the output, and the system is _noninvertible_. Therefore, for an invertible system, it is essential that every input have a unique output so that there is a one-to-one mapping between an input and the corresponding output. The system that achieves the inverse operation [of obtaining \(x(t)\) from \(y(t)\)] is the _inverse system_ for \(\mathcal{S}\). For instance, if \(\mathcal{S}\) is an ideal integrator, then its inverse system is an ideal differentiator. Consider a system \(\mathcal{S}\) connected in tandem with its inverse \(\mathcal{S}_{i}\), as shown in Fig. 1.33. The input \(x(t)\) to this tandem system results in signal \(y(t)\) at the output of \(\mathcal{S}\), and the signal \(y(t)\), which now acts as an input to \(\mathcal{S}_{i}\), yields back the signal \(x(t)\) at the output of \(\mathcal{S}_{i}\). Thus, \(\mathcal{S}_{i}\) undoes the operation of \(\mathcal{S}\) on \(x(t)\), yielding back \(x(t)\). A system whose output is equal to the input (for all possible inputs) is an _identity_ system. Cascading a system with its inverse system, as shown in Fig. 1.33, results in an identity system.

In contrast, a rectifier, specified by an equation \(y(t)=|x(t)|\), is noninvertible because the rectification operation cannot be undone.

Inverse systems are very important in signal processing. In many applications, the signals are distorted during the processing, and it is necessary to undo the distortion. For instance, in transmission of data over a communication channel, the signals are distorted owing to non-ideal frequency response and finite bandwidth of a channel. It is necessary to restore the signal as closely as possible to its original shape. Such equalization is also used in audio systems and photographic systems.

### 1.14 Assessing System Invertibility

Determine whether the following systems are invertible: **(a)**\(y(t)=x(-t)\), **(b)**\(y(t)=tx(t)\), and **(c)**\(y(t)=\frac{d}{dt}x(t)\).

**(a)** Here, the output is a reflection of the input, which does not cause any loss to the input. The input can, in fact, be exactly recovered by simply reflecting the output [\(x(t)=y(-t)\)], which is to say that a reflecting system is its own inverse. Thus, \(y(t)=x(-t)\) is an invertible system.

Figure 1.33: A cascade of a system with its inverse results in an identity system.

**(b)** In this case, one might be tempted to recover the input from the output as \(x(t)=\frac{1}{t}y(t)\). This approach works almost everywhere, except at \(t=0\) where the input value \(x(0)\) cannot be recovered. Due to this single lost point, the system \(y(t)=tx(t)\) is not invertible.

**(c)** Differentiation eliminates any dc component. For example, the inputs \(x_{1}(t)=1\) and \(x_{2}(t)=2\) both produce the same output \(y(t)=0\). Given only \(y(t)=0\), it is impossible to know if the original input was \(x_{1}(t)=1\), \(x_{2}(t)=2\), or something else entirely. Since unique inputs do produce unique outputs, we know that \(y(t)=\frac{d}{dt}x(t)\) is not an invertible system.

### 7-8 Stable and Unstable Systems

Systems can also be classified as _stable_ or _unstable_ systems. Stability can be _internal_ or _external_. If every _bounded input_ applied at the input terminal results in a _bounded output,_ the system is said to be stable _externally_. External stability can be ascertained by measurements at the external terminals (input and output) of the system. This type of stability is also known as the stability in the BIBO (bounded-input/bounded-output) sense. The concept of internal stability is postponed to Ch. 2 because it requires some understanding of internal system behavior, introduced in that chapter.

**EXAMPLE 1.15 Assessing System BIBO Stability**

Determine whether the following systems are BIBO-stable: **(a)**\(y(t)=x^{2}(t)\), **(b)**\(y(t)=tx(t)\), and **(c)**\(y(t)=\frac{d}{dt}x(t)\).

**(a)** This system squares an input to produce the output. If the input is bounded, which is to say that \(|x(t)|\leq M_{x}<\infty\) for all \(t\), then we see that

\[|y(t)|=|x^{2}(t)|=|x(t)|^{2}\leq M_{x}^{2}<\infty\]

Since the output amplitude is guaranteed to be bounded for any bounded-amplitude input, the system \(y(t)=x^{2}(t)\) is BIBO-stable.

**(b)** We can prove that \(y(t)=tx(t)\) is not BIBO-stable with a simple example. The bounded-amplitude input \(x(t)=u(t)\) produces the output \(y(t)=tu(t)\) whose amplitude grows to infinity as \(t\to\infty\). Thus, \(y(t)=tx(t)\) is a BIBO-unstable system.

**(c)** We can prove that \(y(t)=\frac{d}{dt}x(t)\) is not BIBO-stable with an example. The bounded-amplitude input \(x(t)=u(t)\) produces the output \(y(t)=\delta(t)\) whose amplitude is infinite at \(t=0\). Thus, \(y(t)=\frac{d}{dt}x(t)\) is a BIBO-unstable system.

**D****Drill 1.16 A Noninvertible BIBO-Stable System**

Show that a system described by the equation \(y(t)=x^{2}(t)\) is noninvertible but BIBO-stable.

## Chapter 1.8 System Model: Input-Output Description

A system description in terms of the measurements at the input and output terminals is called the _input-output description_. As mentioned earlier, systems theory encompasses a variety of systems, such as electrical, mechanical, hydraulic, acoustic, electromechanical, and chemical, as well as social, political, economic, and biological. The first step in analyzing any system is the construction of a system model, which is a mathematical expression or a rule that satisfactorily approximates the dynamical behavior of the system. In this chapter we shall consider only continuous-time systems. Modeling of discrete-time systems is discussed in Ch. 3.

### 1.8.1 Electrical Systems

To construct a system model, we must study the relationships between different variables in the system. In electrical systems, for example, we must determine a satisfactory model for the voltage-current relationship of each element, such as Ohm's law for a resistor. In addition, we must determine the various constraints on voltages and currents when several electrical elements are interconnected. These are the laws of interconnection--the well-known Kirchhoff laws for voltage and current (KVL and KCL). From all these equations, we eliminate unwanted variables to obtain equation(s) relating the desired output variable(s) to the input(s). The following examples demonstrate the procedure of deriving input-output relationships for some LTI electrical systems.

Application of Kirchhoff's voltage law around the loop yields

\[v_{L}(t)+v_{R}(t)+v_{C}(t)=x(t)\]

Figure 1.34: Circuit for Ex. 1.16.

By using the voltage-current laws of each element (inductor, resistor, and capacitor), we can express this equation as

\[\frac{dy(t)}{dt}+3y(t)+2\int_{-\infty}^{t}y(\tau)\,d\tau=x(t) \tag{1.27}\]

Differentiating both sides of this equation, we obtain

\[\frac{d^{2}y(t)}{dt^{2}}+3\frac{dy(t)}{dt}+2y(t)=\frac{dx(t)}{dt} \tag{1.28}\]

This differential equation is the input-output relationship between the output \(y(t)\) and the input \(x(t)\).

It proves convenient to use a compact notation \(D\) for the differential operator \(d/dt\). This notation can be repeatedly applied. Thus,

\[\frac{dy(t)}{dt}\equiv Dy(t),\qquad\frac{d^{2}y(t)}{dt^{2}}\equiv D^{2}y(t), \qquad\ldots,\qquad\frac{d^{N}y(t)}{dt^{N}}\equiv D^{N}y(t)\]

With this notation, Eq. (1.28) can be expressed as

\[(D^{2}+3D+2)y(t)=Dx(t) \tag{1.29}\]

The differential operator is the inverse of the integral operator, so we can use the operator \(1/D\) to represent integration.2

Footnote 2: Use of operator \(1/D\) for integration generates some subtle mathematical difficulties because the operators \(D\) and \(1/D\) do not commute. For instance, we know that \(D(1/D)=1\) because

\[\frac{d}{dt}\left[\int_{-\infty}^{t}y(\tau)\,d\tau\right]=y(t)\]

 However, \((1/D)D\) is not necessarily unity. Use of Cramer's rule in solving simultaneous integro-differential equations will always result in cancellation of operators \(1/D\) and \(D\). This procedure may yield erroneous results when the factor \(D\) occurs in the numerator as well as in the denominator. This happens, for instance, in circuits with all-inductor loops or all-capacitor cut sets. To eliminate this problem, avoid the integral operation in system equations so that the resulting equations are differential rather than integro-differential. In electrical circuits, this can be done by using charge (instead of current) variables in loops containing capacitors and choosing current variables for loops without capacitors. In the literature this problem of commutativity of \(D\) and \(1/D\) is largely ignored. As mentioned earlier, such a procedure gives erroneous results only in special systems, such as the circuits with all-inductor loops or all-capacitor cut sets. Fortunately such systems constitute a very small fraction of the systems we deal with. For further discussion of this topic and a correct method of handling problems involving integrals, see [4].

Consequently, Eq. (1.27) can be expressed as

\[\left(D+3+\frac{2}{D}\right)y(t)=x(t)\]

Multiplying both sides by \(D\) to differentiate the expression, we obtain

\[(D^{2}+3D+2)y(t)=Dx(t)\]

which is identical to Eq. (1.29).

Recall that Eq. (1.29) is not an algebraic equation, and \(D^{2}+3D+2\) is not an algebraic term that multiplies \(y(t)\); it is an operator that operates on \(y(t)\). It means that we must perform the following operations on \(y(t)\): take the second derivative of \(y(t)\) and add to it 3 times the first derivative of \(y(t)\) and 2 times \(y(t)\). Clearly, a polynomial in \(D\) multiplied by \(y(t)\) represents a certain differential operation on \(y(t)\).

**EXAMPLE 1.17**Input-Output Equation of a Series RC Circuit

Using operator notation, find the equation relating input to output for the series \(RC\) circuit of Fig. 1.35 if the input is the voltage \(x(t)\) and output is

**(a)**: the loop current \(i(t)\)
**(b)**: the capacitor voltage \(y(t)\)

**(a)** The loop equation for the circuit is

\[Ri(t)+\frac{1}{C}\int_{-\infty}^{t}i(\tau)\,d\tau=x(t)\]

or

\[15i(t)+5\int_{-\infty}^{t}i(\tau)\,d\tau=x(t)\]

With operator notation, this equation can be expressed as

\[15i(t)+\frac{5}{D}i(t)=x(t) \tag{1.30}\]

Figure 1.35: Circuit for Ex. 1.17

**(b)** Multiplying both sides of Eq. (1.30) by \(D\) (i.e., differentiating the equation), we obtain

\[(15D+5)\,i(t)=Dx(t)\]

Using the fact that \(i(t)=C\,\frac{dy(t)}{dt}=\frac{1}{3}Dy(t)\), simple substitution yields

\[(3D+1)y(t)=x(t) \tag{1.31}\]

**Drill 1.17**Input-Output Equation of a Series \(RLC\) Circuit with

**Inductor Voltage as Output**

If the inductor voltage \(v_{L}(t)\) is taken as the output, show that the \(RLC\) circuit in Fig. 1.34 has an input-output equation of \((D^{2}+3D+2)v_{L}(t)=D^{2}x(t)\).

**Drill 1.18**Input-Output Equation of a Series \(RC\) Circuit with

**Capacitor Voltage as Output**

If the capacitor voltage \(v_{C}(t)\) is taken as the output, show that the \(RLC\) circuit in Fig. 1.34 has an input-output equation of \((D^{2}+3D+2)v_{C}(t)=2x(t)\).

### 1.8-2 Mechanical Systems

Planar motion can be resolved into translational (rectilinear) motion and rotational (torsional) motion. Translational motion will be considered first. We shall restrict ourselves to motions in one dimension.

Translational Systems

The basic elements used in modeling translational systems are ideal masses, linear springs, and dashpots providing viscous damping. The laws of various mechanical elements are now discussed.

For a \(mass\)\(M\) (Fig. 1.36a), a force \(x(t)\) causes a motion \(y(t)\) and acceleration \(\vec{y}(t)\). From Newton's law of motion,

\[x(t)=M\vec{y}(t)=M\frac{d^{2}y(t)}{dt^{2}}=MD^{2}y(t)\]

The force \(x(t)\) required to stretch (or compress) a _linear spring_ (Fig. 1.36b) by an amount \(y(t)\) is given by

\[x(t)=Ky(t)\]

where \(K\) is the _stiffness_ of the spring.

For _a linear dashpot_ (Fig. 36c), which operates by virtue of viscous friction, the force moving the dashpot is proportional to the relative velocity \(\dot{y}(t)\) of one surface with respect to the other. Thus

\[x(t)=B\dot{y}(t)=B\frac{dy(t)}{dt}=BDy(t)\]

where \(B\) is the _damping coefficient_ of the dashpot or the viscous friction.

**Example 1.18**: **Input-Output Equation for a Translational Mechanical System**

Find the input-output relationship for the translational mechanical system shown in Fig. 37a or its equivalent in Fig. 37b. The input is the force \(x(t)\), and the output is the mass position \(y(t)\).

**Figure 1.37**: Mechanical system for Ex. 1.18In mechanical systems it is helpful to draw a free-body diagram of each junction, which is a point at which two or more elements are connected. In Fig. 1.37, the point representing the mass is a junction. The displacement of the mass is denoted by \(y(t)\). The spring is also stretched by the amount \(y(t)\), and therefore it exerts a force \(-Ky(t)\) on the mass. The dashpot exerts a force \(-B\dot{y}(t)\) on the mass, as shown in the free-body diagram (Fig. 1.37c). By Newton's second law, the net force must be \(M\ddot{y}(t)\). Therefore,

\[M\ddot{y}(t)=-B\dot{y}(t)-Ky(t)+x(t)\]

or

\[(MD^{2}+BD+K)y(t)=x(t)\]

### 1.3 Rotational Systems

In rotational systems, the motion of a body may be defined as its motion about a certain axis. The variables used to describe rotational motion are torque (in place of force), angular position (in place of linear position), angular velocity (in place of linear velocity), and angular acceleration (in place of linear acceleration). The system elements are _rotational mass_ or _moment of inertia_ (in place of mass) and _torsional springs_ and _torsional dashpots_ (in place of linear springs and dashpots). The terminal equations for these elements are analogous to the corresponding equations for translational elements. If \(J\) is the moment of inertia (or rotational mass) of a rotating body about a certain axis, then the external torque required for this motion is equal to \(J\) (rotational mass) times the angular acceleration. If \(\theta(t)\) is the angular position of the body, \(\ddot{\theta}(t)\) is its angular acceleration, and

\[\text{torque}=J\ddot{\theta}(t)=J\frac{d^{2}\theta(t)}{dt^{2}}=JD^{2}\theta(t)\]

Similarly, if \(K\) is the stiffness of a torsional spring (per unit angular twist), and \(\theta\) is the angular displacement of one terminal of the spring with respect to the other, then

\[\text{torque}=K\theta(t)\]

Finally, the torque due to viscous damping of a torsional dashpot with damping coefficient \(B\) is

\[\text{torque}=B\dot{\theta}(t)=BD\theta(t)\]

**Example 1.19**: **Input-Output Equation for Aircraft Roll Angle**

The attitude of an aircraft can be controlled by three sets of surfaces (shown shaded in Fig. 1.38): elevators, rudder, and ailerons. By manipulating these surfaces, one can set the aircraft on a desired flight path. The roll angle \(\varphi(t)\) can be controlled by deflecting in the opposite direction the two aileron surfaces as shown in Fig. 1.38. Assuming only rolling motion, find the equation relating the roll angle \(\varphi(t)\) to the input (deflection) \(\theta(t)\).

The aileron surfaces generate a torque about the roll axis proportional to the aileron deflection angle \(\theta(t)\). Let this torque be \(c\theta(t)\), where \(c\) is the constant of proportionality. Air friction dissipates the torque \(B\dot{\varphi}(t)\). The torque available for rolling motion is then \(c\theta(t)-B\dot{\varphi}(t)\). If \(J\) is the moment of inertia of the plane about the \(x\) axis (roll axis), then

\[\text{net torque}=J\ddot{\varphi}(t)=c\theta(t)-B\dot{\varphi}(t)\]

and

\[J\frac{d^{2}\varphi(t)}{dt^{2}}+B\frac{d\varphi(t)}{dt}=c\,\theta(t)\qquad \text{or}\qquad(JD^{2}+BD)\varphi(t)=c\theta(t)\]

This is the desired equation relating the output (roll angle \(\varphi(t)\)) to the input (aileron angle \(\theta(t)\)).

The roll velocity \(\omega(t)\) is \(\dot{\varphi}(t)\). If the desired output is the roll velocity \(\omega(t)\) rather than the roll angle \(\varphi(t)\), then the input-output equation would be

\[J\frac{d\omega(t)}{dt}+B\omega(t)=c\theta(t)\qquad\text{or}\qquad(JD+B)\omega (t)=c\theta(t)\]

### 1.19 Input-Output Equation of a Rotational Mechanical System

Torque \(\mathcal{T}(t)\) is applied to the rotational mechanical system shown in Fig. 1.39a. The torsional spring stiffness is \(K\); the rotational mass (the cylinder's moment of inertia about the shaft) is \(J\); the viscous damping coefficient between the cylinder and the ground is \(B\). Find the equation

Figure 1.38: Attitude control of an airplane.

### 1.8.2 The \(\mathcal{T}(t)\)

The \(\mathcal{T}(t)\) is a \(\mathcal{T}(t)\)-invariant function of the form \(\mathcal{T}(t)=\mathcal{T}(t)\). The \(\mathcal{T}(t)\) is a \(\mathcal{T}(t)\)-invariant function of the form \(\mathcal{T}(t)=\mathcal{T}(t)\).

## Chapter 1 Introduction

### 1.9 Internal and External

Descriptions of a System

The input-output relationship of a system is an _external description_ of that system. We have found an external description (not the _internal description_) of systems in all the examples discussed so far. This may puzzle the reader because in each of these cases, we derived the input-output relationship by analyzing the internal structure of that system. Why is this not an internal description? What makes a description internal? Although it is true that we did find the input-output description by internal analysis of the system, we did so strictly for convenience. We could have obtained the input-output description by making observations at the external (input and output) terminals, for example, by measuring the output for certain inputs, such as an impulse or a sinusoid. A description that can be obtained from measurements at the external terminals (even when the rest of the system is sealed inside an inaccessible black box) is an external description. Clearly, the input-output description is an external description. What, then, is an internal description? An internal description is capable of providing complete information about all possible signals in the system. An external description may not give such complete information. An external description can always be found from an internal description, but the converse is not necessarily true. We shall now give an example to clarify the distinction between an external and an internal description.

Let the circuit in Fig. 1.41a with the input \(x(t)\) and the output \(y(t)\) be enclosed inside a "black box" with only the input and the output terminals accessible. To determine its external description, let us apply a known voltage \(x(t)\) at the input terminals and measure the resulting output voltage \(y(t)\).

Let us also assume that there is some initial charge \(Q_{0}\) present on the capacitor. The output voltage will generally depend on both, the input \(x(t)\) and the initial charge \(Q_{0}\). To compute the output resulting because of the charge \(Q_{0}\), assume the input \(x(t)=0\) (short across the input). In this case, the currents in the two 2 \(\Omega\) resistors in the upper and the lower branches at the output terminals are equal and opposite because of the balanced nature of the circuit. Clearly, the capacitor charge results in zero voltage at the output.1

Footnote 1: The output voltage \(y(t)\) resulting because of the capacitor charge [assuming \(x(t)=0\)] is the zero-input response, which, as argued above, is zero. The output component due to the input \(x(t)\) (assuming zero initial capacitor charge) is the zero-state response. Complete analysis of this problem is given later in Ex. 1.21.

Figure 1.40: Armature-controlled dc motor.

Now, to compute the output \(y(t)\) resulting from the input voltage \(x(t)\), we assume zero initial capacitor charge (short across the capacitor terminals). The current \(i(t)\) (Fig. 1.41a), in this case, divides equally between the two parallel branches because the circuit is balanced. Thus, the voltage across the capacitor continues to remain zero. Therefore, for the purpose of computing the current \(i(t)\), the capacitor may be removed or replaced by a short. The resulting circuit is equivalent to that shown in Fig. 1.41b, which shows that the input \(x(t)\) sees a load of \(5\,\Omega\), and

\[i(t)=\tfrac{1}{5}x(t)\]

Also, because \(y(t)=2\,i(t)\),

\[y(t)=\tfrac{2}{5}x(t)\]

This is the total response. Clearly, for the external description, the capacitor does not exist. No external measurement or external observation can detect the presence of the capacitor. Furthermore, if the circuit is enclosed inside a "black box" so that only the external terminals are accessible, it is impossible to determine the currents (or voltages) inside the circuit from external measurements or observations. An internal description, however, can provide every possible signal inside the system. In Ex. 1.21, we shall find the internal description of this system and show that it is capable of determining every possible signal in the system.

For most systems, the external and internal descriptions are equivalent, but there are a few exceptions, as in the present case, where the external description gives an inadequate picture of the system. This happens when the system is _uncontrollable_ and/or _unobservable_.

Figure 1.42 shows structural representations of simple uncontrollable and unobservable systems. In Fig. 1.42a, we note that part of the system (subsystem \(\mathcal{S}_{2}\)) inside the box cannot be controlled by the input \(x(t)\). In Fig. 1.42b, some of the system outputs (those in subsystem \(\mathcal{S}_{2}\)) cannot be observed from the output terminals. If we try to describe either of these systems by applying an external input \(x(t)\) and then measuring the output \(y(t)\), the measurement will not characterize the complete system but only the part of the system (here \(\mathcal{S}_{1}\)) that is both controllable

Figure 1.41: A system that cannot be described by external measurements.

and observable (linked to both the input and output). Such systems are undesirable in practice and should be avoided in any system design. The system in Fig. 1.41a can be shown to be neither controllable nor observable. It can be represented structurally as a combination of the systems in Figs. 1.42a and 1.42b.

### 1.10 Internal Description: The State-Space Description

We shall now introduce the _state-space_ description of a linear system, which is an internal description of a system. In this approach, we identify certain key variables, called the _state variables_, of the system. These variables have the property that every possible signal in the system can be expressed as a linear combination of these state variables. For example, we can show that every possible signal in a passive _RLC_ circuit can be expressed as a linear combination of independent capacitor voltages and inductor currents, which, therefore, are state variables for the circuit.

To illustrate this point, consider the network in Fig. 1.43. We identify two state variables: the capacitor voltage \(q_{1}\) and the inductor current \(q_{2}\). If the values of \(q_{1}\), \(q_{2}\), and the input \(x(t)\) are known at some instant \(t\), we can demonstrate that every possible signal (current or voltage) in the circuit can be determined at \(t\). For example, if \(q_{1}=10\), \(q_{2}=1\), and the input \(x=20\) at some instant, the remaining voltages and currents at that instant will be

\[i_{1}=(x-q_{1})/1=20-10=10\,\mathrm{A}\] \[v_{1}=x-q_{1}=20-10=10\,\mathrm{V}\] \[v_{2}=q_{1}=10\,\mathrm{V}\] \[i_{2}=q_{1}/2=5\,\mathrm{A}\] \[i_{C}=i_{1}-i_{2}-q_{2}=10-5-1=4\,\mathrm{A}\] \[i_{3}=q_{2}=1\,\mathrm{A}\] \[v_{3}=5q_{2}=5\,\mathrm{V}\] \[v_{L}=q_{1}-v_{3}=10-5=5\,\mathrm{V} \tag{1.33}\]

Thus all signals in this circuit are determined. Clearly, state variables consist of the _key variables_ in a system; a knowledge of the state variables allows one to determine every possible output of the system. Note that the _state-variable description is an internal description_ of a system because it is capable of describing all possible signals in the system.

Figure 1.42: Structures of uncontrollable and unobservable systems.

## Chapter 1 Signals and Systems

### 1.1 Introduction

The signal-to-noise ratio (SNR) of a signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noisefrom the output equations. In the input-output description, an \(N\)th-order system is described by an \(N\)th-order equation. In the state-variable approach, the same system is described by \(N\) simultaneous first-order state equations.1

Footnote 1: This assumes the system to be controllable and observable. If it is not, the input–output description equation will be of an order lower than the corresponding number of state equations.

**Example 1.21**: Controllability and Observability

Investigate the nature of state equations and the issue of controllability and observability for the circuit in Fig. 1.41a.

This circuit has only one capacitor and no inductors. Hence, there is only one state variable, the capacitor voltage \(q(t)\). Since \(C=1\) F, the capacitor current is \(\dot{q}\). There are two sources in this circuit: the input \(x(t)\) and the capacitor voltage \(q(t)\). The response due to \(x(t)\), assuming \(q(t)=0\), is the zero-state response, which can be found from Fig. 1.44a, where we have shorted the capacitor [\(q(t)=0\)]. The response due to \(q(t)\) assuming \(x(t)=0\), is the zero-input response, which can be found from Fig. 1.44b, where we have shorted \(x(t)\) to ensure \(x(t)=0\). It is now trivial to find both the components.

Figure 1.44a shows zero-state currents in every branch. It is clear that the input \(x(t)\) sees an effective resistance of 5 \(\Omega\), and, hence, the current through \(x(t)\) is \(x/5\) A, which divides in the two parallel branches, resulting in the current \(x/10\) through each branch.

Examining the circuit in Fig. 1.44b for the zero-input response, we note that the capacitor voltage is \(q\) and the current is \(\dot{q}\). We also observe that the capacitor sees two loops in parallel, each with resistance 4 \(\Omega\) and current \(\dot{q}/2\). Interestingly, the 3 \(\Omega\) branch is effectively shorted because the circuit is balanced, and thus the voltage across the terminals \(cd\) is zero. The total current in any branch is the sum of the currents in that branch in Figs. 1.44a and 1.44b (principle of superposition).

\[\begin{array}{llll}\mbox{Branch}&\mbox{Current}&\mbox{Voltage}\\ ca&\frac{x}{10}+\frac{\dot{q}}{2}& 2\biggl{(}\frac{x}{10}+\frac{\dot{q}}{2} \biggr{)}\\ cb&\frac{x}{10}-\frac{\dot{q}}{2}& 2\biggl{(}\frac{x}{10}-\frac{\dot{q}}{2} \biggr{)}\\ ad&\frac{x}{10}-\frac{\dot{q}}{2}& 2\biggl{(}\frac{x}{10}-\frac{\dot{q}}{2} \biggr{)}\\ bd&\frac{x}{10}+\frac{\dot{q}}{2}& 2\biggl{(}\frac{x}{10}+\frac{\dot{q}}{2} \biggr{)}\\ ec&\frac{x}{5}& 3\biggl{(}\frac{x}{5}\biggr{)}\\ ed&\frac{x}{5}& x\end{array} \tag{1.35}\]To find the state equation, we note that the current in branch \(ca\) is \((x/10)+\dot{q}/2\) and the current in branch \(cb\) is \((x/10)-\dot{q}/2\). Hence, the equation around the loop \(acba\) is

\[q=2\biggl{[}-\frac{x}{10}-\frac{\dot{q}}{2}\biggr{]}+2\biggl{[}\frac{x}{10}- \frac{\dot{q}}{2}\biggr{]}=-2\dot{q}\]

or

\[\dot{q}=-0.5q \tag{1.36}\]

This is the desired state equation.

Substitution of \(\dot{q}=-0.5q\) in Eq. (1.35) shows that every possible current and voltage in the circuit can be expressed in terms of the state variable \(q\) and the input \(x\), as desired. Hence, the set of Eq. (1.35) is the output equation for this circuit. Once we have solved the state equation [Eq. (1.36)] for \(q\), we can determine every possible output in the circuit.

The output \(y(t)\) is given by

\[y(t)=2\biggl{[}\frac{x}{10}-\frac{\dot{q}}{2}\biggr{]}+2\biggl{[}\frac{x}{10} +\frac{\dot{q}}{2}\biggr{]}=\frac{2}{5}x(t) \tag{1.37}\]

A little examination of the state and the output equations indicates the nature of this system. Equation (1.36) shows that the state \(q(t)\) is independent of the input \(x(t)\); hence the system state \(q\) cannot be controlled by the input. Moreover, Eq. (1.37) shows that the output \(y(t)\) does not depend on the state \(q(t)\). Thus, the system state cannot be observed from the output terminals. Hence, the system is neither controllable nor observable. Such is not the case of other systems examined earlier. Consider, for example, the circuit in Fig. 1.43. The state equations [Eq. (1.34)] show that the states are influenced by the input directly or indirectly. Hence, the system is controllable. Moreover, as Eq. (1.33) shows, every possible output is expressed in terms of the state variables and the input. Hence, the states are also observable.

Figure 1.44: Analysis of a system that is neither controllable nor observable.

State-space techniques are useful not just because of their ability to provide internal system description, but for several other reasons, including the following.

1. State equations of a system provide a mathematical model of great generality that can describe not just linear systems, but also nonlinear systems; not just time-invariant systems, but also time-varying parameter systems; not just SISO (single-input/single-output) systems, but also multiple-input/multiple-output (MIMO) systems. Indeed, state equations are ideally suited for the analysis, synthesis, and optimization of MIMO systems.
2. Compact matrix notation and the powerful techniques of linear algebra greatly facilitate complex manipulations. Without such features, many important results of the modern system theory would have been difficult to obtain. State equations can yield a great deal of information about a system even when they are not solved explicitly.
3. State equations lend themselves readily to digital computer simulation of complex systems of high order, with or without nonlinearities, and with multiple inputs and outputs.
4. For second-order systems (\(N=2\)), a graphical method called _phase-plane analysis_ can be used on state equations, whether they are linear or nonlinear.

The real benefits of the state-space approach, however, are realized for highly complex systems of large order. Much of the book is devoted to introduction of the basic concepts of linear systems analysis, which must necessarily begin with simpler systems without using the state-space approach. Chapter 10 deals with the state-space analysis of linear, time-invariant, continuous-time, and discrete-time systems.

**DRILL 1.20 State Equations for a Series RLC Circuit**

Write the state equations for the series _RLC_ circuit shown in Fig. 1.45, using the inductor current \(q_{1}(t)\) and the capacitor voltage \(q_{2}(t)\) as state variables. Express every voltage and current in this circuit as a linear combination of \(q_{1}\), \(q_{2}\), and \(x\).

**ANSWERS**

\(q_{1}=-3\,q_{1}-q_{2}+x\) and \(q_{2}=2\,q_{1}\).

**Figure 1.45 Circuit for Drill 1.20.**

### 1.11 MATLAB: Working with Functions

Working with functions is fundamental to signals and systems applications. MATLAB provides several methods of defining and evaluating functions. An understanding and proficient use of these methods are therefore necessary and beneficial.

### 1.11-1 Anonymous Functions

Many simple functions are most conveniently represented by using MATLAB anonymous functions. An anonymous function provides a symbolic representation of a function defined in terms of MATLAB operators, functions, or other anonymous functions. For example, consider defining the exponentially damped sinusoid\(f(t)=e^{-t}\cos(2\pi t)\).

>> f = @(t) exp(-t).*cos(2*pi*t); In this context, the @ symbol identifies the expression as an anonymous function, which is assigned a name of f. Parentheses following the @ symbol are used to identify the function's independent variables (input arguments), which in this case is the single time variable t. Input arguments, such as t, are local to the anonymous function and are not related to any workspace variables with the same names.

Once defined, \(f(t)\) can be evaluated simply by passing the input values of interest. For example,

>> t = 0; f(t) ans = 1 evaluates \(f(t)\) at \(t=0\), confirming the expected result of unity. The same result is obtained by passing \(t=0\) directly.

>> f(0) ans = 1 Vector inputs allow the evaluation of multiple values simultaneously. Consider the task of plotting \(f(t)\) over the interval (\(-2\leq t\leq 2\)). Gross function behavior is clear: \(f(t)\) should oscillate four times with a decaying envelope. Since accurate hand sketches are cumbersome, MATLAB-generated plots are an attractive alternative. As the following example illustrates, care must be taken to ensure reliable results.

Suppose vector t is chosen to include only the integers contained in (\(-2\leq t\leq 2\)), namely, [\(-2,-1,0,1,2\)].

>> t = (-2:2); This vector input is evaluated to form a vector output.

>> f(t) ans = 7.3891 2.7183 1.0000 0.3679 0.1353The plot command graphs the result, which is shown in Fig. 1.46.

``` >>plot(t,f(t)); >>xlabel('t');ylabel('f(t)');grid; Gridlines,addedbyusingthegridcommand,aidfeatureidentification.Unfortunately,theplotdoesnotillustratetheexpectedoscillatorybehavior.Morepointsarerequiredtoadequatelyrepresent\(f(t)\).

The question, then, is how many points is enough?+ If too few points are chosen, information is lost. If too many points are chosen, memory and time are wasted. A balance is needed. For oscillatory functions, plotting 20 to 200 points per oscillation is normally adequate. For the present case, t is chosen to give 100 points per oscillation.

Footnote †: margin: 100

>>t=(-2:0.01:2); Again, thefunctionisevaluatedandplotted.

Figure 1.46: \(f(t)=e^{-t}\cos{(2\pi t)}\)fort=(-2:2).

Figure 1.47: \(f(t)=e^{-t}\cos{(2\pi t)}\)fort=(-2:0.01:2).

* >>plot(t,f(t)); >>xlabel('t');ylabel('f(t)');grid; The result, shown in Fig. 1.47, is an accurate depiction of \(f(t)\).

### 1.11-2 Relational Operators and the Unit Step Function

The unit step function \(u(t)\) arises naturally in many practical situations. For example, a unit step can model the act of turning on a system. With the help of relational operators, anonymous functions can represent the unit step function.

In MATLAB, a relational operator compares two items. If the comparison is true, a logical true (1) is returned. If the comparison is false, a logical false (0) is returned. Sometimes called indicator functions, relational operators indicates whether a condition is true. Six relational operators are available: <, >, <=, >=, ==, and ~=.

The unit step function is readily defined using the >= relational operator.

>> u = @(t) 1.0.*(t>=0); Any function with a jump discontinuity, such as the unit step, is difficult to plot. Consider plotting \(u(t)\) by using t = (-2:2).

>> t = (-2:2);plot(t,u(t)); >>xlabel('t');ylabel('u(t)');

Two significant problems are apparent in the resulting plot, shown in Fig. 1.48. First, MATLAB automatically scales plot axes to tightly bound the data. In this case, this normally desirable feature obscures most of the plot. Second, MATLAB connects plot data with lines, making a true jump discontinuity difficult to achieve. The coarse resolution of vector t emphasizes the effect by showing an erroneous sloping line between \(t=-1\) and \(t=0\).

The first problem is corrected by vertically enlarging the bounding box with the axis command. The second problem is reduced, but not eliminated, by adding points to vector t.

Figure 1.48: \(u(t)\) for t = (-2:2).

* >> t = (-2:0.01:2); plot(t,u(t)); >> xlabel('t'); ylabel('u(t)'); >> axis([-2 2 -0.1 1.1]); The four-element vector argument of axis specifies \(x\) axis minimum, \(x\) axis maximum, \(y\) axis minimum, and \(y\) axis maximum, respectively. The improved results are shown in Fig. 1.49.

Relational operators can be combined using logical AND, logical OR, and logical negation: &, 1, and ~, respectively. For example, (t>0)&(t<1) and ~((t<=0)|(t>=1)) both test if \(0<t<1\). To demonstrate, consider defining and plotting the unit pulse \(p(t)=u(t)-u(t-1)\), as shown in Fig. 1.50:

>> p = @(t) 1.0.*((t>=0)&(t<1)); >> t = (-1:0.01:2); plot(t,p(t)); >> xlabel('t'); ylabel('p(t) = u(t)-u(t-1)'); >> axis([-1 2 -.1 1.1]); Since anonymous functions can be constructed using other anonymous functions, we could have used our previously defined unit step anonymous function to define \(p(t)\) as p = @(t) u(t)-u(t-1);.

Figure 1.49: \(u(t)\) for t = (-2:0.01:2) with axis modification.

Figure 1.50: \(p(t)=u(t)-u(t-1)\) over (\(-1\leq t\leq 2\)).

For scalar operands, MATLAB also supports two short-circuit logical constructs. A short-circuit logical AND is performed by using &&, and a short-circuit logical OR is performed by using \(|\!|\!|\). Short-circuit logical operators are often more efficient than traditional logical operators because they test the second portion of the expression only when necessary. That is, when scalar expression A is found false in (A&&B), scalar expression B is not evaluated, since a false result is already guaranteed. Similarly, scalar expression B is not evaluated when scalar expression A is found true in (A|B), since a true result is already guaranteed.

### 1.11-3 Visualizing Operations on the Independent Variable

Two operations on a function's independent variable are commonly encountered: shifting and scaling. Anonymous functions are well suited to investigate both operations.

Consider \(g(t)=f(t)u(t)=e^{-t}\cos{(2\pi t)}u(t)\), a causal version of \(f(t)\). MATLAB easily multiplies anonymous functions. Thus, we create \(g(t)\) by multiplying our anonymous functions for \(f(t)\) and \(u(t)\).1

Footnote 1: Although we define g in terms of f and u, the function g will not change if we later change either f or u, unless we subsequently redefine g as well.

>> g = @(t) f(t).*u(t);

A combined shifting and scaling operation is represented by \(g(at+b)\), where \(a\) and \(b\) are arbitrary real constants. As an example, consider plotting \(g(2t+1)\) over \((-2\leq t\leq 2)\). With \(a=2\), the function is compressed by a factor of 2, resulting in twice the oscillations per unit \(t\). Adding the condition \(b>0\) shifts the waveform to the left. Given anonymous function g, an accurate plot is nearly trivial to obtain.

>> t = (-2:0.01:2); >> plot(t,g(2*t+1)); xlabel('t'); ylabel('g(2t+1)'); grid;

Figure 1.51 confirms the expected waveform compression and left shift. As a final check, realize that function \(g(\cdot)\) turns on when the input argument is zero. Therefore, \(g(2t+1)\) should turn on when \(2t+1=0\) or at \(t=-0.5\), a fact again confirmed by Fig. 1.51.

Figure 1.51: \(g(2t+1)\) over \((-2\leq t\leq 2)\).

Next, consider plotting \(g(-t+1)\) over \((-2\leq t\leq 2)\). Since \(a<0\), the waveform will be reflected. Adding the condition \(b>0\) shifts the final waveform to the right.

>> plot(t,g(-t+1)); xlabel('t'); ylabel('g(-t+1)'); grid; Figure 1.52 confirms both the reflection and the right shift.

Up to this point, Figs. 1.51 and 1.52 could be reasonably sketched by hand. Consider plotting the more complicated function \(h(t)=g(2t+1)+g(-t+1)\) over \((-2\leq t\leq 2)\) (Fig. 1.53); an accurate hand sketch would be quite difficult. With MATLAB, the work is much less burdensome.

>> plot(t,g(2*t+1)+g(-t+1)); xlabel('t'); ylabel('h(t)'); grid;

### 1.11-4 Numerical Integration and Estimating Signal Energy

Interesting signals often have nontrivial mathematical representations. Computing signal energy, which involves integrating the square of these expressions, can be a daunting task. Fortunately, many difficult integrals can be accurately estimated by means of numerical integration techniques.

Figure 1.52: \(g(-t+1)\) over \((-2\leq t\leq 2)\).

Figure 1.53: \(h(t)=g(2t+1)+g(-t+1)\) over \((-2\leq t\leq 2)\).

Even if the integration appears simple, numerical integration provides a good way to verify analytical results.

To start, consider the simple signal \(x(t)=e^{-t}(u(t)-u(t-1))\). The energy of \(x(t)\) is expressed as \(E_{x}=\int_{-\infty}^{\infty}|x(t)|^{2}\,dt=\int_{0}^{1}e^{-2t}\,dt\). Integrating yields \(E_{x}=0.5(1-e^{-2})\approx 0.4323\). The energy integral can also be evaluated numerically. Figure 1.27 helps illustrate the simple method of rectangular approximation: evaluate the integrand at points uniformly separated by \(\Delta t\), multiply each by \(\Delta t\) to compute rectangle areas, and then sum over all rectangles. First, we create function \(x(t)\).

>> x = @(t) exp(-t).*((t>=0)&(t<1)); With \(\Delta t=0.01\), a suitable time vector is created.

>> t = (0:0.01:1); The final result is computed by using the sum command.

>> E_x = sum(x(t).*x(t)*0.01) E_x = 0.4367 The result is not perfect, but at 1% relative error it is close. By reducing \(\Delta t\), the approximation is improved. For example, \(\Delta t=0.001\) yields E_x = 0.4328, or 0.1% relative error.

Although simple to visualize, rectangular approximation is not the best numerical integration technique. The MATLAB function quad implements a better numerical integration technique called recursive adaptive Simpson quadrature.2 To operate, quad requires a function describing the integrand, the lower limit of integration, and the upper limit of integration. Notice that no \(\Delta t\) needs to be specified.

Footnote 2: A comprehensive treatment of numerical integration is outside the scope of this text. Details of this particular method are not important for the current discussion; it is sufficient to say that it is better than the rectangular approximation.

To use quad to estimate \(E_{x}\), the integrand must first be described.

>> x_squared = @(t) x(t).*x(t); Estimating \(E_{x}\) immediately follows.

>> E_x = quad(x_squared,0,1) E_x = 0.4323 In this case, the relative error is \(-0.0026\%\).

The same techniques can be used to estimate the energy of more complex signals. Consider \(g(t)\), defined previously. Energy is expressed as \(E_{g}=\int_{0}^{\infty}e^{-2t}\cos^{2}\left(2\pi\,t\right)dt\). A closed-form solution exists, but it takes some effort. MATLAB provides an answer more quickly.

>> g_squared = @(t) g(t).*g(t);Although the upper limit of integration is infinity, the exponentially decaying envelope ensures \(g(t)\) is effectively zero well before \(t=100\). Thus, an upper limit of \(t=100\) is used along with \(\Delta t=0.001\).

>> t = (0:0.001:100); >> E_g = sum(g_squared(t)*0.001)  E_g = 0.2567 A slightly better approximation is obtained with the quad function.

>> E_g = quad(g_squared,0,100)  E_g = 0.2562

**DRIL 1.21** **Computing Signal Energy with MATLAB**

Use MATLAB to confirm that the energy of signal \(h(t)\), defined previously as \(h(t)=g(2t+1)+g(-t+1)\), is \(E_{h}=0.3768\).

### 1.12 Summary

A _signal_ is a set of data or information. A _system_ processes input signals to modify them or extract additional information from them to produce output signals (response). A system may be made up of physical components (hardware realization), or it may be an algorithm that computes an output signal from an input signal (software realization).

A convenient measure of the size of a signal is its energy, if it is finite. If the signal energy is infinite, the appropriate measure is its power, if it exists. The signal power is the time average of its energy (averaged over the entire time interval from \(-\infty\) to \(\infty\)). For periodic signals, the time averaging need be performed over only one period in view of the periodic repetition of the signal. Signal power is also equal to the mean squared value of the signal (averaged over the entire time interval from \(t=-\infty\) to \(\infty\)).

Signals can be classified in several ways.

1. A _continuous-time signal_ is specified for a continuum of values of the independent variable (such as time \(t\)). A _discrete-time signal_ is specified only at a finite or a countable set of time instants.
2. An _analog signal_ is a signal whose amplitude can take on any value over a continuum. On the other hand, a signal whose amplitudes can take on only a finite number of values is a _digital signal_. The terms _discrete-time_ and _continuous-time_ qualify the nature of a signal along the time axis (horizontal axis). The terms _analog_ and _digital,_ on the other hand, qualify the nature of the signal amplitude (vertical axis).
3. A _periodic signal_\(x(t)\) is defined by the fact that \(x(t)=x(t+T_{0})\) for some \(T_{0}\). The smallest positive value of \(T_{0}\) for which this relationship is satisfied is called the _fundamental period_. A periodic signal remains unchanged when shifted by an integer multiple of its period. A periodic signal \(x(t)\) can be generated by a periodic extension of any contiguous segment of \(x(t)\) of duration \(T_{0}\). Finally, a periodic signal, by definition, must exist over the entire time interval \(-\infty<t<\infty\). A signal is _aperiodic_ if it is not periodic.

4. An _everlasting signal_ starts at \(t=-\infty\) and continues forever to \(t=\infty\). Hence, periodic signals are everlasting signals. A _causal signal_ is a signal that is zero for \(t<0\).
5. A signal with finite energy is an _energy signal_. Similarly a signal with a finite and nonzero power (mean-square value) is a _power signal_. A signal can be either an energy signal or a power signal, but not both. However, there are signals that are neither energy nor power signals.
6. A signal whose physical description is known completely in a mathematical or graphical form is a _deterministic signal_. A _random signal_ is known only in terms of its probabilistic description such as mean value or mean-square value, rather than by its mathematical or graphical form.

A signal \(x(t)\) delayed by \(T\) seconds (right-shifted) can be expressed as \(x(t-T)\); on the other hand, \(x(t)\) advanced by \(T\) (left-shifted) is \(x(t+T)\). A signal \(x(t)\) time-compressed by a factor \(a\left(a>1\right)\) is expressed as \(x(at)\); on the other hand, the same signal time-expanded by factor \(a\left(a>1\right)\) is \(x(t/a)\). The signal \(x(t)\) when time-reversed can be expressed as \(x(-t)\).

The unit step function \(u(t)\) is very useful in representing causal signals and signals with different mathematical descriptions over different intervals.

In the classical (Dirac) definition, the unit impulse function \(\delta(t)\) is characterized by unit area and is concentrated at a single instant \(t=0\). The impulse function has a sampling (or sifting) property, which states that the area under the product of a function with a unit impulse is equal to the value of that function at the instant at which the impulse is located (assuming the function to be continuous at the impulse location). In the modern approach, the impulse function is viewed as a generalized function and is defined by the sampling property.

The exponential function \(e^{st}\), where \(s\) is complex, encompasses a large class of signals that includes a constant, a monotonic exponential, a sinusoid, and an exponentially varying sinusoid.

A real signal that is symmetrical about the vertical axis (\(t=0\)) is an _even_ function of time, and a real signal that is antisymmetrical about the vertical axis is an _odd_ function of time. The product of an even function and an odd function is an odd function. However, the product of an even function and an even function or an odd function and an odd function is an even function. The area under an odd function from \(t=-a\) to \(a\) is always zero regardless of the value of \(a\). On the other hand, the area under an even function from \(t=-a\) to \(a\) is two times the area under the same function from \(t=0\) to \(a\) (or from \(t=-a\) to \(0\)). Every signal can be expressed as a sum of odd and even functions of time.

A system processes input signals to produce output signals (response). The input is the cause, and the output is its effect. In general, the output is affected by two causes: the internal conditions of the system (such as the initial conditions) and the external input.

Systems can be classified in several ways.

1. Linear systems are characterized by the linearity property, which implies superposition; if several causes (such as various inputs and initial conditions) are acting on a linear system, the total output (response) is the sum of the responses from each cause, assuming that all the remaining causes are absent. A system is nonlinear if superposition does not hold.
2. In time-invariant systems, system parameters do not change with time. The parameters of time-varying-parameter systems change with time.
3. For memoryless (or instantaneous) systems, the system response at any instant \(t\) depends only on the value of the input at \(t\). For systems with memory (also known as dynamic systems), the system response at any instant \(t\) depends not only on the present value of the input, but also on the past values of the input (values before \(t\)).
4. In contrast, if a system response at \(t\) also depends on the future values of the input (values of input beyond \(t\)), the system is noncausal. In causal systems, the response does not depend on the future values of the input. Because of the dependence of the response on the future values of input, the effect (response) of noncausal systems occurs before the cause. When the independent variable is time (temporal systems), the noncausal systems are prophetic systems, and therefore, unrealizable, although close approximation is possible with some time delay in the response. Noncausal systems with independent variables other than time (e.g., space) are realizable.
5. Systems whose inputs and outputs are continuous-time signals are continuous-time systems; systems whose inputs and outputs are discrete-time signals are discrete-time systems. If a continuous-time signal is sampled, the resulting signal is a discrete-time signal. We can process a continuous-time signal by processing the samples of the signal with a discrete-time system.
6. Systems whose inputs and outputs are analog signals are analog systems; those whose inputs and outputs are digital signals are digital systems.
7. If we can obtain the input \(x(t)\) back from the output \(y(t)\) of a system \(\mathcal{S}\) by some operation, the system \(\mathcal{S}\) is said to be invertible. Otherwise the system is noninvertible.
8. A system is stable if bounded input produces bounded output. This defines external stability because it can be ascertained from measurements at the external terminals of the system. External stability is also known as the stability in the BIBO (bounded-input/bounded-output) sense. Internal stability, discussed later in Ch. 2, is measured in terms of the internal behavior of the system.

The system model derived from a knowledge of the internal structure of the system is its internal description. In contrast, an external description is a representation of a system as seen from its input and output terminals; it can be obtained by applying a known input and measuring the resulting output. In the majority of practical systems, an external description of a system so obtained is equivalent to its internal description. At times, however, the external description fails to describe the system adequately. Such is the case with the so-called uncontrollable or unobservable systems.

A system may also be described in terms of certain set of key variables called state variables. In this description, an \(N\)th-order system can be characterized by a set of \(N\) simultaneous first-order differential equations in \(N\) state variables. State equations of a system represent an internal description of that system.

## References

* [1] Papoulis, A., _The Fourier Integral and Its Applications_. McGraw-Hill, New York, 1962.
* [2] Mason, S. J., _Electronic Circuits, Signals, and Systems_. Wiley, New York, 1960.
* [3] Kailath, T., _Linear Systems_. Prentice-Hall, Englewood Cliffs, NJ, 1980.
* [4] Lathi, B. P., _Signals and Systems_. Berkeley-Cambridge Press, Carmichael, CA, 1987.