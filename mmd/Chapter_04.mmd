## Chapter 4 Continuous-Time System Analysis Using the Laplace Transform

Because of the linearity (superposition) property of linear time-invariant systems, we can find the response of these systems by breaking the input \(x(t)\) into several components and then summing the system response to all the components of \(x(t)\). We have already used this procedure in time-domain analysis, in which the input \(x(t)\) is broken into impulsive components. In the _frequency-domain analysis_ developed in this chapter, we break up the input \(x(t)\) into exponentials of the form \(e^{st}\), where the parameter \(s\) is the complex frequency of the signal \(e^{st}\), as explained in Sec. 4.3. This method offers an insight into the system behavior complementary to that seen in the time-domain analysis. In fact, the time-domain and the frequency-domain methods are duals of each other.

The tool that makes it possible to represent arbitrary input \(x(t)\) in terms of exponential components is the _Laplace transform_, which is discussed in the following section.

### 4.1 The Laplace Transform

For a signal \(x(t)\), its Laplace transform \(X(s)\) is defined by

\[X(s)=\int_{-\infty}^{\infty}x(t)e^{-st}\,dt \tag{4.1}\]

The signal \(x(t)\) is said to be the _inverse Laplace transform_ of \(X(s)\). It can be shown that

\[x(t)=\frac{1}{2\pi j}\int_{c-j\infty}^{c+j\infty}X(s)e^{st}\,ds \tag{4.2}\]

where \(c\) is a constant chosen to ensure the convergence of the integral in Eq. (4.1), as explained later. See also [1].

This pair of equations is known as the _bilateral Laplace transform pair_, where \(X(s)\) is the direct Laplace transform of \(x(t)\) and \(x(t)\) is the inverse Laplace transform of \(X(s)\). Symbolically,

\[X(s)=\mathcal{L}[x(t)]\qquad\text{and}\qquad x(t)=\mathcal{L}^{-1}[X(s)]\]

Note that

\[\mathcal{L}^{-1}\{\mathcal{L}[x(t)]\}=x(t)\qquad\text{and}\qquad\mathcal{L}\{ \mathcal{L}^{-1}[X(s)]\}=X(s)\]It is also common practice to use a bidirectional arrow to indicate a Laplace transform pair, as follows:

\[x(t)\Longleftrightarrow X(s)\]

The Laplace transform, defined in this way, can handle signals existing over the entire time interval from \(-\infty\) to \(\infty\) (causal and noncausal signals). For this reason it is called the _bilateral_ (or _two-sided_) Laplace transform. Later we shall consider a special case--the _unilateral_ or _one-sided_ Laplace transform--which can handle only causal signals.

Linearity of the Laplace Transform

We now prove that the Laplace transform is a linear operator by showing that the principle of superposition holds, implying that if

\[x_{1}(t)\Longleftrightarrow X_{1}(s)\qquad\text{and}\qquad x_{2}(t) \Longleftrightarrow X_{2}(s)\]

then

\[a_{1}x_{1}(t)+a_{2}x_{2}(t)\Longleftrightarrow a_{1}X_{1}(s)+a_{2}X_{2}(s)\]

The proof is simple. By definition,

\[\mathcal{L}[a_{1}x_{1}(t)+a_{2}x_{2}(t)] =\int_{-\infty}^{\infty}[a_{1}x_{1}(t)+a_{2}x_{2}(t)]e^{-st}\,dt\] \[=a_{1}\int_{-\infty}^{\infty}x_{1}(t)e^{-st}\,dt+a_{2}\int_{- \infty}^{\infty}x_{2}(t)e^{-st}\,dt\] \[=a_{1}X_{1}(s)+a_{2}X_{2}(s) \tag{4.3}\]

This result can be extended to any finite sum.

The Region of Convergence (ROC)

The _region of convergence_ (ROC), also called the region of existence, for the Laplace transform, \(X(s)\), is the set of values of \(s\) (the region in the complex plane) for which the integral in Eq. (4.1) converges. This concept will become clear in the following example.

**Example 4.1**: **Laplace Transform and ROC of a Causal Exponential**

For a signal \(x(t)=e^{-at}u(t)\), find the Laplace transform \(X(s)\) and its ROC.

By definition,

\[X(s)=\int_{-\infty}^{\infty}e^{-at}u(t)e^{-st}\,dt\]

Because \(u(t)=0\) for \(t<0\) and \(u(t)=1\) for \(t\geq 0\),

\[X(s)=\int_{0}^{\infty}e^{-at}e^{-st}\,dt=\int_{0}^{\infty}e^{-(s+a)t}\,dt=- \frac{1}{s+a}e^{-(s+a)t}\bigg{|}_{0}^{\infty} \tag{4.4}\]Note that \(s\) is complex and as \(t\to\infty\), the term \(e^{-(s+a)t}\) does not necessarily vanish. Here we recall that for a complex number \(z=\alpha+j\beta\),

\[e^{-zt}=e^{-(\alpha+j\beta)t}=e^{-\alpha t}e^{-j\beta t}\]

Now \(|e^{-j\beta t}|=1\) regardless of the value of \(\beta t\). Therefore, as \(t\to\infty\), \(e^{-zt}\to 0\) only if \(\alpha>0\), and \(e^{-zt}\to\infty\) if \(\alpha<0\). Thus,

\[\lim_{t\to\infty}e^{-zt}=\begin{cases}0&\text{Re }z>0\\ \infty&\text{Re }z<0\end{cases} \tag{4.5}\]

Clearly,

\[\lim_{t\to\infty}e^{-(s+a)t}=\begin{cases}0&\text{Re}(s+a)>0\\ \infty&\text{Re}(s+a)<0\end{cases}\]

Use of this result in Eq. (4.4) yields

\[X(s)=\frac{1}{s+a}\qquad\text{Re}(s+a)>0\]

or

\[e^{-at}u(t)\Longleftrightarrow\frac{1}{s+a}\qquad\text{Re }s>-a \tag{4.6}\]

The ROC of \(X(s)\) is Re \(s>-a\), as shown in the shaded area in Fig. 4.1a. This fact means that the integral defining \(X(s)\) in Eq. (4.4) exists only for the values of \(s\) in the shaded region in Fig. 4.1a. For other values of \(s\), the integral in Eq. (4.4) does not converge. For this reason, the shaded region is called the _ROC_ (or the _region of existence_) for \(X(s)\).

Figure 4.1: Signals **(a)**\(e^{-at}u(t)\) and **(b)**\(-e^{-at}u(-t)\) have the same Laplace transform but different regions of convergence.

### Region of Convergence for Finite-Duration Signals

A finite-duration signal \(x_{f}(t)\) is a signal that is nonzero only for \(t_{1}\leq t\leq t_{2}\), where both \(t_{1}\) and \(t_{2}\) are finite numbers and \(t_{2}>t_{1}\). For a finite-duration, absolutely integrable signal, the ROC is the entire \(s\) plane. This is clear from the fact that if \(x_{f}(t)\) is absolutely integrable and a finite-duration signal, then \(x(t)e^{-\sigma t}\) is also absolutely integrable for any value of \(\sigma\) because the integration is over the finite range of \(t\) only. Hence, the Laplace transform of such a signal converges for every value of \(s\). This means that the ROC of a general signal \(x(t)\) remains unaffected by the addition of any absolutely integrable, finite-duration signal \(x_{f}(t)\) to \(x(t)\). In other words, if \({\cal R}\) represents the ROC of a signal \(x(t)\), then the ROC of a signal \(x(t)+x_{f}(t)\) is also \({\cal R}\).

### Role of the Region of Convergence

The ROC is required for evaluating the inverse Laplace transform \(x(t)\) from \(X(s)\), as defined by Eq. (4.2). The operation of finding the inverse transform requires an integration in the complex plane, which needs some explanation. The path of integration is along \(c+j\omega\), with \(\omega\) varying from \(-\infty\) to \(\infty\).2 Moreover, the path of integration must lie in the ROC (or existence) for \(X(s)\). For the signal \(e^{-at}u(t)\), this is possible if \(c>-a\). One possible path of integration is shown (dotted) in Fig. 4.1a. Thus, to obtain \(x(t)\) from \(X(s)\), the integration in Eq. (4.2) is performed along this path. When we integrate \([1/(s+a)]e^{st}\) along this path, the result is \(e^{-at}u(t)\). Such integration in the complex plane requires a background in the theory of functions of complex variables. We can avoid this integration by compiling a table of Laplace transforms (Table 4.1), where the Laplace transform pairs are tabulated for a variety of signals. To find the inverse Laplace transform of, say, \(1/(s+a)\), instead of using the complex integral of Eq. (4.2), we look up the table and find the inverse Laplace transform to be \(e^{-at}u(t)\) (assuming that the ROC is Re \(s>-a\)). Although the table given here is rather short, it comprises the functions of most practical interest. A more comprehensive table appears in Doetsch [2].

Footnote 2: The discussion about the path of convergence is rather complicated, requiring the concepts of contour integration and understanding of the theory of complex variables. For this reason, the discussion here is somewhat simplified.

### The Unilateral Laplace Transform

To understand the need for defining unilateral transform, let us find the Laplace transform of signal \(x(t)\) illustrated in Fig. 4.1b:

\[x(t)=-e^{-at}u(-t)\]

The Laplace transform of this signal is

\[X(s)=\int_{-\infty}^{\infty}-e^{-at}u(-t)e^{-st}dt\]

Because \(u(-t)=1\) for \(t<0\) and \(u(-t)=0\) for \(t>0\),

\[X(s)=\int_{-\infty}^{0}-e^{-at}e^{-st}dt=-\int_{-\infty}^{0}e^{-(s+a)t}dt= \left.\frac{1}{s+a}e^{-(s+a)t}\right|_{-\infty}^{0}\]

\begin{table}
\begin{tabular}{l l l} \hline
**No.** & \(\mathbf{x(t)}\) & \(\mathbf{X(s)}\) \\ \hline
1 & \(\delta(t)\) & 1 \\
2 & \(u(t)\) & \(\dfrac{1}{s}\) \\
3 & \(tu(t)\) & \(\dfrac{1}{s^{2}}\) \\
4 & \(t^{n}u(t)\) & \(\dfrac{n!}{s^{n+1}}\) \\
5 & \(e^{\lambda t}u(t)\) & \(\dfrac{1}{s-\lambda}\) \\
6 & \(te^{\lambda t}u(t)\) & \(\dfrac{1}{(s-\lambda)^{2}}\) \\
7 & \(t^{n}e^{\lambda t}u(t)\) & \(\dfrac{n!}{(s-\lambda)^{n+1}}\) \\
8a & \(\cos bt\,u(t)\) & \(\dfrac{s}{s^{2}+b^{2}}\) \\
8b & \(\sin bt\,u(t)\) & \(\dfrac{b}{s^{2}+b^{2}}\) \\
9a & \(e^{-at}\cos bt\,u(t)\) & \(\dfrac{s+a}{(s+a)^{2}+b^{2}}\) \\
9b & \(e^{-at}\sin bt\,u(t)\) & \(\dfrac{b}{(s+a)^{2}+b^{2}}\) \\
10a & \(re^{-at}\cos\left(bt+\theta\right)u(t)\) & \(\dfrac{(r\cos\theta)s+(ar\cos\theta-br\sin\theta)}{s^{2}+2as+(a^{2}+b^{2})}\) \\
10b & \(re^{-at}\cos\left(bt+\theta\right)u(t)\) & \(\dfrac{0.5re^{i\theta}}{s+a-jb}+\dfrac{0.5re^{-j\theta}}{s+a+jb}\) \\
10c & \(re^{-at}\cos\left(bt+\theta\right)u(t)\) & \(\dfrac{As+B}{s^{2}+2as+c}\) \\  & \(r=\sqrt{\dfrac{A^{2}c+B^{2}-2ABa}{c-a^{2}}}\) \\  & \(\theta=\tan^{-1}\left(\dfrac{Aa-B}{A\sqrt{c-a^{2}}}\right)\) \\  & \(b=\sqrt{c-a^{2}}\) \\
10d & \(e^{-at}\Bigg{[}A\cos bt+\dfrac{B-Aa}{b}\sin bt\Bigg{]}u(t)\) & \(\dfrac{As+B}{s^{2}+2as+c}\) \\  & \(b=\sqrt{c-a^{2}}\) \\ \hline \end{tabular}
\end{table}
Table 4.1: Select (Unilateral) Laplace Transform PairsEquation (4.5) shows that

\[\lim_{t\to-\infty}e^{-(s+a)t}=0\qquad{\rm Re}\ (s+a)<0\]

Hence,

\[X(s)=\frac{1}{s+a}\qquad{\rm Re}\ s<-a\]

The signal \(-e^{-at}u(-t)\) and its ROC (\({\rm Re}\ s<-a\)) are depicted in Fig. 4.1b. Note that the Laplace transforms for the signals \(e^{-at}u(t)\) and \(-e^{-at}u(-t)\) are identical except for their regions of convergence. Therefore, for a given \(X(s)\), there may be more than one inverse transform, depending on the ROC. In other words, unless the ROC is specified, there is no one-to-one correspondence between \(X(s)\) and \(x(t)\). This fact increases the complexity in using the Laplace transform. The complexity is the result of trying to handle causal as well as noncausal signals. If we restrict all our signals to the causal type, such an ambiguity does not arise. There is only one inverse transform of \(X(s)=1/(s+a)\), namely, \(e^{-at}u(t)\). To find \(x(t)\) from \(X(s)\), we need not even specify the ROC. In summary, if all signals are restricted to the causal type, then, for a given \(X(s)\), there is only one inverse transform \(x(t)\).2

Footnote 2: Actually, \(X(s)\) specifies \(x(t)\) within a null function \(n(t)\), which has the property that the area under \(|n(t)|^{2}\) is zero over any finite interval \(0\) to \(t\) (\(t>0\)) (Lerch’s theorem). For example, if two functions are identical everywhere except at finite number of points, they differ by a null function.

The unilateral Laplace transform is a special case of the bilateral Laplace transform in which all signals are restricted to being causal; consequently, the limits of integration for the integral in Eq. (4.1) can be taken from \(0\) to \(\infty\). Therefore, the unilateral Laplace transform \(X(s)\) of a signal \(x(t)\) is defined as

\[X(s)=\int_{0^{-}}^{\infty}x(t)e^{-st}\,dt \tag{4.7}\]

We choose \(0^{-}\) (rather than \(0^{+}\) used in some texts) as the lower limit of integration. This convention not only ensures inclusion of an impulse function at \(t=0\), but also allows us to use initial conditions at \(0^{-}\) (rather than at \(0^{+}\)) in the solution of differential equations via the Laplace transform. In practice, we are likely to know the initial conditions before the input is applied (at \(0^{-}\)), not after the input is applied (at \(0^{+}\)). Indeed, the very meaning of the term "initial conditions" implies conditions at \(t=0^{-}\) (conditions before the input is applied). Detailed analysis of desirability of using \(t=0^{-}\) appears in Sec. 4.3.

The unilateral Laplace transform simplifies the system analysis problem considerably because of its _uniqueness property_, which says that for a given \(X(s)\), there is a unique inverse transform. But there is a price for this simplification: we cannot analyze noncausal systems or use noncausal inputs. However, in most practical problems, this restriction is of little consequence. For this reason, we shall first consider the unilateral Laplace transform and its application to system analysis. (The bilateral Laplace transform is discussed later, in Sec. 4.11.)

Basically there is no difference between the unilateral and the bilateral Laplace transform. The unilateral transform is the bilateral transform that deals with a subclass of signals starting at \(t=0\) (causal signals). Therefore, the expression [Eq. (4.2)] for the inverse Laplace transform remains unchanged. In practice, the term _Laplace transform_ means _the unilateral Laplace transform_.

## Chapter 4 Continuous-time system analysis

### 4.1 Existence of the Laplace Transform

The variable \(s\) in the Laplace transform is complex in general, and it can be expressed as \(s=\sigma+j\omega\). By definition,

\[X(s)=\int_{0^{-}}^{\infty}x(t)e^{-st}\,dt=\int_{0^{-}}^{\infty}[x(t)e^{-\sigma t }]e^{-j\omega t}\,dt\]

Because \(|e^{j\omega t}|=1\), the integral on the right-hand side of this equation converges if

\[\int_{0^{-}}^{\infty}\left|x(t)e^{-\sigma t}\right|\,dt<\infty \tag{4.8}\]

Hence the existence of the Laplace transform is guaranteed if the integral in Eq. (4.8) is finite for some value of \(\sigma\). Any signal that grows no faster than an exponential signal \(Me^{\omega_{0}t}\) for some \(M\) and \(\sigma_{0}\) satisfies the condition of Eq. (4.8). Thus, if for some \(M\) and \(\sigma_{0}\),

\[|x(t)|\leq Me^{\sigma_{0}t} \tag{4.9}\]

we can choose \(\sigma>\sigma_{0}\) to satisfy Eq. (4.8).+ The signal \(e^{t^{2}}\), in contrast, grows at a rate faster than \(e^{\sigma_{0}t}\), and consequently is not Laplace-transformable.++ Fortunately such signals (which are not Laplace-transformable) are of little consequence from either a practical or a theoretical viewpoint. If \(\sigma_{0}\) is the smallest value of \(\sigma\) for which the integral in Eq. (4.8) is finite, \(\sigma_{0}\) is called the _abscissa of convergence_ and the ROC of \(X(s)\) is Re \(s>\sigma_{0}\). The abscissa of convergence for \(e^{-\sigma t}u(t)\) is \(-a\) (the ROC is Re \(s>-a\)).

Footnote †: The condition of Eq. (4.9) is sufficient but not necessary for the existence of the Laplace transform. For example, \(x(t)=1/\sqrt{t}\) is infinite at \(t=0\), and Eq. (4.9) cannot be satisfied; but the transform of \(1/\sqrt{t}\) exists and is given by \(\sqrt{\pi/s}\).

Footnote ‡: However, if we consider a truncated (finite-duration) signal \(e^{t^{2}}\), the Laplace transform exists.

**Example 4.2**: **Bilateral Laplace Transform of Common Causal Signals**

Determine the Laplace transform of the following: **(a)**\(\delta(t)\), **(b)**\(u(t)\), and **(c)**\(\cos\omega_{0}t\,u(t)\).

**(a)**

\[{\cal L}[\delta(t)]=\int_{0^{-}}^{\infty}\delta(t)e^{-\pi}\,dt\]

Using the sampling property [Eq. (1.11) with \(T=0\)], we obtain

\[{\cal L}[\delta(t)]=1\qquad\mbox{for all $s$}\]

that is,

\[\delta(t)\Longleftrightarrow 1\qquad\mbox{for all $s$}\]

**(b)** To find the Laplace transform of \(u(t)\), recall that \(u(t)=1\) for \(t\geq 0\). Therefore,

\[\mathcal{L}\left[u(t)\right] =\int_{0^{-}}^{\infty}u(t)e^{-st}dt=\int_{0^{-}}^{\infty}e^{-st}dt =\left.-\frac{1}{s}e^{-st}\right|_{0^{-}}^{\infty}\] \[=\frac{1}{s}\qquad\text{Re}\;s>0\]

We also could have obtained this result from Eq. (4.6) by letting \(a=0\).

**(c)** Because \(\cos\,\omega_{0}tu(t)=\frac{1}{2}[e^{i\omega_{0}t}+e^{-j\omega_{0}t}]u(t)\), we know that

\[\mathcal{L}\left[\cos\,\omega_{0}tu(t)\right]=\frac{1}{2}\,\mathcal{L}[e^{i \omega_{0}t}u(t)+e^{-j\omega_{0}t}u(t)]\]

From Eq. (4.6), it follows that

\[\mathcal{L}[\cos\,\omega_{0}tu(t)] =\frac{1}{2}\Bigg{[}\frac{1}{s-j\omega_{0}}+\frac{1}{s+j\omega_{ 0}}\Bigg{]}\qquad\text{Re}\;(s\pm j\omega)=\text{Re}\,s>0\] \[=\frac{s}{s^{2}+{\omega_{0}}^{2}}\qquad\text{Re}\;s>0 \tag{4.10}\]

For the unilateral Laplace transform, there is a unique inverse transform of \(X(s)\); consequently, there is no need to specify the ROC explicitly. For this reason, we shall generally ignore any mention of the ROC for unilateral transforms. Recall, also, that in the unilateral Laplace transform it is understood that every signal \(x(t)\) is zero for \(t<0\), and it is appropriate to indicate this fact by multiplying the signal by \(u(t)\).

**Drill 4.1 Bilateral Laplace Transform of Gate Functions**

By direct integration, find the Laplace transform \(X(s)\) and the region of convergence of \(X(s)\) for the gate functions shown in Fig. 4.2.

## Answers

* \(\frac{1}{s}(1-e^{-2s})\) for all \(s\)
* \(\frac{1}{s}(1-e^{-2s})e^{-2s}\) for all \(s\)

Figure 4.2: Gate functions for Drill 4.1.

### 4.1-1 Finding the Inverse Transform

Finding the inverse Laplace transform by using Eq. (4.2) requires integration in the complex plane, a subject beyond the scope of this book (but see, e.g., [3]). For our purpose, we can find the inverse transforms from Table 4.1. All we need is to express \(X(s)\) as a sum of simpler functions of the forms listed in the table. Most of the transforms \(X(s)\) of practical interest are _rational functions_, that is, ratios of polynomials in \(s\). Such functions can be expressed as a sum of simpler functions by using partial fraction expansion (see Sec. B.5).

Values of \(s\) for which \(X(s)=0\) are called the _zeros_ of \(X(s)\); the values of \(s\) for which \(X(s)\to\infty\) are called the _poles_ of \(X(s)\). If \(X(s)\) is a rational function of the form \(P(s)/Q(s)\), the roots of \(P(s)\) are the zeros and the roots of \(Q(s)\) are the poles of \(X(s)\).

**Example 4.3**: **Inverse Unilateral Laplace Transform**

Find the inverse unilateral Laplace transforms of

\[\begin{array}{ll}&\mbox{\bf(a)}\ \ \frac{7s-6}{s^{2}-s-6}\\ &\mbox{\bf(b)}\ \ \frac{2s^{2}+5}{s^{2}+3s+2}\\ &\mbox{\bf(c)}\ \ \frac{6(s+34)}{s(s^{2}+10s+34)}\\ &\mbox{\bf(d)}\ \ \frac{8s+10}{(s+1)(s+2)^{3}}\end{array}\]

In no case is the inverse transform of these functions directly available in Table 4.1. Rather, we need to expand these functions into partial fractions, as discussed in Sec. B.5-1. Today, it is very easy to find partial fractions via software such as MATLAB. However, just as the availability of a calculator does not obviate the need for learning the mechanics of arithmetical operations (addition, multiplication, etc.), the widespread availability of computers does not eliminate the need to learn the mechanics of partial fraction expansion.

**(a)**

\[X(s)=\frac{7s-6}{(s+2)(s-3)}=\frac{k_{1}}{s+2}+\frac{k_{2}}{s-3}\]

To determine \(k_{1}\), corresponding to the term \((s+2)\), we cover up (conceal) the term \((s+2)\) in \(X(s)\) and substitute \(s=-2\) (the value of \(s\) that makes \(s+2=0\)) in the remaining expression (see Sec. B.5-2):

\[k_{1}=\frac{7s-6}{(s+2)(s-3)}\bigg{|}_{s=-2}=\frac{-14-6}{-2-3}=4\]

Similarly, to determine \(k_{2}\) corresponding to the term \((s-3)\), we cover up the term \((s-3)\) in \(X(s)\) and substitute \(s=3\) in the remaining expression

\[k_{2}=\frac{7s-6}{(s+2)(s-3)}\bigg{|}_{s=3}=\frac{21-6}{3+2}=3\]Therefore,

\[X(s)=\frac{7s-6}{(s+2)(s-3)}=\frac{4}{s+2}+\frac{3}{s-3} \tag{4.11}\]

Checking the Answer

It is easy to make a mistake in partial fraction computations. Fortunately it is simple to check the answer by recognizing that \(X(s)\) and its partial fractions must be equal for every value of \(s\) if the partial fractions are correct. Let us verify this assertion in Eq. (4.11) for some convenient value, say, \(s=0\). Substitution of \(s=0\) in Eq. (4.11) yields+

Footnote †: \({}^{\dagger}\) Because \(X(s)=\infty\) at its poles, we should avoid the pole values (\(-2\) and \(3\) in the present case) for checking. The answers may check even if partial fractions are wrong. This situation can occur when two or more errors cancel their effects. But the chances of this problem arising for randomly selected values of \(s\) are extremely small.

\[1=2-1=1\]

We can now be sure of our answer with a high margin of confidence. Using pair \(5\) of Table 4.1 in Eq. (4.11), we obtain

\[x(t)=\mathcal{L}^{-1}\left(\frac{4}{s+2}+\frac{3}{s-3}\right)=(4e^{-2t}+3e^{3 t})u(t)\]

**(b)**: \[X(s)=\frac{2s^{2}+5}{s^{2}+3s+2}=\frac{2s^{2}+5}{(s+1)(s+2)}\]

Observe that \(X(s)\) is an improper function with \(M=N\). In such a case, we can express \(X(s)\) as a sum of the coefficient of the highest power in the numerator plus partial fractions corresponding to the poles of \(X(s)\) (see Sec. B.5-5). In the present case, the coefficient of the highest power in the numerator is \(2\). Therefore,

\[X(s)=2+\frac{k_{1}}{s+1}+\frac{k_{2}}{s+2}\]

where

\[k_{1}=\frac{2s^{2}+5}{(s+1)(s+2)}\bigg{|}_{s=-1}=\frac{2+5}{-1+2}=7\]

and

\[k_{2}=\frac{2s^{2}+5}{(s+1)(s+2)}\bigg{|}_{s=-2}=\frac{8+5}{-2+1}=-13\]

Therefore,

\[X(s)=2+\frac{7}{s+1}-\frac{13}{s+2}\]

From Table 4.1, pairs \(1\) and \(5\), we obtain

\[x(t)=2\delta(t)+(7e^{-t}-13e^{-2t})u(t)\]\[X(s) =\frac{6(s+34)}{s(s^{2}+10s+34)}=\frac{6(s+34)}{s(s+5-j3)(s+5+j3)}\] \[=\frac{k_{1}}{s}+\frac{k_{2}}{s+5-j3}+\frac{k_{2}^{*}}{s+5+j3}\]

Note that the coefficients (\(k_{2}\) and \(k_{2}^{*}\)) of the conjugate terms must also be conjugate (see Sec. B.5). Now

\[k_{1} =\frac{6(s+34)}{s(s^{2}+10s+34)}\bigg{|}_{s=0}=\frac{6\times 34}{34 }=6\] \[k_{2} =\frac{6(s+34)}{s\left(\underline{s+5-j3}\right)(s+5+j3)}\bigg{|} _{s=-5+j3}=\frac{29+j3}{-3-j5}=-3+j4\]

Therefore,

\[k_{2}^{*}=-3-j4\]

To use pair 10b of Table 4.1, we need to express \(k_{2}\) and \(k_{2}^{*}\) in polar form.

\[-3+j4=\left(\sqrt{3^{2}+4^{2}}\right)e^{j\tan^{-1}(4/-3)}=5e^{j\tan^{-1}(4/-3)}\]

Observe that \(\tan^{-1}(4/-3)\neq\tan^{-1}(-4/3)\). This fact is evident in Fig. 4.3. For further discussion of this topic, see Ex. B.1.

From Fig. 4.3, we observe that

\[k_{2}=-3+j4=5e^{j126.9^{\circ}}\]

so

\[k_{2}^{*}=5e^{-j126.9^{\circ}}\]

Figure 4.3: Visualizing \(\tan^{-1}(-4/3)\neq\tan^{-1}(4/-3)\).

Therefore, \[X(s)=\frac{6}{s}+\frac{5e^{i126.9^{\circ}}}{s+5-j3}+\frac{5e^{-j126.9^{\circ}}}{s+5 +j3}\] From Table 4.1 (pairs 2 and 10b), we obtain \[x(t)=[6+10e^{-5t}\cos{(3t+126.9^{\circ})}]u(t)\]

Alternative Method Using Quadratic Factors

The foregoing procedure involves considerable manipulation of complex numbers. Pair 10c (Table 4.1) indicates that the inverse transform of quadratic terms (with complex conjugate poles) can be found directly without having to find first-order partial fractions. We discussed such a procedure in Sec. B.5-2. For this purpose, we shall express \(X(s)\) as

\[X(s)=\frac{6(s+34)}{s(s^{2}+10s+34)}=\frac{k_{1}}{s}+\frac{As+B}{s^{2}+10s+34}\]

We have already determined that \(k_{1}=6\) by the (Heaviside) "cover-up" method. Therefore,

\[\frac{6(s+34)}{s(s^{2}+10s+34)}=\frac{6}{s}+\frac{As+B}{s^{2}+10s+34}\]

Clearing the fractions by multiplying both sides by \(s(s^{2}+10s+34)\) yields

\[6(s+34)=(6+A)s^{2}+(60+B)s+204\]

Now, equating the coefficients of \(s^{2}\) and \(s\) on both sides yields

\[A=-6\qquad\text{and}\qquad B=-54\]

and

\[X(s)=\frac{6}{s}+\frac{-6s-54}{s^{2}+10s+34}\]

We now use pairs 2 and 10c to find the inverse Laplace transform. The parameters for pair 10c are \(A=-6\), \(B=-54\), \(a=5\), \(c=34\), \(b=\sqrt{c-a^{2}}=3\), and

\[r=\sqrt{\frac{A^{2}c+B^{2}-2ABa}{c-a^{2}}}=10\qquad\theta=\tan^{-1}\frac{Aa-B} {A\sqrt{c-a^{2}}}=126.9^{\circ}\]

Therefore,

\[x(t)=[6+10e^{-5t}\cos{(3t+126.9^{\circ})}]u(t)\]

which agrees with the earlier result.

Shortcuts

The partial fractions with quadratic terms also can be obtained by using shortcuts. We have

\[X(s)=\frac{6(s+34)}{s(s^{2}+10s+34)}=\frac{6}{s}+\frac{As+B}{s^{2}+10s+34}\]We can determine \(A\) by eliminating \(B\) on the right-hand side. This step can be accomplished by multiplying both sides of the equation for \(X(s)\) by \(s\) and then letting \(s\to\infty\). This procedure yields

\[0=6+A\quad\Longrightarrow\quad A=-6\]

Therefore,

\[\frac{6(s+34)}{s(s^{2}+10s+34)}=\frac{6}{s}+\frac{-6s+B}{s^{2}+10s+34}\]

To find \(B\), we let \(s\) take on any convenient value, say, \(s=1\), in this equation to obtain

\[\frac{210}{45}=6+\frac{B-6}{45}\quad\Longrightarrow\quad B=-54\]

a result that agrees with the answer found earlier.

**(d)**

\[X(s)=\frac{8s+10}{(s+1)(s+2)^{3}}=\frac{k_{1}}{s+1}+\frac{a_{0}}{(s+2)^{3}}+ \frac{a_{1}}{(s+2)^{2}}+\frac{a_{2}}{s+2}\]

where

\[k_{1}=\frac{8s+10}{(s+1)(s+2)^{3}}\bigg{|}_{s=-1}=2\] \[a_{0}=\frac{8s+10}{(s+1)(s+2)^{3}}\bigg{|}_{s=-2}=6\] \[a_{1}=\left\{\frac{d}{ds}\left[\frac{8s+10}{(s+1)(s+2)^{3}} \right]\right\}_{s=-2}=-2\] \[a_{2}=\frac{1}{2}\left\{\frac{d^{2}}{ds^{2}}\left[\frac{8s+10}{ (s+1)(s+2)^{3}}\right]\right\}_{s=-2}=-2\]

Therefore,

\[X(s)=\frac{2}{s+1}+\frac{6}{(s+2)^{3}}-\frac{2}{(s+2)^{2}}-\frac{2}{s+2}\]

and

\[x(t)=[2e^{-t}+(3t^{2}-2t-2)e^{-2t}]u(t)\]

Alternative Method: A Hybrid of Heaviside

and Clearing Fractions

In this method, the simpler coefficients \(k_{1}\) and \(a_{0}\) are determined by the Heaviside "cover-up" procedure, as discussed earlier. To determine the remaining coefficients, we use the clearing-fraction method. Using the values \(k_{1}=2\) and \(a_{0}=6\) obtained earlier by the Heaviside "cover-up" method, we have

\[\frac{8s+10}{(s+1)(s+2)^{3}}=\frac{2}{s+1}+\frac{6}{(s+2)^{3}}+\frac{a_{1}}{ (s+2)^{2}}+\frac{a_{2}}{s+2}\]

[MISSING_PAGE_FAIL:14]

### 4.4 Inverse Laplace Transform with MATLAB

Using the MATLAB residue command, determine the inverse Laplace transform of each of the following functions:

**(a)**: \(X_{a}(s)=\frac{2s^{2}+5}{s^{2}+3s+2}\)
**(b)**: \(X_{b}(s)=\frac{2s^{2}+7s+4}{(s+1)(s+2)^{2}}\)
**(c)**: \(X_{c}(s)=\frac{8s^{2}+21s+19}{(s+2)(s^{2}+s+7)}\)

In each case, we use the MATLAB residue command to perform the necessary partial fraction expansions. The inverse Laplace transform follows using Table 4.1.

**(a)**:

>> num = [2 0 5]; den = [1 3 2]; >> [r, p, k] = residue(num,den) r = -13 7 p = -2 -1 k = 2 Therefore, \(X_{a}(s)=-13/(s+2)+7/(s+1)+2\) and \(x_{a}(t)=(-13e^{-2t}+7e^{-t})u(t)+2\delta(t)\).

**(b)**:

>> num = [2 7 4]; den = [conv([1 1],conv([1 2],[1 2]))]; >> [r, p, k] = residue(num,den) r = 3 2 -1 p = -2 -2 -1 k = [] Therefore, \(X_{b}(s)=3/(s+2)+2/(s+2)^{2}-1/(s+1)\) and \(x_{b}(t)=(3e^{-2t}+2te^{-2t}-e^{-t})u(t)\).

**(c)** In this case, a few calculations are needed beyond the results of the residue command so that pair 10b of Table 4.1 can be utilized.

>> num = [8 21 19]; den = [conv([1 2],[1 1 7])]; >> [r, p, k]= residue(num,den)r = 3.5000-0.48113i  3.5000+0.48113i  1.0000  p = -0.5000+2.5981i  -0.5000-2.5981i  -2.0000  k = []  >> ang = angle(r), mag = abs(r)  ang = -0.13661  0.13661  0  mag = 3.5329  3.5329  1.0000  Thus, \[X_{c}(s)=\frac{1}{s+2}+\frac{3.5329e^{-\beta 0.13661}}{s+0.5-j2.5981}+\frac{3.5329e^{ \beta 0.13661}}{s+0.5+j2.5981}\] and \[x_{c}(t)=[e^{-2t}+1.7665e^{-0.5t}\cos(2.5981t-0.1366)]u(t).\]

**Example 4.5**: Symbolic Laplace and Inverse Laplace Transforms

**with MATLAB**

Using MATLAB's symbolic math toolbox, determine the following:

**(a)**: the direct unilateral Laplace transform of \(x_{a}(t)=\sin{(at)}+\cos{(bt)}\)
**(b)**: the inverse unilateral Laplace transform of \(X_{b}(s)=as^{2}/(s^{2}+b^{2})\)

**(a)**: Here, we use the sym command to symbolically define our variables and expression for \(x_{a}(t)\), and then we use the laplace command to compute the (unilateral) Laplace transform.

>> syms a b t; x_a = sin(a*t)+cos(b*t); >> X_a = laplace(x_a);  X_a = a/(a^2 + s^2) + s/(b^2 + s^2) Therefore, \(X_{a}(s)=\frac{a}{s^{2}+a^{2}}+\frac{s}{s^{2}+b^{2}}\). It is also easy to use MATLAB to determine \(X_{a}(s)\) in standard rational form.

>> X_a = collect(X_a)  X_a = (a^2*s + a*b^2 + a*s^2 + s^3)/(s^4 + (a^2 + b^2)*s^2 + a^2*b^2) Thus, we also see that \(X_{a}(s)=\frac{s^{3}+as^{2}+a^{2}s+ab^{2}}{s^{4}+(a^{2}+b^{2})^{2}+a^{2}b^{2}}\)

**(b)**: A similar approach is taken for the inverse Laplace transform, except that the laplace command is used rather than the laplace command.

>>symsabs;X_b=(a*s^2)/(s^2+b^2); >>x_b=iplace(X_b) x_b=a*dirac(t)-a*b*sin(b*t) Therefore, \(x_{b}(t)=a\delta(t)-ab\sin(bt)u(t)\).

### 4.2 Laplace Transform

Show that the Laplace transform of \(10e^{-3t}\cos{(4t+53.13^{\circ})}\) is \((6s-14)/(s^{2}+6s+25)\). Use Table 4.1.

### 4.3 Inverse Laplace Transform

Find the inverse Laplace transform of the following:

**(a)**: \(\frac{s+17}{s^{2}+4s-5}\)
**(b)**: \(\frac{3s-5}{(s+1)(s^{2}+2s+5)}\)
**(c)**: \(\frac{16s+43}{(s-2)(s+3)^{2}}\)

### 4.4 Answers

**(a)**: \((3e^{t}-2e^{-5t})u(t)\)
**(b)**: \(\left[-2e^{-t}+\frac{5}{2}e^{-t}\cos{(2t-36.87^{\circ})}\right]u(t)\)
**(c)**: \([3e^{2t}+(t-3)e^{-3t}]u(t)\)

## 5 A Historical Note: Marquis Pierre-Simon de Laplace

(1749-1827)

The Laplace transform is named after the great French mathematician and astronomer Laplace, who first presented the transform and its applications to differential equations in a paper published in 1779.

Laplace developed the foundations of potential theory and made important contributions to special functions, probability theory, astronomy, and celestial mechanics. In his _Exposition du systeme du monde_ (1796), Laplace formulated a nebular hypothesis of cosmic origin and tried to explain the universe as a pure mechanism. In his _Traite de mecanique celeste_ (_celestial mechanics_), which completed the work of Newton, Laplace used mathematics and physics to subject the solar system and all heavenly bodies to the laws of motion and the principle of gravitation. Newton had been unable to explain the irregularities of some heavenly bodies; in desperation, he concluded that God himself must intervene now and then to prevent such catastrophes as Jupiter eventually falling into the sun (and the moon into the earth), as predicted by Newton's calculations. Laplace proposed to show that these irregularities would correct themselves periodically and that a little patience--in Jupiter's case, 929 years--would see everything returning automatically to order; thus there was no reason why the solar and the stellar systems could not continue to operate by the laws of Newton and Laplace to the end of time [4].

Laplace presented a copy of _Mecanique celeste_ to Napoleon, who, after reading the book, took Laplace to task for not including God in his scheme: "You have written this huge book on the system of the world without once mentioning the author of the universe." "Sire," Laplace retorted, "I had no need of that hypothesis." Napoleon was not amused, and when he reported this reply to another great mathematician-astronomer, Louis de Lagrange, the latter remarked, "Ah, but that is a fine hypothesis. It explains so many things" [5].

Napoleon, following his policy of honoring and promoting scientists, made Laplace the minister of the interior. To Napoleon's dismay, however, the new appointee attempted to bring "the spirit of infinitesimals" into administration, and so Laplace was transferred hastily to the Senate.

Oliver Heaviside (1850-1925)

Although Laplace published his transform method to solve differential equations in 1779, the method did not catch on until a century later. It was rediscovered independently in a rather awkward form by an eccentric British engineer, Oliver Heaviside (1850-1925), one of the tragic figures in the history of science and engineering. Despite his prolific contributions to electrical engineering, he was severely criticized during his lifetime and was neglected later to the point that hardly a textbook today mentions his name or credits him with contributions. Nevertheless, his studies had a major impact on many aspects of modern electrical engineering. It was Heaviside who made transatlantic communication possible by inventing cable loading, but few mention him as a pioneer or an innovator in telephony. It was Heaviside who suggested the use of inductive cable loading, but the credit is given to M. Pupin, who was not even responsible for building the first loading coil.2 In addition, Heaviside was [6]:

Footnote 2: Heaviside developed the theory for cable loading, George Campbell built the first loading coil, and the telephone circuits using Campbell’s coils were in operation before Pupin published his paper. In the legal fight over the patent, however, Pupin won the battle: he was a shrewd self-promoter, and Campbell had poor legal support.

* The first to find a solution to the distortionless transmission line.
* The innovator of lowpass filters.
* The first to write Maxwell's equations in modern form.
* The codiscoverer of rate energy transfer by an electromagnetic field.
* An early champion of the now-common phasor analysis.
* An important contributor to the development of vector analysis. In fact, he essentially created the subject independently of Gibbs [7].
* An originator of the use of operational mathematics used to solve linear integro-differential equations, which eventually led to rediscovery of the ignored Laplace transform.
* The first to theorize (along with Kennelly of Harvard) that a conducting layer (the Kennelly-Heaviside layer) of atmosphere exists, which allows radio waves to follow earth's curvature instead of traveling off into space in a straight line.
* The first to posit that an electrical charge would increase in mass as its velocity increases, an anticipation of an aspect of Einstein's special theory of relativity [8]. He also forecast the possibility of superconductivity.

Heaviside was a self-made, self-educated man. Although his formal education ended with elementary school, he eventually became a pragmatically successful mathematical physicist. He began his career as a telegrapher, but increasing deafness forced him to retire at the age of 24. He then devoted himself to the study of electricity. His creative work was disdained by many professional mathematicians because of his lack of formal education and his unorthodox methods.

Heaviside had the misfortune to be criticized both by mathematicians, who faulted him for lack of rigor, and by men of practice, who faulted him for using too much mathematics and thereby confusing students. Many mathematicians, trying to find solutions to the distortionless transmission line, failed because no rigorous tools were available at the time. Heaviside succeeded because he used mathematics not with rigor, but with insight and intuition. Using his much maligned operational method, Heaviside successfully attacked problems that the rigid mathematicians could not solve, problems such as the flow-of-heat in a body of spatially varying conductivity. Heaviside brilliantly used this method in 1895 to demonstrate a fatal flaw in Lord Kelvin's determination of the geological age of the earth by secular cooling; he used the same flow-of-heat theory as for his cable analysis. Yet the mathematicians of the Royal Society remained unmoved and were not the least impressed by the fact that Heaviside had found the answer to problems no one else could solve. Many mathematicians who examined his work dismissed it with contempt, asserting that his methods were either complete nonsense or a rehash of known ideas [6].

Sir William Preece, the chief engineer of the British Post Office, a savage critic of Heaviside, ridiculed Heaviside's work as too theoretical and, therefore, leading to faulty conclusions. Heaviside's work on transmission lines and loading was dismissed by the British Post Office and might have remained hidden, had not Lord Kelvin himself publicly expressed admiration for it [6].

Heaviside's operational calculus may be formally inaccurate, but in fact it anticipated the operational methods developed in more recent years [9]. Although his method was not fully understood, it provided correct results. When Heaviside was attacked for the vague meaning of his operational calculus, his pragmatic reply was, "Shall I refuse my dinner because I do not fully understand the process of digestion?"

Heaviside lived as a bachelor hermit, often in near-squalid conditions, and died largely unnoticed, in poverty. His life demonstrates the persistent arrogance and snobbishness of the intellectual establishment, which does not respect creativity unless it is presented in the strict language of the establishment.

## 4.2 Some Properties of the Laplace Transform

Properties of the Laplace transform are useful not only in the derivation of the Laplace transform of functions but also in the solutions of linear integro-differential equations. A glance at Eqs. (4.2) and (4.1) shows that there is a certain measure of symmetry in going from \(x(t)\) to \(X(s)\), and vice versa. This symmetry or duality is also carried over to the properties of the Laplace transform. This fact will be evident in the following development.

We are already familiar with two properties: linearity [Eq. (4.3)] and the uniqueness property of the Laplace transform discussed earlier.

### Time Shifting

The time-shifting property states that if

\[x(t)\Longleftrightarrow X(s)\]

then for \(t_{0}\geq 0\)

\[x(t-t_{0})\Longleftrightarrow X(s)e^{-xt_{0}} \tag{4.12}\]

Observe that \(x(t)\) starts at \(t=0\), and, therefore, \(x(t-t_{0})\) starts at \(t=t_{0}\). This fact is implicit, but is not explicitly indicated in Eq. (4.12). This often leads to inadvertent errors. To avoid such a pitfall, we should restate the property as follows. If

\[x(t)u(t)\Longleftrightarrow X(s)\]

then

\[x(t-t_{0})u(t-t_{0})\Longleftrightarrow X(s)e^{-xt_{0}}\qquad t_{0}\geq 0\]

**Proof.**

\[\mathcal{L}\left[x(t-t_{0})u(t-t_{0})\right]=\int_{0}^{\infty}x(t-t_{0})u(t-t_{0} )e^{-st}dt\]

Setting \(t-t_{0}=\tau\), we obtain

\[\mathcal{L}\left[x(t-t_{0})u(t-t_{0})\right]=\int_{-t_{0}}^{\infty}x(\tau)u( \tau)e^{-s(\tau+t_{0})}\,d\tau\]

Because \(u(\tau)=0\) for \(\tau<0\) and \(u(\tau)=1\) for \(\tau\geq 0\), the limits of integration can be taken from \(0\) to \(\infty\). Thus,

\[\mathcal{L}\left[x(t-t_{0})u(t-t_{0})\right] =\int_{0}^{\infty}x(\tau)e^{-s(\tau+t_{0})}\,d\tau\] \[=e^{-st_{0}}\int_{0}^{\infty}x(\tau)e^{-st}\,d\tau\] \[=X(s)e^{-st_{0}}\]

Note that \(x(t-t_{0})u(t-t_{0})\) is the signal \(x(t)u(t)\) delayed by \(t_{0}\) seconds. The time-shifting property states that _delaying a signal by \(t_{0}\) seconds amounts to multiplying its transform \(e^{-st_{0}}\)_.

This property of the unilateral Laplace transform holds only for positive \(t_{0}\) because if \(t_{0}\) were negative, the signal \(x(t-t_{0})u(t-t_{0})\) may not be causal.

We can readily verify this property in Drill 4.1. If the signal in Fig. 4.2a is \(x(t)u(t)\), then the signal in Fig. 4.2b is \(x(t-2)u(t-2)\). The Laplace transform for the pulse in Fig. 4.2a is \((1/s)(1-e^{-2s})\). Therefore, the Laplace transform for the pulse in Fig. 4.2b is \((1/s)(1-e^{-2s})e^{-2s}\).

The time-shifting property proves very convenient in finding the Laplace transform of functions with different descriptions over different intervals, as the following example demonstrates.

**Example 4.6**: **Laplace Transform and the Time-Shifting Property**

Find the Laplace transform of \(x(t)\) depicted in Fig. 4.4a.

Describing mathematically a function such as the one in Fig. 4.4a is discussed in Sec. 1.4. The function \(x(t)\) in Fig. 4.4a can be described as a sum of two components shown in Fig. 4.4b. The equation for the first component is \(t-1\) over \(1\leq t\leq 2\) so that this component can be described by \((t-1)[u(t-1)-u(t-2)]\). The second component can be described by \(u(t-2)-u(t-4)\). Therefore,

\[x(t) =(t-1)[u(t-1)-u(t-2)]+[u(t-2)-u(t-4)]\] \[=(t

### Some Properties of the Laplace Transform

The first term on the right-hand side is the signal \(tu(t)\) delayed by 1 second. Also, the third and fourth terms are the signal \(u(t)\) delayed by 2 and 4 seconds, respectively. The second term, however, cannot be interpreted as a delayed version of any entry in Table 4.1. For this reason, we rearrange it as

\[(t-1)u(t-2)=(t-2+1)u(t-2)=(t-2)u(t-2)+u(t-2)\]

We have now expressed the second term in the desired form as \(tu(t)\) delayed by 2 seconds plus \(u(t)\) delayed by 2 seconds. With this result, Eq. (4.13) can be expressed as

\[x(t)=(t-1)u(t-1)-(t-2)u(t-2)-u(t-4)\]

Application of the time-shifting property to \(tu(t)\Longleftrightarrow 1/s^{2}\) yields

\[(t-1)u(t-1)\Longleftrightarrow\frac{1}{s^{2}}e^{-s}\qquad\text{and}\qquad(t- 2)u(t-2)\Longleftrightarrow\frac{1}{s^{2}}e^{-2s}\]

Also

\[u(t)\Longleftrightarrow\frac{1}{s}\qquad\text{and}\qquad u(t-4)\Longleftrightarrow \frac{1}{s}e^{-4s}\]

Therefore,

\[X(s)=\frac{1}{s^{2}}e^{-s}-\frac{1}{s^{2}}e^{-2s}-\frac{1}{s}e^{-4s}\]

**Example 4.7**: **Inverse Laplace Transform and the Time-Shifting Property**

Find the inverse Laplace transform of

\[X(s)=\frac{s+3+5e^{-2s}}{(s+1)(s+2)}\]

Figure 4.4: Finding a piecewise representation of a signal \(x(t)\).

Observe the exponential term \(e^{-2s}\) in the numerator of \(X(s)\), indicating time delay. In such a case, we should separate \(X(s)\) into terms with and without a delay factor, as

\[X(s)=\underbrace{\frac{s+3}{(s+1)(s+2)}}_{X_{1}(s)}+\underbrace{\frac{5e^{-2s}} {(s+1)(s+2)}}_{X_{2}(s)e^{-2s}}\]

where

\[X_{1}(s)=\frac{s+3}{(s+1)(s+2)}=\frac{2}{s+1}-\frac{1}{s+2}\] \[X_{2}(s)=\frac{5}{(s+1)(s+2)}=\frac{5}{s+1}-\frac{5}{s+2}\]

Therefore,

\[x_{1}(t)=(2e^{-t}-e^{-2t})u(t)\] \[x_{2}(t)=5(e^{-t}-e^{-2t})u(t)\]

Also, because

\[X(s)=X_{1}(s)+X_{2}(s)e^{-2s}\]

we can write

\[x(t) =x_{1}(t)+x_{2}(t-2)\] \[=(2e^{-t}-e^{-2t})u(t)+5\left[e^{-(t-2)}-e^{-2(t-2)}\right]u(t-2)\]

### 4.4 Laplace Transform and the Time-Shifting Property

Find the Laplace transform of the signal illustrated in Fig. 4.5.

**Answer**

\(\frac{1}{s^{2}}\left(1-3e^{-2s}+2e^{-3s}\right)\)

### 4.2 Some Properties of the Laplace Transform

**Drill 4.5**: **Inverse Laplace Transform and the Time-Shifting Property**

Find the inverse Laplace transform of \(X(s)=\frac{3e^{-2s}}{(s-1)(s+2)}\).

**Answer**

\(\left[e^{t-2}-e^{-2(t-2)}\right]u(t-2)\)

### 4.2-2 Frequency Shifting

The frequency-shifting property states that if

\[x(t)\Longleftrightarrow X(s)\]

then

\[x(t)e^{s_{0}t}\Longleftrightarrow X(s-s_{0}) \tag{4.14}\]

Observe the symmetry (or duality) between this property and the time-shifting property of Eq. (4.12).

**Proof.**

\[{\cal L}[x(t)e^{s_{0}t}]=\int_{0^{-}}^{\infty}x(t)e^{s_{0}t}e^{-st}\,dt=\int_{ 0^{-}}^{\infty}x(t)e^{-(s-s_{0})t}\,dt=X(s-s_{0})\]

**EXAMPLE 4.8**: **Frequency-Shifting Property**

Derive pair 9a in Table 4.1 from pair 8a and the frequency-shifting property.

Pair 8a is

\[\cos bt\,u(t)\Longleftrightarrow\frac{s}{s^{2}+b^{2}}\]

From the frequency-shifting property [Eq. (4.14)] with \(s_{0}=-a\), we obtain

\[e^{-at}\cos bt\,u(t)\Longleftrightarrow\frac{s+a}{(s+a)^{2}+b^{2}}\]

**Drill 4.6**: **Frequency-Shifting Property**

Derive pair 6 in Table 4.1 from pair 3 and the frequency-shifting property.

[MISSING_PAGE_EMPTY:25]

### 4.2 Some Properties of the Laplace Transform

**Example 4.9**: **Laplace Transform and the Time-Differentiation Property**

Find the Laplace transform of the signal \(x(t)\) in Fig. 4.6a by using Table 4.1 and the time-differentiation and time-shifting properties of the Laplace transform.

Figures 4.6b and 4.6c show the first two derivatives of \(x(t)\). Recall that the derivative at a point of jump discontinuity is an impulse of strength equal to the amount of jump [see Eq. (1.12)]. Therefore,

\[\frac{d^{2}x(t)}{dt^{2}}=\delta(t)-3\delta(t-2)+2\delta(t-3)\]The Laplace transform of this equation yields

\[\mathcal{L}\left(\frac{d^{2}x(t)}{dt^{2}}\right)=\mathcal{L}\left[\delta(t)-3 \delta(t-2)+2\delta(t-3)\right]\]

Using the time-differentiation property of Eq. (4.15), the time-shifting property of Eq. (4.12), and the facts that \(x(0^{-})=\dot{x}(0^{-})=0\), and \(\delta(t)\Longleftrightarrow 1\), we obtain

\[s^{2}X(s)-0-0=1-3e^{-2s}+2e^{-3s}\]

Therefore,

\[X(s)=\frac{1}{s^{2}}(1-3e^{-2s}+2e^{-3s})\]

which confirms the earlier result in Drill 4.4.

### 4.2-4 The Time-Integration Property

The time-integration property states that if+

Footnote †: margin: \(\frac{x(t)}{t}\Longleftrightarrow\int_{s}^{\infty}X(z)\,dz\)

\[x(t)\Longleftrightarrow X(s)\]

then

\[\int_{0^{-}}^{t}x(\tau)\,d\tau\Longleftrightarrow\frac{X(s)}{s}\qquad\text{ and}\qquad\int_{-\infty}^{t}x(\tau)\,d\tau\Longleftrightarrow\frac{X(s)}{s}+\frac{ \int_{-\infty}^{0^{-}}x(\tau)\,d\tau}{s} \tag{4.16}\]

**Proof.** To prove the first part of Eq. (4.16), we define

\[g(t)=\int_{0^{-}}^{t}x(\tau)\,d\tau\]

so that

\[\frac{d}{dt}g(t)=x(t)\qquad\text{and}\qquad g(0^{-})=0\]

Now, if

\[g(t)\Longleftrightarrow G(s)\]

then

\[X(s)=\mathcal{L}\left[\frac{d}{dt}g(t)\right]=sG(s)-g(0^{-})=sG(s)\]

### Some Properties of the Laplace Transform

Therefore,

\[G(s)=\frac{X(s)}{s}\]

or

\[\int_{0^{-}}^{t}x(\tau)\,d\tau\Longleftrightarrow\frac{X(s)}{s}\]

To prove the second part of Eq. (4.16), observe that

\[\int_{-\infty}^{t}x(\tau)\,d\tau=\int_{-\infty}^{0^{-}}x(\tau)\,d\tau+\int_{0^ {-}}^{t}x(\tau)\,d\tau\]

Note that the first term on the right-hand side is a constant for \(t\geq 0\). Taking the Laplace transform of the foregoing equation and using the first part of Eq. (4.16), we obtain

\[\int_{-\infty}^{t}x(\tau)\,d\tau\Longleftrightarrow\frac{\int_{-\infty}^{0^ {-}}x(\tau)\,d\tau}{s}+\frac{X(s)}{s}\]

### The Scaling Property

The scaling property states that if

\[x(t)\Longleftrightarrow X(s)\]

then for \(a>0\)

\[x(at)\Longleftrightarrow\frac{1}{a}X\biggl{(}\frac{s}{a}\biggr{)}\]

The proof is given in Ch. 7. Note that \(a\) is restricted to positive values because if \(x(t)\) is causal, then \(x(at)\) is anticausal (is zero for \(t\geq 0\)) for negative \(a\), and anticausal signals are not permitted in the (unilateral) Laplace transform.

Recall that \(x(at)\) is the signal \(x(t)\) time-compressed by the factor \(a\), and \(X(\frac{s}{a})\) is \(X(s)\) expanded along the \(s\) scale by the same factor \(a\) (see Sec. 1.2-2). The scaling property states that _time compression of a signal by a factor \(a\) causes expansion of its Laplace transform in the \(s\) scale by the same factor. Similarly, time expansion \(x(t)\) causes compression of \(X(s)\) in the \(s\) scale by the same factor._

### Time Convolution and Frequency Convolution

Another pair of properties states that if

\[x_{1}(t)\Longleftrightarrow X_{1}(s)\qquad\text{and}\qquad x_{2}(t) \Longleftrightarrow X_{2}(s)\]

then (_time-convolution property_)

\[x_{1}(t)*x_{2}(t)\Longleftrightarrow X_{1}(s)X_{2}(s) \tag{4.17}\]and (_frequency-convolution property_)

\[x_{1}(t)x_{2}(t)\Longleftrightarrow\frac{1}{2\pi j}[X_{1}(s)*X_{2}(s)]\]

Observe the symmetry (or duality) between the two properties. Proofs of these properties are postponed to Ch. 7.

Equation (39) indicates that \(H(s)\), the transfer function of an LTIC system, is the Laplace transform of the system's impulse response \(h(t)\); that is,

\[h(t)\Longleftrightarrow H(s)\]

If the system is causal, \(h(t)\) is causal, and, according to Eq. (39), \(H(s)\) is the unilateral Laplace transform of \(h(t)\). Similarly, if the system is noncausal, \(h(t)\) is noncausal, and \(H(s)\) is the bilateral transform of \(h(t)\).

We can apply the time-convolution property to the LTIC input-output relationship \(y(t)=x(t)*h(t)\) to obtain

\[Y(s)=X(s)H(s) \tag{48}\]

The response \(y(t)\) is the zero-state response of the LTIC system to the input \(x(t)\). From Eq. (48), it follows that

\[H(s)=\frac{Y(s)}{X(s)}=\frac{\mathcal{L}[\text{zero-state response}]}{\mathcal{L}[\text{input}]} \tag{49}\]

This may be considered an alternate definition of the LTIC system transfer function \(H(s)\). It is the _ratio of the transform of zero-state response to the transform of the input._

**Example 4.10**: **Time-Convolution Property**

Use the time-convolution property of the Laplace transform to determine \(c(t)=e^{at}u(t)*e^{bt}u(t)\).

From Eq. (4.17), it follows that

\[C(s)=\frac{1}{(s-a)(s-b)}=\frac{1}{a-b}\left[\frac{1}{s-a}-\frac{1}{s-b}\right]\]

The inverse transform of this equation yields

\[c(t)=\frac{1}{a-b}(e^{at}-e^{bt})u(t)\]

### Initial and Final Values

In certain applications, it is desirable to know the values of \(x(t)\) as \(t\to 0\) and \(t\to\infty\) [initial and final values of \(x(t)\)] from the knowledge of its Laplace transform \(X(s)\). Initial and final value theorems provide such information.

_The initial value theorem_ states that if \(x(t)\) and its derivative \(dx/dt\) are both Laplace transformable, then

\[x(0^{+})=\lim_{s\to\infty}sX(s) \tag{4.20}\]

provided the limit on the right-hand side of Eq. (4.20) exists.

_The final value theorem_ states that if both \(x(t)\) and \(dx/dt\) are Laplace transformable, then

\[\lim_{t\to\infty}x(t)=\lim_{s\to 0}sX(s) \tag{4.21}\]

provided \(sX(s)\) has no poles in the RHP or on the imaginary axis. To prove these theorems, we begin by setting \(n=1\) in Eq. (4.15). Using the definition of the Laplace transform, we see that

\[sX(s)-x(0^{-}) =\int_{0^{-}}^{\infty}\frac{dx(t)}{dt}e^{-st}\,dt\] \[=\int_{0^{-}}^{0^{+}}\frac{dx(t)}{dt}e^{-st}\,dt+\int_{0^{+}}^{ \infty}\frac{dx(t)}{dt}e^{-st}\,dt\] \[=x(t)\big{|}_{0^{-}}^{0^{+}}+\int_{0^{+}}^{\infty}\frac{dx(t)}{dt }e^{-st}\,dt\] \[=x(0^{+})-x(0^{-})+\int_{0^{+}}^{\infty}\frac{dx(t)}{dt}e^{-st}\,dt\]

Therefore,

\[sX(s)=x(0^{+})+\int_{0^{+}}^{\infty}\frac{dx(t)}{dt}e^{-st}\,dt\]

and

\[\lim_{s\to\infty}sX(s) =x(0^{+})+\lim_{s\to\infty}\int_{0^{+}}^{\infty}\frac{dx(t)}{dt}e ^{-st}\,dt\] \[=x(0^{+})+\int_{0^{+}}^{\infty}\frac{dx(t)}{dt}\,\Big{(}\lim_{s \to\infty}e^{-st}\,\Big{)}\,dt\] \[=x(0^{+})\]

**Comment.** The initial value theorem applies only if \(X(s)\) is strictly proper (\(M<N\)), because for \(M\geq N\), \(\lim_{s\to\infty}sX(s)\) does not exist, and the theorem does not apply. In such a case, we can still find the answer by using long division to express \(X(s)\) as a polynomial in \(s\) plus a strictly proper fraction, where \(M<N\). For example, by using long division, we can express

\[\frac{s^{3}+3s^{2}+s+1}{s^{2}+2s+1}=(s+1)-\frac{2s}{s^{2}+2s+1}\]

The inverse transform of the polynomial in \(s\) is in terms of \(\delta(t)\), and its derivatives, which are zero at \(t=0^{+}\). In the foregoing case, the inverse transform of \(s+1\) is \(\dot{\delta}(t)+\delta(t)\). Hence, the desired\(x(0^{+})\) is the value of the remainder (strictly proper) fraction, for which the initial value theorem applies. In the present case,

\[x(0^{+})=\lim_{s\to\infty}\frac{-2s^{2}}{s^{2}+2s+1}=-2\]

To prove the final value theorem, we let \(n=1\) and \(s\to 0\) in Eq. (4.15) to obtain

\[\lim_{s\to 0}[sX(s)-x(0^{-})] =\lim_{s\to 0}\int_{0^{-}}^{\infty}\frac{dx(t)}{dt}e^{-st}\,dt= \int_{0^{-}}^{\infty}\frac{dx(t)}{dt}\,dt\] \[=x(t)\big{|}_{0^{-}}^{\infty}=\lim_{t\to\infty}x(t)-x(0^{-})\]

a deduction that leads to the desired result, Eq. (4.21).

**Comment.** The final value theorem applies only if the poles of \(X(s)\) are in the LHP (including \(s=0\)). If \(X(s)\) has a pole in the RHP, \(x(t)\) contains an exponentially growing term and \(x(\infty)\) does not exist. If there is a pole on the imaginary axis, then \(x(t)\) contains an oscillating term and \(x(\infty)\) does not exist. However, if there is a pole at the origin, then \(x(t)\) contains a constant term, and hence, \(x(\infty)\) exists and is a constant.

**EXAMPLE 4.11**: **Initial and Final Values**

Determine the initial and final values of \(y(t)\) if its Laplace transform \(Y(s)\) is given by

\[Y(s)=\frac{10(2s+3)}{s(s^{2}+2s+5)}\]

Equations (4.20) and (4.21) yield

\[y(0^{+})=\lim_{s\to\infty}sY(s)=\lim_{s\to\infty}\frac{10(2s+3)}{(s^{2}+2s+5) }=0\]

\[y(\infty)=\lim_{s\to 0}sY(s)=\lim_{s\to 0}\frac{10(2s+3)}{(s^{2}+2s+5)}=6\]

Table 4.2 summarizes the most important unilateral Laplace transform properties.

## 4.3 Solution of Differential and Integro-Differential Equations

The time-differentiation property of the Laplace transform has set the stage for solving linear differential (or integro-differential) equations with constant coefficients. Because \(d^{k}y/dt^{k}\Longleftrightarrow s^{k}Y(s)\), the Laplace transform of a differential equation is an algebraic equation that can be readily solved for \(Y(s)\). Next we take the inverse Laplace transform of \(Y(s)\) to find the desired solution \(y(t)\). The following examples demonstrate the Laplace transform procedure for solving linear differential equations with constant coefficients.

**Example 4.12**: **Laplace Transform to Solve a Second-Order Linear Differential Equation**

Solve the second-order linear differential equation

\[(D^{2}+5D+6)y(t)=(D+1)x(t)\]

for the initial conditions \(y(0^{-})=2\) and \(\dot{y}(0^{-})=1\) and the input \(x(t)=e^{-4t}u(t)\).

\begin{table}
\begin{tabular}{l l l} \hline \hline
**Operation** & \(\mathbf{x(t)}\) & \(\mathbf{X(s)}\) \\ \hline Addition & \(x_{1}(t)+x_{2}(t)\) & \(X_{1}(s)+X_{2}(s)\) \\ Scalar multiplication & \(kx(t)\) & \(kX(s)\) \\ Time differentiation & \(\dfrac{dx(t)}{dt}\) & \(sX(s)-x(0^{-})\) \\  & \(\dfrac{d^{2}x(t)}{dt^{2}}\) & \(s^{2}X(s)-sx(0^{-})-\dot{x}(0^{-})\) \\  & \(\dfrac{d^{3}x(t)}{dt^{3}}\) & \(s^{3}X(s)-s^{2}x(0^{-})-\dot{x}(0^{-})-\ddot{x}(0^{-})\) \\  & \(\dfrac{d^{n}x(t)}{dt^{n}}\) & \(s^{n}X(s)-\sum\limits_{k=1}^{n}s^{n-k}x^{(k-1)}(0^{-})\) \\ Time integration & \(\int_{0^{-}}^{t}x(\tau)\,d\tau\) & \(\dfrac{1}{s}X(s)\) \\  & \(\int_{-\infty}^{t}x(\tau)\,d\tau\) & \(\dfrac{1}{s}X(s)+\dfrac{1}{s}\int_{-\infty}^{0^{-}}x(t)\,dt\) \\ Time shifting & \(x(t-t_{0})u(t-t_{0})\) & \(X(s)e^{-st_{0}}\) \(t_{0}\geq 0\) \\ Frequency shifting & \(x(t)e^{s_{0}t}\) & \(X(s-s_{0})\) \\ Frequency differentiation & \(-tx(t)\) & \(\dfrac{dX(s)}{ds}\) \\ Frequency integration & \(\dfrac{x(t)}{t}\) & \(\int_{s}^{\infty}X(z)\,dz\) \\ Scaling & \(x(at),a\geq 0\) & \(\dfrac{1}{a}X\bigg{(}\dfrac{s}{a}\bigg{)}\) \\ Time convolution & \(x_{1}(t)*x_{2}(t)\) & \(X_{1}(s)X_{2}(s)\) \\ Frequency convolution & \(x_{1}(t)x_{2}(t)\) & \(\dfrac{1}{2\pi j}X_{1}(s)*X_{2}(s)\) \\ Initial value & \(x(0^{+})\) & \(\underset{s\rightarrow\infty}{\lim}\,sX(s)\) & \((n>m)\) \\ Final value & \(x(\infty)\) & \(\underset{s\rightarrow\infty}{\lim}\,sX(s)\) & [poles of \(sX(s)\) in LHP] \\ \hline \hline \end{tabular}
\end{table}
Table 4.2: Unilateral Laplace Transform PropertiesThe equation is \[\frac{d^{2}y(t)}{dt^{2}}+5\frac{dy(t)}{dt}+6y(t)=\frac{dx(t)}{dt}+x(t)\] (4.22) Let \[y(t)\Longleftrightarrow Y(s)\] Then from Eq. (4.15), \[\frac{dy(t)}{dt}\Longleftrightarrow sY(s)-y(0^{-})=sY(s)-2\] and \[\frac{d^{2}y(t)}{dt^{2}}\Longleftrightarrow s^{2}Y(s)-sy(0^{-})-\dot{y}(0^{- })=s^{2}Y(s)-2s-1\] Moreover, for \(x(t)=e^{-4t}u(t)\), \[X(s)=\frac{1}{s+4}\quad\text{and}\quad\frac{dx(t)}{dt}\Longleftrightarrow sX (s)-x(0^{-})=\frac{s}{s+4}-0=\frac{s}{s+4}\] Taking the Laplace transform of Eq. (4.22), we obtain \[[s^{2}Y(s)-2s-1]+5[sY(s)-2]+6Y(s)=\frac{s}{s+4}+\frac{1}{s+4}\] Collecting all the terms of \(Y(s)\) and the remaining terms separately on the left-hand side, we obtain \[(s^{2}+5s+6)Y(s)-(2s+11)=\frac{s+1}{s+4}\] (4.23) Therefore, \[(s^{2}+5s+6)Y(s)=(2s+11)+\frac{s+1}{s+4}=\frac{2s^{2}+20s+45}{s+4}\] and \[Y(s)=\frac{2s^{2}+20s+45}{(s^{2}+5s+6)(s+4)}=\frac{2s^{2}+20s+45}{(s+2)(s+3)(s +4)}\] Expanding the right-hand side into partial fractions yields \[Y(s)=\frac{13/2}{s+2}-\frac{3}{s+3}-\frac{3/2}{s+4}\] The inverse Laplace transform of this equation yields \[y(t)=\left(\tfrac{13}{2}e^{-2t}-3e^{-3t}-\tfrac{3}{2}e^{-4t}\right)u(t)\] (4.24)

Example 4.12 demonstrates the ease with which the Laplace transform can solve linear differential equations with constant coefficients. The method is general and can solve a linear differential equation with constant coefficients of any order.

## Zero-Input and Zero-State Components of Response

The Laplace transform method gives the total response, which includes zero-input and zero-state components. It is possible to separate the two components if we so desire. The initial condition terms in the response give rise to the zero-input response. For instance, in Ex. 4.12, the terms attributable to initial conditions \(y(0^{-})=2\) and \(\dot{y}(0^{-})=1\) in Eq. (4.23) generate the zero-input response. These initial condition terms are \(-(2s+11)\), as seen in Eq. (4.23). The terms on the right-hand side are exclusively due to the input. Equation (4.23) is reproduced below with the proper labeling of the terms

\[\left(s^{2}+5s+6\right)Y(s)-(2s+11)=\frac{s+1}{s+4}\]

so that

\[\left(s^{2}+5s+6\right)Y(s)=\underbrace{\left(2s+11\right)}_{\text{initial condition terms}}+\underbrace{\frac{s+1}{s+4}}_{\text{input terms}}\]

Therefore,

\[Y(s) =\underbrace{\frac{2s+11}{s^{2}+5s+6}}_{\text{2IR}}+\underbrace{ \frac{s+1}{(s+4)(s^{2}+5s+6)}}_{\text{2SR}}\] \[=\left[\frac{7}{s+2}-\frac{5}{s+3}\right]+\left[\frac{-1/2}{s+2} +\frac{2}{s+3}-\frac{3/2}{s+4}\right]\]

Taking the inverse transform of this equation yields

\[y(t)=\underbrace{\left(7e^{-2t}-5e^{-3t}\right)u(t)}_{\text{ZIR}}+\underbrace {\left(-\tfrac{1}{2}e^{-2t}+2e^{-3t}-\tfrac{3}{2}e^{-4t}\right)u(t)}_{\text{ ZSR}}\]

### Comments on Initial Conditions at \(0^{-}\) and at \(0^{+}\)

The initial conditions in Ex. 4.12 are \(y(0^{-})=2\) and \(\dot{y}(0^{-})=1\). If we let \(t=0\) in the total response in Eq. (4.24), we find \(y(0)=2\) and \(\dot{y}(0)=2\), which is at odds with the given initial conditions. Why? Because the initial conditions are given at \(t=0^{-}\) (just before the input is applied), when only the zero-input response is present. The zero-state response is the result of the input \(x(t)\) applied at \(t=0\). Hence, this component does not exist at \(t=0^{-}\). Consequently, the initial conditions at \(t=0^{-}\) are satisfied by the zero-input response, not by the total response. We can readily verify in this example that the zero-input response does indeed satisfy the given initial conditions at \(t=0^{-}\). It is the total response that satisfies the initial conditions at \(t=0^{+}\), which are generally different from the initial conditions at \(0^{-}\).

There also exists a \(\mathcal{L}_{+}\) version of the Laplace transform, which uses the initial conditions at \(t=0^{+}\) rather than at \(0^{-}\) (as in our present \(\mathcal{L}_{-}\) version). The \(\mathcal{L}_{+}\) version, which was in vogue till the early 1960s, is identical to the \(\mathcal{L}_{-}\) version except the limits of Laplace integral [Eq. (4.7)] are from \(0^{+}\) to \(\infty\). Hence, by definition, the origin \(t=0\) is excluded from the domain. This version, still used in some math books, has some serious difficulties. For instance, the Laplace transform of \(\delta(t)\) is zero because \(\delta(t)=0\) for \(t\geq 0^{+}\). Moreover, this approach is rather clumsy in the theoretical studyof linear systems because the response obtained cannot be separated into zero-input and zero-state components. As we know, the zero-state component represents the system response as an explicit function of the input, and without knowing this component, it is not possible to assess the effect of the input on the system response in a general way. The \(\mathcal{L}_{+}\) version can separate the response in terms of the natural and the forced components, which are not as interesting as the zero-input and the zero-state components. Note that we can always determine the natural and the forced components from the zero-input and the zero-state components [e.g., Eq. (2.44) from Eq. (2.43)], but the converse is not true. Because of these and some other problems, electrical engineers (wisely) started discarding the \(\mathcal{L}_{+}\) version in the early 1960s.

It is interesting to note the time-domain duals of these two Laplace versions. The classical method is the dual of the \(\mathcal{L}_{+}\) method, and the convolution (zero-input/zero-state) method is the dual of the \(\mathcal{L}_{-}\) method. The first pair uses the initial conditions at \(0^{+}\), and the second pair uses those at \(t=0^{-}\). The first pair (the classical method and the \(\mathcal{L}_{+}\) version) is awkward in the theoretical study of linear system analysis. It was no coincidence that the \(\mathcal{L}_{-}\) version was adopted immediately after the introduction to the electrical engineering community of state-space analysis (which uses zero-input/zero-state separation of the output).

### 4.7 Laplace Transform to Solve a Second-Order Linear Differential Equation

Solve

\[\frac{d^{2}y(t)}{dt^{2}}+4\frac{dy(t)}{dt}+3y(t)=2\frac{dx(t)}{dt}+x(t)\]

for the input \(x(t)=u(t)\). The initial conditions are \(y(0^{-})=1\) and \(\dot{y}(0^{-})=2\).

### 4.13 Laplace Transform to Solve an Electric Circuit

In the circuit of Fig. 4.7a, the switch is in the closed position for a long time before \(t=0\), when it is opened instantaneously. Find the inductor current \(y(t)\) for \(t\geq 0\).

When the switch is in the closed position (for a long time), the inductor current is 2 amperes and the capacitor voltage is 10 volts. When the switch is opened, the circuit is equivalent to that depicted in Fig. 4.7b, with the initial inductor current \(y(0^{-})=2\) and the initial capacitor voltage \(v_{C}(0^{-})=10\). The input voltage is 10 volts, starting at \(t=0\), and, therefore, can be represented by \(10u(t)\).

The loop equation of the circuit in Fig. 4.7b is

\[\frac{dy(t)}{dt}+2y(t)+5\int_{-\infty}^{t}y(\tau)\,d\tau=10u(t) \tag{4.25}\]

If

\[y(t)\Longleftrightarrow Y(s)\]

then

\[\frac{dy(t)}{dt}\Longleftrightarrow sY(s)-y(0^{-})=sY(s)-2\]

and [see Eq. (4.16)]

\[\int_{-\infty}^{t}y(\tau)\,d\tau\Longleftrightarrow\frac{Y(s)}{s}+\frac{ \int_{-\infty}^{0^{-}}y(\tau)\,d\tau}{s}\]

Because \(y(t)\) is the capacitor current, the integral \(\int_{-\infty}^{0^{-}}y(\tau)\,d\tau\) is \(q_{C}(0^{-})\), the capacitor charge at \(t=0^{-}\), which is given by \(C\) times the capacitor voltage at \(t=0^{-}\). Therefore,

\[\int_{-\infty}^{0^{-}}y(\tau)\,d\tau=q_{C}(0^{-})=Cv_{C}(0^{-})=\tfrac{1}{5}( 10)=2\]

and

\[\int_{-\infty}^{t}y(\tau)\,d\tau\Longleftrightarrow\frac{Y(s)}{s}+\frac{2}{s}\]

Figure 4.7: Analysis of a network with a switching action.

Using these results, the Laplace transform of Eq. (4.25) is

\[sY(s)-2+2Y(s)+\frac{5Y(s)}{s}+\frac{10}{s}=\frac{10}{s}\]

or

\[\left[s+2+\frac{5}{s}\right]Y(s)=2\]

and

\[Y(s)=\frac{2s}{s^{2}+2s+5}\]

To find the inverse Laplace transform of \(Y(s)\), we use pair 10c (Table 4.1) with values \(A=2\), \(B=0\), \(a=1\), and \(c=5\). This yields

\[r=\sqrt{\frac{20}{4}}=\sqrt{5},\quad b=\sqrt{c-a^{2}}=2\quad\text{and}\quad \theta=\tan^{-1}\left(\frac{2}{4}\right)=26.6^{\circ}\]

Therefore,

\[y(t)=\sqrt{5}e^{-t}\cos{(2t+26.6^{\circ})}u(t)\]

This response is shown in Fig. 4.7c.

**Comment.** In our discussion so far, we have multiplied input signals by \(u(t)\), implying that the signals are zero prior to \(t=0\). This is needlessly restrictive. These signals can have any arbitrary value prior to \(t=0\). As long as the initial conditions at \(t=0\) are specified, we need only the knowledge of the input for \(t\geq 0\) to compute the response for \(t\geq 0\). Some authors use the notation \(1(t)\) to denote a function that is equal to \(u(t)\) for \(t\geq 0\) and that has arbitrary value for negative \(t\). We have abstained from this usage to avoid needless confusion caused by the introduction of a new function, which is very similar to \(u(t)\).

### 4.3-2 Zero-State Response

Consider an \(N\)th-order LTIC system specified by the equation

\[Q(D)y(t)=P(D)x(t)\]

or

\[(D^{N}+a_{1}D^{N-1}+\cdot\cdot\cdot+a_{N-1}D+a_{N})y(t)=(b_{0}D^{N}+b_{1}D^{N- 1}+\cdot\cdot\cdot+b_{N-1}D+b_{N})x(t) \tag{4.26}\]

We shall now find the general expression for the zero-state response of an LTIC system. Zero-state response \(y(t)\), by definition, is the system response to an input when the system is initially relaxed (in zero state). Therefore, \(y(t)\) satisfies Eq. (4.26) with zero initial conditions

\[y(0^{-})=\dot{y}(0^{-})=\ddot{y}(0^{-})=\cdot\cdot\cdot=y^{(N-1)}(0^{-})=0\]Moreover, the input \(x(t)\) is causal so that

\[x(0^{-})=\dot{x}(0^{-})=\ddot{x}(0^{-})=\cdot\cdot\cdot=x^{(N-1)}(0^{-})=0\]

Let

\[y(t)\Longleftrightarrow Y(s)\qquad\text{and}\qquad x(t)\Longleftrightarrow X(s)\]

Because of zero initial conditions,

\[D^{r}y(t) =\frac{d^{r}}{dt^{r}}y(t)\Longleftrightarrow s^{r}Y(s)\] \[D^{k}x(t) =\frac{d^{k}}{dt^{k}}x(t)\Longleftrightarrow s^{k}X(s)\]

Therefore, the Laplace transform of Eq. (4.26) yields

\[(s^{N}+a_{1}s^{N-1}+\cdot\cdot\cdot+a_{N-1}s+a_{N})Y(s)=(b_{0}s^{N}+b_{1}s^{N- 1}+\cdot\cdot\cdot+b_{N-1}s+b_{N})X(s)\]

or

\[Y(s)=\frac{b_{0}s^{N}+b_{1}s^{N-1}+\cdot\cdot\cdot+b_{N-1}s+b_{N}}{s^{N}+a_{1} s^{N-1}+\cdot\cdot\cdot+a_{N-1}s+a_{N}}X(s)=\frac{P(s)}{Q(s)}X(s)\]

But we have shown in Eq. (4.18) that \(Y(s)=H(s)X(s)\). Consequently,

\[H(s)=\frac{P(s)}{Q(s)} \tag{4.27}\]

This is the transfer function of a linear differential system specified in Eq. (4.26). The same result has been derived earlier in Eq. (2.41) using an alternate (time-domain) approach.

We have shown that \(Y(s)\), the Laplace transform of the zero-state response \(y(t)\), is the product of \(X(s)\) and \(H(s)\), where \(X(s)\) is the Laplace transform of the input \(x(t)\) and \(H(s)\) is the system transfer function [relating the particular output \(y(t)\) to the input \(x(t)\)].

### Intuitive Interpretation of the Laplace Transform

So far we have treated the Laplace transform as a machine that converts linear integro-differential equations into algebraic equations. There is no physical understanding of how this is accomplished or what it means. We now discuss a more intuitive interpretation and meaning of the Laplace transform.

In Ch. 2, Eq. (2.38), we showed that LTI system response to an everlasting exponential \(e^{st}\) is \(H(s)e^{st}\). If we could express every signal as a linear combination of everlasting exponentials of the form \(e^{st}\), we could readily obtain the system response to any input. For example, if

\[x(t)=\sum_{k=1}^{K}X(s_{i})e^{s_{i}t}\]

the response of an LTIC system to such input \(x(t)\) is given by

\[y(t)=\sum_{k=1}^{K}X(s_{i})H(s_{i})e^{s_{i}t}\]Unfortunately, the class of signals that can be expressed in this form is very small. However, we can express almost all signals of practical utility as a sum of everlasting exponentials over a continuum of frequencies. This is precisely what the Laplace transform in Eq. (4.2) does.

\[x(t)=\frac{1}{2\pi j}\int_{c-j\infty}^{c+j\infty}X(s)e^{st}ds \tag{4.28}\]

Invoking the linearity property of the Laplace transform, we can find the system response \(y(t)\) to input \(x(t)\) in Eq. (4.28) as1

Footnote 1: Recall that \(H(s)\) has its own region of validity. Hence, the limits of integration for the integral in Eq. (4.28) are modified in Eq. (4.29) to accommodate the region of existence (validity) of \(X(s)\) as well as \(H(s)\).

\[y(t)=\frac{1}{2\pi j}\int_{c^{\prime}-j\infty}^{c^{\prime}+j\infty}X(s)H(s)e^{ st}\,ds=\mathcal{L}^{-1}X(s)H(s) \tag{4.29}\]

Clearly,

\[Y(s)=X(s)H(s)\]

We can now represent the transformed version of the system, as depicted in Fig. 4.8a. The input \(X(s)\) is the Laplace transform of \(x(t)\), and the output \(Y(s)\) is the Laplace transform of (the zero-input response) \(y(t)\). The system is described by the transfer function \(H(s)\). The output \(Y(s)\) is the product \(X(s)H(s)\).

Recall that \(s\) is the complex frequency of \(e^{st}\). This explains why the Laplace transform method is also called the _frequency-domain_ method. Note that \(X(s),Y(s)\), and \(H(s)\) are the frequency-domain representations of \(x(t),y(t)\), and \(h(t)\), respectively. We may view the boxes marked \(\mathcal{L}\) and \(\mathcal{L}^{-1}\) in Fig. 4.8a as the interfaces that convert the time-domain entities into the corresponding frequency-domain entities, and vice versa. All real-life signals begin in the time domain, and the final answers must also be in the time domain. First, we convert the time-domain input(s) into the frequency-domain counterparts. The problem itself is solved in the frequency domain, resulting in the answer \(Y(s)\), also in the frequency domain. Finally, we convert \(Y(s)\) to \(y(t)\). Solving the problem is relatively simpler in the frequency domain than in the time domain. Henceforth, we shall omit the explicit representation of the interface boxes \(\mathcal{L}\) and \(\mathcal{L}^{-1}\), representing signals and systems in the frequency domain, as shown in Fig. 4.8b.

Figure 4.8: Alternate interpretation of the Laplace transform.

[MISSING_PAGE_FAIL:40]

Therefore, \[H(s)=\frac{Y(s)}{X(s)}=e^{-sT}\] (4.30) **(b) Ideal Differentiator.** For an ideal differentiator, the input \(x(t)\) and the output \(y(t)\) are related by \[y(t)=\frac{dx(t)}{dt}\] The Laplace transform of this equation yields \[Y(s)=sX(s)\qquad[x(0^{-})=0\text{ for a causal signal}]\] and \[H(s)=\frac{Y(s)}{X(s)}=s\] (4.31) **(c) Ideal Integrator.** For an ideal integrator with zero initial state, that is, \(y(0^{-})=0\), \[y(t)=\int_{0}^{t}x(\tau)\,d\tau\quad\text{and}\quad Y(s)=\frac{1}{s}X(s)\] Therefore, \[H(s)=\frac{1}{s}\] (4.32)

### 4.8 Differential Equation and Zero-State Response from a System Transfer Function

For an LTIC system with transfer function,

\[H(s)=\frac{s+5}{s^{2}+4s+3}\]

1. Describe the differential equation relating the input \(x(t)\) and output \(y(t)\).
2. Find the system response \(y(t)\) to the input \(x(t)=e^{-2t}u(t)\) if the system is initially in zero state.

### 4.9 Answers

1. \(\frac{d^{2}y(t)}{dt^{2}}+4\frac{dy(t)}{dt}+3y(t)=\frac{dx(t)}{dt}+5x(t)\)
2. \(y(t)=(2e^{-t}-3e^{-2t}+e^{-3t})u(t)\)

### Stability

Equation (4.27) shows that the denominator of \(H(s)\) is \(Q(s)\), which is apparently identical to the characteristic polynomial \(Q(\lambda)\) defined in Ch. 2. Does this mean that the denominator of \(H(s)\) is the characteristic polynomial of the system? This may or may not be the case, since if \(P(s)\) and \(Q(s)\) in Eq. (4.27) have any common factors, they cancel out, and the effective denominator of \(H(s)\) is not necessarily equal to \(Q(s)\). Recall also that the system transfer function \(H(s)\), like \(h(t)\), is defined in terms of measurements at the external terminals. Consequently, \(H(s)\) and \(h(t)\) are both external descriptions of the system. In contrast, the characteristic polynomial \(Q(s)\) is an internal description. Clearly, we can determine only external stability, that is, BIBO stability, from \(H(s)\). If all the poles of \(H(s)\) are in LHP, all the terms in \(h(t)\) are decaying exponentials, and \(h(t)\) is absolutely integrable [see Eq. (2.45)].2 Consequently, the system is BIBO-stable. Otherwise the system is BIBO-unstable.

Footnote 2: Values of \(s\) for which \(H(s)\) is \(\infty\) are the _poles_ of \(H(s)\). Thus, poles of \(H(s)\) are the values of \(s\) for which the denominator of \(H(s)\) is zero.

So far, we have assumed that \(H(s)\) is a proper function, that is, \(M\leq N\). We now show that if \(H(s)\) is improper, that is, if \(M>N\), the system is BIBO-unstable. In such a case, using long division, we obtain \(H(s)=R(s)+H^{\prime}(s)\), where \(R(s)\) is an \((M-N)\)th-order polynomial and \(H^{\prime}(s)\) is a proper transfer function. For example,

\[H(s)=\frac{s^{3}+4s^{2}+4s+5}{s^{2}+3s+2}=s+\frac{s^{2}+2s+5}{s^{2}+3s+2}\]

As shown in Eq. (4.31), the term \(s\) is the transfer function of an ideal differentiator. If we apply step function (bounded input) to this system, the output will contain an impulse (unbounded output). Clearly, the system is BIBO-unstable. Moreover, such a system greatly amplifies noise because differentiation enhances higher frequencies, which generally predominate in a noise signal. Theseare two good reasons to avoid improper systems (\(M>N\)). In our future discussion, we shall implicitly assume that the systems are proper, unless stated otherwise.

If \(P(s)\) and \(Q(s)\) do not have common factors, then the denominator of \(H(s)\) is identical to \(Q(s)\), the characteristic polynomial of the system. In this case, we can determine internal stability by using the criterion described in Sec. 2.5. Thus, if \(P(s)\) and \(Q(s)\) have no common factors, the asymptotic stability criterion in Sec. 2.5 can be restated in terms of the poles of the transfer function of a system, as follows:

1. An LTIC system is asymptotically stable if and only if all the poles of its transfer function \(H(s)\) are in the LHP. The poles may be simple or repeated.
2. An LTIC system is unstable if and only if either one or both of the following conditions exist: (i) at least one pole of \(H(s)\) is in the RHP; (ii) there are repeated poles of \(H(s)\) on the imaginary axis.
3. An LTIC system is marginally stable if and only if there are no poles of \(H(s)\) in the RHP and some unrepeated poles on the imaginary axis.

The locations of zeros of \(H(s)\) have no role in determining the system stability.

**Example 4.16**: **BIBO and Asymptotic Stability**

Figure 4.9a shows a cascade connection of two LTIC systems \(\mathcal{S}_{1}\) followed by \(\mathcal{S}_{2}\). The transfer functions of these systems are \(H_{1}(s)=1/(s-1)\) and \(H_{2}(s)=(s-1)/(s+1)\), respectively. Determine the BIBO and asymptotic stability of the composite (cascade) system.

If the impulse responses of \(\mathcal{S}_{1}\) and \(\mathcal{S}_{2}\) are \(h_{1}(t)\) and \(h_{2}(t)\), respectively, then the impulse response of the cascade system is \(h(t)=h_{1}(t)*h_{2}(t)\). Hence, \(H(s)=H_{1}(s)H_{2}(s)\). In the present case,

\[H(s)=\left(\frac{1}{s-1}\right)\left(\frac{s-1}{s+1}\right)=\frac{1}{s+1}\]

The pole of \(\mathcal{S}_{1}\) at \(s=1\) cancels with the zero at \(s=1\) of \(\mathcal{S}_{2}\). This results in a composite system having a single pole at \(s=-1\). If the composite cascade system were to be enclosed inside a

Figure 4.9: Distinction between BIBO and asymptotic stability.

black box with only the input and the output terminals accessible, any measurement from these external terminals would show that the transfer function of the system is \(1/(s+1)\), without any hint of the fact that the system is housing an unstable system (Fig. 9b).

The impulse response of the cascade system is \(h(t)=e^{-t}u(t)\), which is absolutely integrable. Consequently, the system is BIBO-stable.

To determine the asymptotic stability, we note that \(\mathcal{S}_{1}\) has one characteristic root at \(1\), and \(\mathcal{S}_{2}\) also has one root at \(-1\). Recall that the two systems are independent (one does not load the other), and the characteristic modes generated in each subsystem are independent of the other. Clearly, the mode \(e^{t}\) will not be eliminated by the presence of \(\mathcal{S}_{2}\). Hence, the composite system has two characteristic roots, located at \(\pm 1\), and the system is asymptotically unstable, though BIBO-stable.

Interchanging the positions of \(\mathcal{S}_{1}\) and \(\mathcal{S}_{2}\) makes no difference in this conclusion. This example shows that BIBO stability can be misleading. If a system is asymptotically unstable, it will destroy itself (or, more likely, lead to saturation condition) because of unchecked growth of the response due to intended or unintended stray initial conditions. BIBO stability is not going to save the system. Control systems are often compensated to realize certain desirable characteristics. One should never try to stabilize an unstable system by canceling its RHP pole(s) with RHP zero(s). Such a misguided attempt will fail, not because of the practical impossibility of exact cancellation but for the more fundamental reason, as just explained.

### 4.9 BIBO and Asymptotic Stability

Show that an ideal integrator is marginally stable but BIBO-unstable.

### 4.3-4 Inverse Systems

If \(H(s)\) is the transfer function of a system \(\mathcal{S}\), then \(\mathcal{S}_{i}\), its inverse system has a transfer function \(H_{i}(s)\) given by

\[H_{i}(s)=\frac{1}{H(s)}\]

This follows from the fact the cascade of \(\mathcal{S}\) with its inverse system \(\mathcal{S}_{i}\) is an identity system, with impulse response \(\delta(t)\), implying \(H(s)H_{i}(s)=1\). For example, an ideal integrator and its inverse, an ideal differentiator, have transfer functions \(1/s\) and \(s\), respectively, leading to \(H(s)H_{i}(s)=1\).

## 4.4 Analysis of Electrical Networks:

The Transformed Network

Example 4.12 shows how electrical networks may be analyzed by writing the integro-differential equation(s) of the system and then solving these equations by the Laplace transform. We now show that it is also possible to analyze electrical networks directly without having to write the integro-differential equations. This procedure is considerably simpler because it permits us to treat an electrical network as if it were a resistive network. For this purpose, we need to represent a network in the "frequency domain" where all the voltages and currents are represented by their Laplace transforms.

For the sake of simplicity, let us first discuss the case with zero initial conditions. If \(v(t)\) and \(i(t)\) are the voltage across and the current through an inductor of \(L\) henries, then

\[v(t)=L\frac{di(t)}{dt}\]

The Laplace transform of this equation (assuming zero initial current) is

\[V(s)=LsI(s)\]

Similarly, for a capacitor of \(C\) farads, the voltage-current relationship is \(i(t)=C(dv/dt)\) and its Laplace transform, assuming zero initial capacitor voltage, yields \(I(s)=CsV(s)\); that is,

\[V(s)=\frac{1}{Cs}I(s)\]

For a resistor of \(R\) ohms, the voltage-current relationship is \(v(t)=Ri(t)\), and its Laplace transform is

\[V(s)=RI(s)\]

Thus, in the "frequency domain," the voltage-current relationships of an inductor and a capacitor are algebraic; these elements behave like resistors of "resistance" \(Ls\) and \(1/Cs\), respectively. The generalized "resistance" of an element is called its _impedance_ and is given by the ratio \(V(s)/I(s)\) for the element (under zero initial conditions). The impedances of a resistor of \(R\) ohms, an inductor of \(L\) henries, and a capacitance of \(C\) farads are \(R\), \(Ls\), and \(1/Cs\), respectively.

Also, the interconnection constraints (Kirchhoff's laws) remain valid for voltages and currents in the frequency domain. To demonstrate this point, let \(v_{j}(t)\) (\(j=1,2,\ldots,k\)) be the voltages across \(k\) elements in a loop and let \(i_{j}(t)(j=1,2,\ldots\,.,m)\) be the \(j\) currents entering a node. Then

\[\sum_{j=1}^{k}v_{j}(t)=0\qquad\text{and}\qquad\sum_{j=1}^{m}i_{j}(t)=0\]

Now if

\[v_{j}(t)\Longleftrightarrow V_{j}(s)\qquad\text{and}\qquad i_{j}(t) \Longleftrightarrow I_{j}(s)\]

then

\[\sum_{j=1}^{k}V_{j}(s)=0\qquad\text{and}\qquad\sum_{j=1}^{m}I_{j}(s)=0\]

This result shows that if we represent all the voltages and currents in an electrical network by their Laplace transforms, we can treat the network as if it consisted of the "resistances" \(R\), \(Ls\), and \(1/Cs\) corresponding to a resistor \(R\), an inductor \(L\), and a capacitor \(C\), respectively. The system equations (loop or node) are now algebraic. Moreover, the simplification techniques that have been developed for resistive circuits--equivalent series and parallel impedances, voltage and current divider rules, Thevenin and Norton theorems--can be applied to general electrical networks. The following examples demonstrate these concepts.

```
EXAMPLE 4.17Transform Analysis of a Simple Circuit
```

Find the loop current \(i(t)\) in the circuit shown in Fig. 10a if all the initial conditions are zero.

In the first step, we represent the circuit in the frequency domain, as illustrated in Fig. 10b. All the voltages and currents are represented by their Laplace transforms. The voltage \(10u(t)\) is represented by \(10/s\) and the (unknown) current \(i(t)\) is represented by its Laplace transform \(I(s)\). All the circuit elements are represented by their respective impedances. The inductor of 1 henry is represented by \(s\), the capacitor of \(1/2\) farad is represented by \(2/s\), and the resistor of 3 ohms is represented by 3. We now consider the frequency-domain representation of voltages and currents. The voltage across any element is \(I(s)\) times its impedance. Therefore, the total voltage drop in the loop is \(I(s)\) times the total loop impedance, and it must be equal to \(V(s)\), (transform of) the input voltage. The total impedance in the loop is

\[Z(s)=s+3+\frac{2}{s}=\frac{s^{2}+3s+2}{s}\]

The input"voltage" is \(V(s)=10/s\). Therefore, the "loop current" \(I(s)\) is

\[I(s)=\frac{V(s)}{Z(s)}=\frac{10/s}{(s^{2}+3s+2)/s}=\frac{10}{s^{2}+3s+2}=\frac {10}{(s+1)(s+2)}=\frac{10}{s+1}-\frac{10}{s+2}\]

The inverse transform of this equation yields the desired result:

\[i(t)=10(e^{-t}-e^{-2t})u(t)\]

Figure 10: **(a)** A circuit and **(b)** its transformed version.

## Chapter 4 Continuous-time system analysis

### 4.1 Initial Condition Generators

The discussion in which we assumed zero initial conditions can be readily extended to the case of nonzero initial conditions because the initial condition in a capacitor or an inductor can be represented by an equivalent source. We now show that a capacitor \(C\) with an initial voltage \(v(0^{-})\) (Fig. 4.11a) can be represented in the frequency domain by an uncharged capacitor of impedance \(1/Cs\) in series with a voltage source of value \(v(0^{-})/s\) (Fig. 4.11b) or as the same uncharged capacitor in parallel with a current source of value \(Cv(0^{-})\) (Fig. 4.11c). Similarly, an inductor \(L\) with an initial current \(i(0^{-})\) (Fig. 4.11d) can be represented in the frequency domain by an inductor of impedance \(Ls\) in series with a voltage source of value \(Li(0^{-})\) (Fig. 4.11e) or by the same inductor in parallel with a current source of value \(i(0^{-})/s\) (Fig. 4.11f).

To prove this point, consider the terminal relationship of the capacitor in Fig. 4.11a:

\[i(t)=C\frac{dv(t)}{dt}\]

The Laplace transform of this equation yields

\[I(s)=C[sV(s)-v(0^{-})]\]

Figure 4.11: Initial condition generators for a capacitor and an inductor.

This equation can be rearranged as

\[V(s)=\frac{1}{Cs}I(s)+\frac{v(0^{-})}{s} \tag{4.33}\]

Observe that \(V(s)\) is the voltage (in the frequency domain) across the charged capacitor and \(I(s)/Cs\) is the voltage across the same capacitor without any charge. Therefore, the charged capacitor can be represented by the uncharged capacitor in series with a voltage source of value \(v(0^{-})/s\), as depicted in Fig. 4.11b. Equation (4.33) can also be rearranged as

\[V(s)=\frac{1}{Cs}[I(s)+Cv(0^{-})]\]

This equation shows that the charged capacitor voltage \(V(s)\) is equal to the uncharged capacitor voltage caused by a current \(I(s)+Cv(0^{-})\). This result is reflected precisely in Fig. 4.11c, where the current through the uncharged capacitor is \(I(s)+Cv(0^{-})\).2

Footnote 2: In the time domain, a charged capacitor \(C\) with initial voltage \(v(0^{-})\) can be represented as the same capacitor uncharged in series with a voltage source \(v(0^{-})u(t)\), or in parallel with a current source \(Cv(0^{-})\delta(t)\). Similarly, an inductor \(L\) with initial current \(i(0^{-})\) can be represented by the same inductor with zero initial current in series with a voltage source \(Li(0^{-})\delta(t)\) or with a parallel current source \(i(0^{-})u(t)\).

For the inductor in Fig. 4.11d, the terminal equation is

\[v(t)=L\frac{di(t)}{dt}\]

and

\[V(s)=L[sI(s)-i(0^{-})]=LsI(s)-Li(0^{-}) \tag{4.34}\]

This expression is consistent with Fig. 4.11e. We can rearrange Eq. (4.34) as

\[V(s)=Ls\bigg{[}I(s)-\frac{i(0^{-})}{s}\bigg{]}\]

This expression is consistent with Fig. 4.11f.

Let us rework Ex. 4.13 using these concepts. Figure 4.12a shows the circuit in Fig. 4.7b with the initial conditions \(y(0^{-})=2\) and \(v_{C}(0^{-})=10\). Figure 4.12b shows the frequency-domain representation (transformed circuit) of the circuit in Fig. 4.12a. The resistor is represented by its impedance \(2\); the inductor with initial current of \(2\) amperes is represented according to the arrangement in Fig. 4.11e with a series voltage source \(Ly(0^{-})=2\). The capacitor with initial voltage of \(10\) volts is represented according to the arrangement in Fig. 4.11b with a series voltage source \(v(0^{-})/s=10/s\). Note that the impedance of the inductor is \(s\) and that of the capacitor is \(5/s\). The input of \(10u(t)\) is represented by its Laplace transform \(10/s\).

The total voltage in the loop is \((10/s)+2-(10/s)=2\), and the loop impedance is \((s+2+(5/s))\). Therefore,

\[Y(s)=\frac{2}{s+2+5/s}=\frac{2s}{s^{2}+2s+5}\]

which confirms our earlier result in Ex. 4.13.

[MISSING_PAGE_EMPTY:49]

amperes. Therefore, when the switch is opened (at \(t=0\)), the initial conditions are \(v_{C}(0^{-})=16\) and \(y_{2}(0^{-})=4\). Figure 4.13b shows the transformed version of the circuit in Fig. 4.13a. We have used equivalent sources to account for the initial conditions. The initial capacitor voltage of 16 volts is represented by a series voltage of \(16/s\) and the initial inductor current of 4 amperes is represented by a source of value \(Ly_{2}(0^{-})=2\).

From Fig. 4.13b, the loop equations can be written directly in the frequency domain as

\[\frac{Y_{1}(s)}{s}+\frac{1}{5}[Y_{1}(s)-Y_{2}(s)]=\frac{4}{s}\] \[-\frac{1}{5}Y_{1}(s)+\frac{6}{5}Y_{2}(s)+\frac{s}{2}Y_{2}(s)=2\] \[\begin{bmatrix}\frac{1}{s}+\frac{1}{5}&-\frac{1}{5}\\ -\frac{1}{5}&\frac{6}{5}+\frac{s}{2}\end{bmatrix}\begin{bmatrix}Y_{1}(s)\\ Y_{2}(s)\end{bmatrix}=\begin{bmatrix}\frac{4}{s}\\ 2\end{bmatrix}\]

Application of Cramer's rule to this equation yields

\[Y_{1}(s)=\frac{24(s+2)}{s^{2}+7s+12}=\frac{24(s+2)}{(s+3)(s+4)}=\frac{-24}{s+ 3}+\frac{48}{s+4}\]

and

\[y_{1}(t)=(-24e^{-3t}+48e^{-4t})u(t)\]

Similarly, we obtain

\[Y_{2}(s)=\frac{4(s+7)}{s^{2}+7s+12}=\frac{16}{s+3}-\frac{12}{s+4}\]

and

\[y_{2}(t)=(16e^{-3t}-12e^{-4t})u(t)\]

We also could have used Thevenin's theorem to compute \(Y_{1}(s)\) and \(Y_{2}(s)\) by replacing the circuit to the right of the capacitor (right of terminals \(ab\)) with its Thevenin equivalent, as shown in Fig. 4.13c. Figure 4.13b shows that the Thevenin impedance \(Z(s)\) and the Thevenin source \(V(s)\) are

\[Z(s)=\frac{\frac{1}{5}\left(\frac{s}{2}+1\right)}{\frac{1}{5}+ \frac{s}{2}+1}=\frac{s+2}{5s+12}\] \[V(s)=\frac{-\frac{1}{5}}{\frac{1}{5}+ \frac{s}{2}+1}2=\frac{-4}{5s+12}\]

According to Fig. 4.13c, the current \(Y_{1}(s)\) is given by

\[Y_{1}(s)=\frac{\frac{4}{s}-V(s

[MISSING_PAGE_FAIL:51]

[MISSING_PAGE_EMPTY:52]

### 4.4 Analysis of Active Circuits

Although we have considered examples of only passive networks so far, the circuit analysis procedure using the Laplace transform is also applicable to active circuits. All that is needed is to replace the active elements with their mathematical models (or equivalent circuits) and proceed as before.

The operational amplifier (depicted by the triangular symbol in Fig. 4.16a) is a well-known element in modern electronic circuits. The terminals with the positive and the negative signs correspond to noninverting and inverting terminals, respectively. This means that the polarity of the output voltage \(v_{2}\) is the same as that of the input voltage at the terminal marked by the positive sign (noninverting). The opposite is true for the inverting terminal, marked by the negative sign.

Figure 4.16b shows the model (equivalent circuit) of the operational amplifier (op amp) in Fig. 4.16a. A typical op amp has a very large gain. The output voltage \(v_{2}=-Av_{1}\), where \(A\) is typically \(10^{5}\) to \(10^{6}\). The input impedance is very high, of the order of \(10^{12}\,\Omega\), and the output impedance is very low (50-100 \(\Omega\)). For most applications, we are justified in assuming the gain \(A\) and the input impedance to be infinite and the output impedance to be zero. For this reason we see an ideal voltage source at the output.

Consider now the operational amplifier with resistors \(R_{a}\) and \(R_{b}\) connected, as shown in Fig. 4.16c. This configuration is known as the _noninverting amplifier_. Observe that the input polarities in this configuration are inverted in comparison to those in Fig. 4.16a. We now show that the output voltage \(v_{2}\) and the input voltage \(v_{1}\) in this case are related by

\[v_{2}=Kv_{1},\qquad\text{where }K=1+\tfrac{R_{b}}{R_{a}}\]

First, we recognize that because the input impedance and the gain of the operational amplifier approach infinity, the input current \(i_{x}\) and the input voltage \(v_{x}\) in Fig. 4.16c are infinitesimal and may be taken as zero. The dependent source in this case is \(Av_{x}\) instead of \(-Av_{x}\) because of the input polarity inversion. The dependent source \(Av_{x}\) (see Fig. 4.16b) at the output will generate current \(i_{o}\), as illustrated in Fig. 4.16c. Now

\[v_{2}=(R_{b}+R_{a})i_{o}\]

Figure 4.15: Circuit for Drill 4.10.

and also

\[v_{1}=v_{x}+R_{a}i_{o}=R_{a}i_{o}\]

Therefore,

\[\frac{v_{2}}{v_{1}}=\frac{R_{b}+R_{a}}{R_{a}}=1+\frac{R_{b}}{R_{a}}=K\]

or

\[v_{2}(t)=Kv_{1}(t)\]

The equivalent circuit of the noninverting amplifier is depicted in Fig. 16d.

##### 4.20 Transform Analysis of a Sallen-Key Circuit

The circuit in Fig. 17a is called the _Sallen-Key_ circuit, which is frequently used in filter design. Find the transfer function \(H(s)\) relating the output voltage \(v_{o}(t)\) to the input voltage \(v_{i}(t)\).

Figure 16: Operational amplifier and its equivalent circuit.

We are required to find

\[H(s)=\frac{V_{o}(s)}{V_{i}(s)}\]

assuming all initial conditions to be zero.

Figure 17b shows the transformed version of the circuit in Fig. 17a. The noninverting amplifier is replaced by its equivalent circuit. All the voltages are replaced by their Laplace transforms, and all the circuit elements are shown by their impedances. All the initial conditions are assumed to be zero, as required for determining \(H(s)\).

We shall use node analysis to derive the result. There are two unknown node voltages, \(V_{a}(s)\) and \(V_{b}(s)\), requiring two node equations.

At node \(a\), \(I_{R_{1}}(s)\), the current in \(R_{1}\) (leaving the node \(a\)), is \([V_{a}(s)-V_{i}(s)]/R_{1}\). Similarly, \(I_{R_{2}}(s)\), the current in \(R_{2}\) (leaving the node \(a\)), is \([V_{a}(s)-V_{b}(s)]/R_{2}\), and \(I_{C_{1}}(s)\), the current in capacitor \(C_{1}\) (leaving the node \(a\)), is \([V_{a}(s)-V_{o}(s)]C_{1}s=[V_{a}(s)-KV_{b}(s)]C_{1}s\).

Figure 17: **(a)** Sallen–Key circuit and **(b)** its equivalent.

The sum of all the three currents is zero. Therefore,

\[\frac{V_{a}(s)-V_{i}(s)}{R_{1}}+\frac{V_{a}(s)-V_{b}(s)}{R_{2}}+[V_{a}(s)-KV_{b}( s)]C_{1}s=0\]

or

\[\left(\frac{1}{R_{1}}+\frac{1}{R_{2}}+C_{1}s\right)V_{a}(s)-\left(\frac{1}{R_{2 }}+KC_{1}s\right)V_{b}(s)=\frac{1}{R_{1}}V_{i}(s)\]

Similarly, the node equation at node \(b\) yields

\[\frac{V_{b}(s)-V_{a}(s)}{R_{2}}+C_{2}sV_{b}(s)=0\]

or

\[-\frac{1}{R_{2}}V_{a}(s)+\left(\frac{1}{R_{2}}+C_{2}s\right)V_{b}(s)=0\]

The two node equations in two unknown node voltages \(V_{a}(s)\) and \(V_{b}(s)\) can be expressed in matrix form as

\[\begin{bmatrix}G_{1}+G_{2}+C_{1}s&-(G_{2}+KC_{1}s)\\ -G_{2}&(G_{2}+C_{2}s)\end{bmatrix}\begin{bmatrix}V_{a}(s)\\ V_{b}(s)\end{bmatrix}=\begin{bmatrix}G_{1}V_{i}(s)\\ 0\end{bmatrix}\]

where

\[G_{1}=\frac{1}{R_{1}}\qquad\text{and}\qquad G_{2}=\frac{1}{R_{2}}\]

Application of Cramer's rule yields

\[\frac{V_{b}(s)}{V_{i}(s)} =\frac{G_{1}G_{2}}{C_{1}C_{2}s^{2}+[G_{1}C_{2}+G_{2}C_{2}+G_{2}C_ {1}(1-K)]s+G_{1}G_{2}}\] \[=\frac{{\omega_{0}}^{2}}{s^{2}+2\alpha s+{\omega_{0}}^{2}}\]

where

\[K=1+\frac{R_{b}}{R_{a}}\qquad\text{and}\qquad{\omega_{0}}^{2}= \frac{G_{1}G_{2}}{C_{1}C_{2}}=\frac{1}{R_{1}R_{2}C_{1}C_{2}}\] \[2\alpha=\frac{G_{1}C_{2}+G_{2}C_{2}+G_{2}C_{1}(1-K)}{C_{1}C_{2}}= \frac{1}{R_{1}C_{1}}+\frac{1}{R_{2}C_{1}}+\frac{1}{R_{2}C_

## Chapter 4 Continuous-Time System Analysis

### 4.5 Block Diagrams

Large systems may consist of an enormous number of components or elements. As anyone who has seen the circuit diagram of a radio or a television receiver can appreciate, analyzing such systems all at once could be next to impossible. In such cases, it is convenient to represent a system by suitably interconnected subsystems, each of which can be readily analyzed. Each subsystem can be characterized in terms of its input-output relationships. A linear system can be characterized by its transfer function \(H(s)\). Figure 4.18a shows a block diagram of a system with a transfer function \(H(s)\) and its input and output \(X(s)\) and \(Y(s)\), respectively.

Subsystems may be interconnected by using cascade, parallel, and feedback interconnections (Figs. 4.18b, 4.18c, 4.18d), the three elementary types. When transfer functions appear in cascade, as depicted in Fig. 4.18b, then, as shown earlier, the transfer function of the overall system is the product of the two transfer functions. This result can also be proved by observing that in Fig. 4.18b

\[\frac{Y(s)}{X(s)}=\frac{W(s)}{X(s)}\;\frac{Y(s)}{W(s)}=H_{1}(s)H_{2}(s)\]

Figure 4.18: Elementary connections of blocks and their equivalents.

We can extend this result to any number of transfer functions in cascade. It follows from this discussion that the subsystems in cascade can be interchanged without affecting the overall transfer function. This commutation property of LTI systems follows directly from the commutative (and associative) property of convolution. We have already proved this property in Sec. 2.4-3. Every possible ordering of the subsystems yields the same overall transfer function. However, there may be practical consequences (such as sensitivity to parameter variation) affecting the behavior of different ordering.

Similarly, when two transfer functions, \(H_{1}(s)\) and \(H_{2}(s)\), appear in parallel, as illustrated in Fig. 4.18c, the overall transfer function is given by \(H_{1}(s)+H_{2}(s)\), the sum of the two transfer functions. The proof is trivial. This result can be extended to any number of systems in parallel.

When the output is fed back to the input, as shown in Fig. 4.18d, the overall transfer function \(Y(s)/X(s)\) can be computed as follows. The inputs to the adder are \(X(s)\) and \(-H(s)Y(s)\). Therefore, \(E(s)\), the output of the adder, is

\[E(s)=X(s)-H(s)Y(s)\]

But

\[Y(s) =G(s)E(s)\] \[=G(s)[X(s)-H(s)Y(s)]\]

Therefore,

\[Y(s)[1+G(s)H(s)]=G(s)X(s)\]

so that

\[\frac{Y(s)}{X(s)}=\frac{G(s)}{1+G(s)H(s)} \tag{4.35}\]

Therefore, the feedback loop can be replaced by a single block with the transfer function shown in Eq. (4.35) (see Fig. 4.18d).

In deriving these equations, we implicitly assume that when the output of one subsystem is connected to the input of another subsystem, the latter does not load the former. For example, the transfer function \(H_{1}(s)\) in Fig. 4.18b is computed by assuming that the second subsystem \(H_{2}(s)\) was not connected. This is the same as assuming that \(H_{2}(s)\) does not load \(H_{1}(s)\). In other words, the input-output relationship of \(H_{1}(s)\) will remain unchanged regardless of whether \(H_{2}(s)\) is connected. Many modern circuits use op amps with high input impedances, so this assumption is justified. When such an assumption is not valid, \(H_{1}(s)\) must be computed under operating conditions [i.e., with \(H_{2}(s)\) connected].

**Example 4.21**: **Transfer Functions of Feedback Systems Using MATLAB**

Consider the feedback system of Fig. 4.18d with \(G(s)=K/(s(s+8))\) and \(H(s)=1\). Use MATLAB to determine the transfer function for each of the following cases: **(a)**\(K=7\), **(b)**\(K=16\), and **(c)**\(K=80\).

We solve these cases using the control system toolbox function feedback.

[MISSING_PAGE_FAIL:59]

### Direct Form I Realization

Rather than realize the general \(N\)th-order system described by Eq. (4.36), we begin with a specific case of the following third-order system and then extend the results to the \(N\)th-order case:

\[H(s)=\frac{b_{0}s^{3}+b_{1}s^{2}+b_{2}s+b_{3}}{s^{3}+a_{1}s^{2}+a_{2}s+a_{3}}= \frac{b_{0}+\frac{b_{1}}{s}+\frac{b_{2}}{s^{2}}+\frac{b_{3}}{s^{3}}}{1+\frac{a _{1}}{s}+\frac{a_{2}}{s^{2}}+\frac{a_{3}}{s^{3}}}\]

We can express \(H(s)\) as

\[H(s)=\underbrace{\left(b_{0}+\frac{b_{1}}{s}+\frac{b_{2}}{s^{2}}+\frac{b_{3}} {s^{3}}\right)}_{H_{1}(s)}\underbrace{\left(\frac{1}{1+\frac{a_{1}}{s}+ \frac{a_{2}}{s^{2}}+\frac{a_{3}}{s^{3}}}\right)}_{H_{2}(s)}\]

We can realize \(H(s)\) as a cascade of transfer function \(H_{1}(s)\) followed by \(H_{2}(s)\), as depicted in Fig. 4.20a, where the output of \(H_{1}(s)\) is denoted by \(W(s)\). Because of the commutative property of LTI system transfer functions in cascade, we can also realize \(H(s)\) as a cascade of \(H_{2}(s)\) followed by \(H_{1}(s)\), as illustrated in Fig. 4.20b, where the (intermediate) output of \(H_{2}(s)\) is denoted by \(V(s)\).

The output of \(H_{1}(s)\) in Fig. 4.20a is given by \(W(s)=H_{1}(s)X(s)\). Hence,

\[W(s)=\left(b_{0}+\frac{b_{1}}{s}+\frac{b_{2}}{s^{2}}+\frac{b_{3}}{s^{3}}\right) X(s) \tag{4.37}\]

Also, the output \(Y(s)\) and the input \(W(s)\) of \(H_{2}(s)\) in Fig. 4.20a are related by \(Y(s)=H_{2}(s)W(s)\). Hence,

\[W(s)=\left(1+\frac{a_{1}}{s}+\frac{a_{2}}{s^{2}}+\frac{a_{3}}{s^{3}}\right)Y(s) \tag{4.38}\]

Figure 4.19: **(a)** Time-domain and **(b)** frequency-domain representations of an integrator.

Figure 4.20: Realization of a transfer function in two steps.

We shall first realize \(H_{1}(s)\). Equation (4.37) shows that the output \(W(s)\) can be synthesized by adding the input \(b_{0}X(s)\) to \(b_{1}(X(s)/s),b_{2}(X(s)/s^{2})\), and \(b_{3}(X(s)/s^{3})\). Because the transfer function of an integrator is \(1/s\), the signals \(X(s)/s,X(s)/s^{2}\), and \(X(s)/s^{3}\) can be obtained by successive integration of the input \(x(t)\). The left-half section of Fig. 4.21a shows how \(W(s)\) can be synthesized from \(X(s)\), according to Eq. (4.37). Hence, this section represents a realization of \(H_{1}(s)\).

To complete the picture, we shall realize \(H_{2}(s)\), which is specified by Eq. (4.38). We can rearrange Eq. (4.38) as

\[Y(s)=W(s)-\left(\frac{a_{1}}{s}+\frac{a_{2}}{s^{2}}+\frac{a_{3}}{s^{3}}\right) Y(s) \tag{4.39}\]

Hence, to obtain \(Y(s)\), we subtract \(a_{1}Y(s)/s\), \(a_{2}Y(s)/s^{2}\), and \(a_{3}Y(s)/s^{3}\) from \(W(s)\). We have already obtained \(W(s)\) from the first step [output of \(H_{1}(s)\)]. To obtain signals \(Y(s)/s\), \(Y(s)/s^{2}\), and \(Y(s)/s^{3}\), we assume that we already have the desired output \(Y(s)\). Successive integration of \(Y(s)\) yields the needed signals \(Y(s)/s\), \(Y(s)/s^{2}\), and \(Y(s)/s^{3}\). We now synthesize the final output \(Y(s)\) according to Eq. (4.39), as seen in the right-half section of Fig. 4.21a.1 The left-half section in Fig. 4.21a represents \(H_{1}(s)\) and the right-half is \(H_{2}(s)\). We can generalize this procedure, known as the _direct form I_ (DFI) realization, for any value of \(N\). This procedure requires \(2N\) integrators to realize an \(N\)th-order transfer function, as shown in Fig. 4.21b.

Footnote 1: It may seem odd that we first assumed the existence of \(Y(s)\), integrated it successively, and then in turn generated \(Y(s)\) from \(W(s)\) and the three successive integrals of signal \(Y(s)\). This procedure poses a dilemma similar to “Which came first, the chicken or the egg?” The problem here is satisfactorily resolved by writing the expression for \(Y(s)\) at the output of the right-hand adder (at the top) in Fig. 4.21a and verifying that this expression is indeed the same as Eq. (4.38).

### Direct Form II Realization

In the direct form I, we realize \(H(s)\) by implementing \(H_{1}(s)\) followed by \(H_{2}(s)\), as shown in Fig. 4.20a. We can also realize \(H(s)\), as shown in Fig. 4.20b, where \(H_{2}(s)\) is followed by \(H_{1}(s)\).

Figure 4.21: Direct form I realization of an LTIC system: **(a)** third-order and **(b)**\(N\)th-order.

This procedure is known as the _direct form II_ realization. Figure 4.22a shows direct form II realization, where we have interchanged sections representing \(H_{1}(s)\) and \(H_{2}(s)\) in Fig. 4.21b. The output of \(H_{2}(s)\) in this case is denoted by \(V(s)\).4

Footnote 4: The reader can show that the equations relating \(X(s),V(s)\), and \(Y(s)\) in Fig. 4.22a are

\[V(s)=X(s)-\left(\frac{a_{1}}{s}+\frac{a_{2}}{s^{2}}+\cdot\cdot\cdot+\frac{a_{N }}{s^{N}}\right)V(s)\]

 and \[Y(s)=\left(b_{0}+\frac{b_{1}}{s}+\frac{b_{2}}{s^{2}}+\cdot\cdot\cdot+\frac{b_{ N}}{s^{N}}\right)V(s)\]

An interesting observation in Fig. 4.22a is that the input signal to both the chains of integrators is \(V(s)\). Clearly, the outputs of integrators in the left-side chain are identical to the corresponding outputs of the right-side integrator chain, thus making the right-side chain redundant. We can eliminate this chain and obtain the required signals from the left-side chain, as shown in Fig. 4.22b. This implementation halves the number of integrators to \(N\), and, thus, is more efficient in hardware utilization than either Figs. 4.21b or 4.22a. This is the _direct form II_ (DFII) realization.

An \(N\)th-order differential equation with \(N=M\) has a property that its implementation requires a minimum of \(N\) integrators. A realization is _canonic_ if the number of integrators used in the realization is equal to the order of the transfer function realized. Thus, canonic realization has no redundant integrators. The DFII form in Fig. 4.22b is a canonic realization, and is also called the _direct canonic_ form. Note that the DFI is noncanonic.

The direct form I realization (Fig. 4.22b) implements zeros first [the left-half section represented by \(H_{1}(s)\)] followed by realization of poles [the right-half section represented by \(H_{2}(s)\)] of \(H(s)\). In contrast, canonic direct implements poles first followed by zeros. Although both these realizations result in the same transfer function, they generally behave differently from the viewpoint of sensitivity to parameter variations.

Figure 4.22: Direct form II realization of an \(N\)th-order LTIC system.

**Example 4.22**: **Canonic Direct Form Realizations**

Find the canonic direct form realization of the following transfer functions:

\[\begin{array}{ll}\mbox{\bf(a)}&\frac{5}{s+7}\\ \mbox{\bf(b)}&\frac{s}{s+7}\\ \mbox{\bf(c)}&\frac{s+5}{s+7}\\ \mbox{\bf(d)}&\frac{4s+28}{s^{2}+6s+5}\\ \end{array}\]

All four of these transfer functions are special cases of \(H(s)\) in Eq. (4.36).

**(a)** The transfer function \(5/(s+7)\) is of the first order (\(N=1\)); therefore, we need only one integrator for its realization. The feedback and feedforward coefficients are

\[a_{1}=7\qquad\mbox{and}\qquad b_{0}=0,\quad b_{1}=5\]

The realization is depicted in Fig. 4.23a. Because \(N=1\), there is a single feedback connection from the output of the integrator to the input adder with coefficient \(a_{1}=7\). For \(N=1\), generally, there are \(N+1=2\) feedforward connections. However, in this case, \(b_{0}=0\), and there is only one feedforward connection with coefficient \(b_{1}=5\) from the output of the integrator to the output adder. Because there is only one input signal to the output adder, we can do away with the adder, as shown in Fig. 4.23a.

**(b)**

\[H(s)=\frac{s}{s+7}\]

In this first-order transfer function, \(b_{1}=0\). The realization is shown in Fig. 4.23b. Because there is only one signal to be added at the output adder, we can discard the adder.

**(c)**

\[H(s)=\frac{s+5}{s+7}\]

The realization appears in Fig. 4.23c. Here \(H(s)\) is a first-order transfer function with \(a_{1}=7\) and \(b_{0}=1\), \(b_{1}=5\). There is a single feedback connection (with coefficient \(7\)) from the integrator output to the input adder. There are two feedforward connections (Fig. 4.23c).2

Footnote 2: When \(M=N\) (as in this case), \(H(s)\) can also be realized in another way by recognizing that

\[H(s)=1-\frac{2}{s+7}\]

We now realize \(H(s)\) as a parallel combination of two transfer functions, as indicated by this equation.

\[H(s)=\frac{4s+28}{s^{2}+6s+5}\]

This is a second-order system with \(b_{0}=0\), \(b_{1}=4\), \(b_{2}=28\), \(a_{1}=6\), and \(a_{2}=5\). Figure 4.23d shows a realization with two feedback connections and two feedforward connections.

### 4.11 Canonic Direct Form Realization

Give the canonic direct realization of

\[H(s)=\frac{2s}{s^{2}+6s+25}\]

### 4.6-3 Cascade and Parallel Realizations

An \(N\)th-order transfer function \(H(s)\) can be expressed as a product or a sum of \(N\) first-order transfer functions. Accordingly, we can also realize \(H(s)\) as a cascade (series) or parallel form of these \(N\) first-order transfer functions. Consider, for instance, the transfer function in part (d) of Ex. 4.22.

\[H(s)=\frac{4s+28}{s^{2}+6s+5}\]

Figure 4.23: Realizations of \(H(s)\) for Ex. 4.22.

We can express \(H(s)\) as

\[H(s)=\frac{4s+28}{(s+1)(s+5)}=\underbrace{\left(\frac{4s+28}{s+1}\right)}_{H_{1}( s)}\underbrace{\left(\frac{1}{s+5}\right)}_{H_{2}(s)}\]

We can also express \(H(s)\) as a sum of partial fractions as

\[H(s)=\frac{4s+28}{(s+1)(s+5)}=\frac{6}{\underbrace{s+1}_{H_{3}(s)}}-\frac{2}{ \underbrace{s+5}_{H_{4}(s)}}\]

These equations give us the option of realizing \(H(s)\) as a cascade of \(H_{1}(s)\) and \(H_{2}(s)\), as shown in Fig. 4.24a, or a parallel of \(H_{3}(s)\) and \(H_{4}(s)\), as depicted in Fig. 4.24b. Each of the first-order transfer functions in Fig. 4.24 can be implemented by using canonic direct realizations, discussed earlier.

This discussion by no means exhausts all the possibilities. In the cascade form alone, there are different ways of grouping the factors in the numerator and the denominator of \(H(s)\), and each grouping can be realized in DFI or canonic direct form. Accordingly, several cascade forms are possible. In Sec. 4.6-4, we shall discuss yet another form that essentially doubles the numbers of realizations discussed so far.

From a practical viewpoint, parallel and cascade forms are preferable because parallel and certain cascade forms are numerically less sensitive than canonic direct form to small parameter variations in the system. Qualitatively, this difference can be explained by the fact that in a canonic realization all the coefficients interact with each other, and a change in any coefficient will be magnified through its repeated influence from feedback and feedforward connections. In a parallel realization, in contrast, the change in a coefficient will affect only a localized segment; the case with a cascade realization is similar.

In the examples of cascade and parallel realization, we have separated \(H(s)\) into first-order factors. For \(H(s)\) of higher orders, we could group \(H(s)\) into factors, not all of which are necessarily of the first order. For example, if \(H(s)\) is a third-order transfer function, we could realize this function as a cascade (or a parallel) combination of a first-order and a second-order factor.

Figure 4.24: Realization of \((4s+28)/[(s+1)(s+5)]\): **(a)** cascade form and **(b)** parallel form.

## Realization of Complex Conjugate Poles

The complex poles in \(H(s)\) should be realized as a second-order (quadratic) factor because we cannot implement multiplication by complex numbers. Consider, for example,

\[H(s) =\frac{10s+50}{(s+3)(s^{2}+4s+13)}\] \[=\frac{10s+50}{(s+3)(s+2-j3)(s+2+j3)}\] \[=\frac{2}{s+3}-\frac{1+j2}{s+2-j3}-\frac{1-j2}{s+2+j3}\]

We cannot realize first-order transfer functions individually with the poles \(-2\pm j3\) because they require multiplication by complex numbers in the feedback and the feedforward paths. Therefore, we need to combine the conjugate poles and realize them as a second-order transfer function.1 In the present example, we can create a cascade realization from \(H(s)\) expressed in product form as

Footnote 1: It is possible to realize complex, conjugate poles indirectly by using a cascade of two first-order transfer functions and feedback. A transfer function with poles \(-a\pm jb\) can be realized by using a cascade of two identical first-order transfer functions, each having a pole at \(-a\) (see Prob. 4.6-15).

\[H(s)=\bigg{(}\frac{10}{s+3}\bigg{)}\bigg{(}\frac{s+5}{s^{2}+4s+13}\bigg{)}\]

Similarly, we can create a parallel realization from \(H(s)\) expressed in sum form as

\[H(s)=\frac{2}{s+3}-\frac{2s-8}{s^{2}+4s+13}\]

## Realization of Repeated Poles

When repeated poles occur, the procedure for canonic and cascade realization is exactly the same as before. For a parallel realization, however, the procedure requires special handling, as explained in Ex. 4.23.

**Example 4.23**: **Parallel Realization**

Determine the parallel realization of

\[H(s)=\frac{7s^{2}+37s+51}{(s+2)(s+3)^{2}}=\frac{5}{s+2}+\frac{2}{s+3}-\frac{3} {(s+3)^{2}}\]

This third-order transfer function should require no more than three integrators. But if we try to realize each of the three partial fractions separately, we require four integrators because of the one second-order term. This difficulty can be avoided by observing that the terms \(1/(s+3)\) and\(1/(s+3)^{2}\) can be realized with a cascade of two subsystems, each having a transfer function \(1/(s+3)\), as shown in Fig. 4.25. Each of the three first-order transfer functions in Fig. 4.25 may now be realized as in Fig. 4.23.

##### 4.6.1.2 Canonic, Cascade, and Parallel Realizations

Find the canonic, cascade, and parallel realization of

\[H(s)=\frac{s+3}{s^{2}+7s+10}=\left(\frac{s+3}{s+2}\right)\left(\frac{1}{s+5}\right)\]

### 4.6-4 Transposed Realization

Two realizations are said to be _equivalent_ if they have the same transfer function. A simple way to generate an equivalent realization from a given realization is to use its _transpose_. To generate a transpose of any realization, we change the given realization as follows:

1. Reverse all the arrow directions without changing the scalar multiplier values.
2. Replace pickoff nodes by adders and vice versa.
3. Replace the input \(X(s)\) with the output \(Y(s)\) and vice versa.

Figure 4.26a shows the transposed version of the canonic direct form realization in Fig. 4.22b found according to the rules just listed. Figure 4.26b is Fig. 4.26a reoriented in the conventional form so that the input \(X(s)\) appears at the left and the output \(Y(s)\) appears at the right. Observe that this realization is also canonic.

Rather than prove the theorem on equivalence of the transposed realizations, we shall verify that the transfer function of the realization in Fig. 4.26b is identical to that in Eq. (4.36).

Figure 4.26b shows that \(Y(s)\) is being fed back through \(N\) paths. The fed-back signal appearing at the input of the top adder is

\[\left(\frac{-a_{1}}{s}+\frac{-a_{2}}{s^{2}}+\cdot\cdot\cdot+\frac{-a_{N-1}}{s^{N -1}}+\frac{-a_{N}}{s^{N}}\right)Y(s)\]

The signal \(X(s)\), fed to the top adder through \(N+1\) forward paths, contributes

\[\left(b_{0}+\frac{b_{1}}{s}+\cdot\cdot\cdot+\frac{b_{N-1}}{s^{N-1}}+\frac{b_{N }}{s^{N}}\right)X(s)\]

The output \(Y(s)\) is equal to the sum of these two signals (feed forward and feed back). Hence,

\[Y(s) =\left(\frac{-a_{1}}{s}+\frac{-a_{2}}{s^{2}}+\cdot\cdot\cdot+ \frac{-a_{N-1}}{s^{N-1}}+\frac{-a_{N}}{s^{N}}\right)Y(s)\] \[\quad+\left(b_{0}+\frac{b_{1}}{s}+\cdot\cdot\cdot+\frac{b_{N-1}}{ s^{N-1}}+\frac{b_{N}}{s^{N}}\right)X(s)\]

Transporting all the \(Y(s)\) terms to the left side and multiplying throughout by \(s^{N}\), we obtain

\[(s^{N}+a_{1}s^{N-1}+\cdot\cdot\cdot+a_{N-1}s+a_{N})Y(s)=(b_{0}s^{N}+b_{1}s^{N- 1}+\cdot\cdot\cdot+b_{N-1}s+b_{N})X(s)\]

Consequently,

\[H(s)=\frac{Y(s)}{X(s)}=\frac{b_{0}s^{N}+b_{1}s^{N-1}+\cdot\cdot\cdot+b_{N-1}s+ b_{N}}{s^{N}+a_{1}s^{N-1}+\cdot\cdot\cdot+a_{N-1}s+a_{N}}\]

Hence, the transfer function \(H(s)\) is identical to that in Eq. (4.36).

Figure 4.26: Realization of an \(N\)th-order LTI transfer function in the transposed form.

We have essentially doubled the number of possible realizations. Every realization that was found earlier has a transpose. Note that the transpose of a transpose results in the same realization.

**Example 4.24**: Transposed Realizations

Find the transpose canonic direct realizations for parts (a) and (d) of Ex. 4.22 (Figs. 4.23c and 4.23d). The transfer functions are:

**(a)**: \(\dfrac{s+5}{s+7}\)
**(b)**: \(\dfrac{4s+28}{s^{2}+6s+5}\)

Both these realizations are special cases of the one in Fig. 4.26b.

**(a)** In this case, \(N=1\) with \(a_{1}=7,b_{0}=1,b_{1}=5\). The desired realization can be obtained by transposing Fig. 4.23c. However, we already have the general model of the transposed realization in Fig. 4.26b. The desired solution is a special case of Fig. 4.26b with \(N=1\) and \(a_{1}=7,b_{0}=1,b_{1}=5\), as shown in Fig. 4.27a.

**(b)** In this case, \(N=2\) with \(b_{0}=0\), \(b_{1}=4\), \(b_{2}=28\), \(a_{1}=6\), \(a_{2}=5\). Using the model of Fig. 4.26b, we obtain the desired realization, as shown in Fig. 4.27b.

**(a)**: \(\dfrac{Y(s)}{s}\)
**(b)**: \(\dfrac{Y(s)}{s}\)
**(a)**: \(\dfrac{Y(s)}{s}\)
**(b)**: \(\dfrac{Y(s)}{s}\)
**(b)**: \(\dfrac{Y(s)}{s}\)
**(c)**: \(\dfrac{Y(s)}{s}\)
**(d)**: \(\dfrac{Y(s)}{s}\)
**(e)**: \(\dfrac{Y(s)}{s}\)
**(f)**: \(\dfrac{Y(s)}{s}\)
**(g)**: \(\dfrac{Y(s)}{s}\)
**(h)**: \(\dfrac{Y(s)}{s}\)

### 4.6-5 Using Operational Amplifiers for System Realization

In this section, we discuss practical implementation of the realizations described in Sec. 4.6-4. Earlier we saw that the basic elements required for the synthesis of an LTIC system (or a given transfer function) are (scalar) multipliers, integrators, and adders. All these elements can be realized by operational amplifier (op-amp) circuits.

Operational Amplifier Circuits

Figure 4.28 shows an op-amp circuit in the frequency domain (the transformed circuit). Because the input impedance of the op amp is infinite (very high), all the current \(I(s)\) flows in the feedback path, as illustrated. Moreover \(V_{x}(s)\), the voltage at the input of the op amp, is zero (very small) because of the infinite (very large) gain of the op amp. Therefore, for all practical purposes,

\[Y(s)=-I(s)Z_{f}(s)\]

Moreover, because \(v_{x}\approx 0\),

\[I(s)=\frac{X(s)}{Z(s)}\]

Substitution of the second equation in the first yields

\[Y(s)=-\frac{Z_{f}(s)}{Z(s)}X(s)\]

Therefore, the op-amp circuit in Fig. 4.28 has the transfer function

\[H(s)=-\frac{Z_{f}(s)}{Z(s)}\]

By properly choosing \(Z(s)\) and \(Z_{f}(s)\), we can obtain a variety of transfer functions, as the following development shows.

Figure 4.28: A basic inverting configuration op-amp circuit.

## Chapter 4 Continuous-Time System Analysis

### 4.1 The Scalar Multiplier

If we use a resistor \(R_{f}\) in the feedback and a resistor \(R\) at the input (Fig. 4.29a), then \(Z_{f}(s)=R_{f}\), \(Z(s)=R\), and

\[H(s)=-\frac{R_{f}}{R}\]

The system acts as a scalar multiplier (or an amplifier) with a negative gain \(R_{f}/R\). A positive gain can be obtained by using two such multipliers in cascade or by using a single noninverting amplifier, as depicted in Fig. 4.16c. Figure 4.29a also shows the compact symbol used in circuit diagrams for a scalar multiplier.

### 4.2 The Linear Multiplier

If we use a resistor \(R\) at the input (Fig. 4.29b), then \(Z_{f}(s)=1/Cs\), \(Z(s)=R\), and

\[H(s)=\left(-\frac{1}{RC}\right)\frac{1}{s}\]

The system acts as an ideal integrator with a gain \(-1/RC\). Figure 4.29b also shows the compact symbol used in circuit diagrams for an integrator.

Figure 4.29: **(a)** Op-amp inverting amplifier. **(b)** Integrator.

### The Adder

Consider now the circuit in Fig. 4.30a with \(r\) inputs \(X_{1}(s)\), \(X_{2}(s)\), \(\ldots\), \(X_{r}(s)\). As usual, the input voltage \(V_{x}(s)\simeq 0\) because the op-amp gain \(\rightarrow\infty\). Moreover, the current going into the op amp is very small (\(\simeq 0\)) because the input impedance \(\rightarrow\infty\). Therefore, the total current in the feedback resistor \(R_{f}\) is \(I_{1}(s)+I_{2}(s)+\cdot\cdot\cdot+I_{r}(s)\). Moreover, because \(V_{x}(s)=0\),

\[I_{j}(s)=\frac{X_{j}(s)}{R_{j}}\qquad j=1,2,\ldots,r\]

Also,

\[Y(s) =-R_{f}[I_{1}(s)+I_{2}(s)+\cdot\cdot\cdot+I_{r}(s)]\] \[=-\bigg{[}\frac{R_{f}}{R_{1}}X_{1}(s)+\frac{R_{f}}{R_{2}}X_{2}(s) +\cdot\cdot\cdot+\frac{R_{f}}{R_{r}}X_{r}(s)\bigg{]}\] \[=k_{1}X_{1}(s)+k_{2}X_{2}(s)+\cdot\cdot\cdot+k_{r}X_{r}(s)\]

where

\[k_{i}=\frac{-R_{f}}{R_{i}}\]

Clearly, the circuit in Fig. 4.30 serves an adder and an amplifier with any desired gain for each of the input signals. Figure 4.30b shows the compact symbol used in circuit diagrams for an adder with \(r\) inputs.

### 4.25 Op-Amp Realization

Use op-amp circuits to realize the canonic direct form of the transfer function

\[H(s)=\frac{2s+5}{s^{2}+4s+10}\]

Figure 4.30: Op-amp summing and amplifying circuit.

Figure 4.31: Op-amp realization of a second-order transfer function \((2s+5)/(s^{2}+4s+10)\).

The basic canonic realization is shown in Fig. 4.31a. The same realization with horizontal reorientation is shown in Fig. 4.31b. Signals at various points are also indicated in the realization. For convenience, we denote the output of the last integrator by \(W(s)\). Consequently, the signals at the inputs of the two integrators are \(sW(s)\) and \(s^{2}W(s)\), as shown in Figs. 4.31a and 4.31b. Op-amp elements (multipliers, integrators, and adders) change the polarity of the output signals. To incorporate this fact, we modify the canonic realization in Fig. 4.31b to that depicted in Fig. 4.31c. In Fig. 4.31b, the successive outputs of the adder and the integrators are \(s^{2}W(s),sW(s)\), and \(W(s)\), respectively. Because of polarity reversals in op-amp circuits, these outputs are \(-s^{2}W(s),sW(s)\), and \(-W(s)\), respectively, in Fig. 4.31c. This polarity reversal requires corresponding modifications in the signs of feedback and feedforward gains. According to Fig. 4.31b,

\[s^{2}W(s)=X(s)-4sW(s)-10W(s)\]

Therefore,

\[-s^{2}W(s)=-X(s)+4sW(s)+10W(s)\]

Because the adder gains are always negative (see Fig. 4.30b), we rewrite the foregoing equation as

\[-s^{2}W(s)=-1[X(s)]-4[-sW(s)]-10[-W(s)]\]

Figure 4.31c shows the implementation of this equation. The hardware realization appears in Fig. 4.31d. Both integrators have a unity gain, which requires \(RC=1\). We have used \(R=100\) k\(\Omega\) and \(C=10\)\(\mu\)F. The gain of 10 in the outer feedback path is obtained in the adder by choosing the feedback resistor of the adder to be 100 k\(\Omega\) and an input resistor of 10 k\(\Omega\). Similarly, the gain of 4 in the inner feedback path is obtained by using the corresponding input resistor of 25 k\(\Omega\). The gains of 2 and 5, required in the feedforward connections, are obtained by using a feedback resistor of 100 k\(\Omega\) and input resistors of 50 and 20 k\(\Omega\), respectively.1

Footnote 1: It is possible to avoid the two inverting op amps (with gain \(-1\)) in Fig. 4.31d by adding signal \(sW(s)\) to the input and output adders directly, using the noninverting amplifier configuration in Fig. 4.16d.

The op-amp realization in Fig. 4.31 is not necessarily the one that uses the fewest op amps. This example is given just to illustrate a systematic procedure for designing an op-amp circuit of an arbitrary transfer function. There are more efficient circuits (such as Sallen-Key or biquad) that use fewer op amps to realize a second-order transfer function.

## 4.14 Transfer Functions of Op-Amp Circuits

Show that the transfer functions of the op-amp circuits in Figs. 4.32a and 4.32b are \(H_{1}(s)\) and \(H_{2}(s)\), respectively, where

\[H_{1}(s)=\frac{-R_{f}}{R}\left(\frac{a}{s+a}\right)\qquad a= \frac{1}{R_{f}C_{f}}\] \[H_{2}(s)=-\frac{C}{C_{f}}\left(\frac{s+b}{s+a}\right)\qquad a= \frac{1}{R_{f}C_{f}}\quad b=\frac{1}{RC}\]

### 4.7 Application to Feedback and Controls

Generally, systems are designed to produce a desired output \(y(t)\) for a given input \(x(t)\). Using the given performance criteria, we can design a system, as shown in Fig. 4.33a. Ideally, such an open-loop system should yield the desired output. In practice, however, the system characteristics change with time, as a result of aging or replacement of some components, or because of changes in the operating environment. Such variations cause changes in the output for the same input. Clearly, this is undesirable in precision systems.

A possible solution to this problem is to add a signal component to the input that is not a predetermined function of time but will change to counteract the effects of changing system characteristics and the environment. In short, we must provide a correction at the system input to account for the undesired changes just mentioned. Yet since these changes are generally unpredictable, it is not clear how to preprogram appropriate corrections to the input. However, the difference between the actual output and the desired output gives an indication of the suitable

Figure 4.33: **(a)** Open-loop and **(b)** closed-loop (feedback) systems.

Figure 4.32: Op-amp circuits for Drill 4.14.

correction to be applied to the system input. It may be possible to counteract the variations by feeding the output (or some function of output) back to the input.

We unconsciously apply this principle in daily life. Consider an example of marketing a certain product. The optimum price of the product is the value that maximizes the profit of a merchant. The output in this case is the profit, and the input is the price of the item. The output (profit) can be controlled (within limits) by varying the input (price). The merchant may price the product too high initially, in which case, he will sell too few items, reducing the profit. Using feedback of the profit (output), he adjusts the price (input), to maximize his profit. If there is a sudden or unexpected change in the business environment, such as a strike-imposed shutdown of a large factory in town, the demand for the item goes down, thus reducing his output (profit). He adjusts his input (reduces price) using the feedback of the output (profit) in a way that will optimize his profit in the changed circumstances. If the town suddenly becomes more prosperous because a new factory opens, he will increase the price to maximize the profit. Thus, by continuous feedback of the output to the input, he realizes his goal of maximum profit (optimum output) in any given circumstances. We observe thousands of examples of feedback systems around us in everyday life. Most social, economical, educational, and political processes are, in fact, feedback processes. A block diagram of such a system, called the _feedback_ or _closed-loop_ system, is shown in Fig. 4.33b.

A feedback system can address the problems arising because of unwanted disturbances such as random-noise signals in electronic systems, a gust of wind affecting a tracking antenna, a meteorite hitting a spacecraft, and the rolling motion of antiaircraft gun platforms mounted on ships or moving tanks. Feedback may also be used to reduce nonlinearities in a system or to control its rise time (or bandwidth). Feedback is used to achieve, with a given system, the desired objective within a given tolerance, despite partial ignorance of the system and the environment. A feedback system, thus, has an ability for supervision and self-correction in the face of changes in the system parameters and external disturbances (change in the environment).

Consider the feedback amplifier in Fig. 4.34. Let the forward amplifier gain \(G=10{,}000\). One-hundredth of the output is fed back to the input (\(H=0.01\)). The gain \(T\) of the feedback amplifier is obtained by [see Eq. (4.35)]

\[T=\frac{G}{1+GH}=\frac{10{,}000}{1+100}=99.01\]

Suppose that because of aging or replacement of some transistors, the gain \(G\) of the forward amplifier changes from 10,000 to 20,000. The new gain of the feedback amplifier is given by

\[T=\frac{G}{1+GH}=\frac{20{,}000}{1+200}=99.5\]

Surprisingly, 100% variation in the forward gain \(G\) causes only 0.5% variation in the feedback amplifier gain \(T\). Such reduced sensitivity to parameter variations is a must in precision amplifiers. In this example, we reduced the sensitivity of gain to parameter variations at the cost of forward

Figure 4.34: Effects of negative and positive feedback.

gain, which is reduced from 10,000 to 99. There is no dearth of forward gain (obtained by cascading stages). But low sensitivity is extremely precious in precision systems.

Now, consider what happens when we add (instead of subtract) the signal fed back to the input. Such addition means the sign on the feedback connection is \(+\) instead of \(-\) (which is same as changing the sign of \(H\) in Fig. 4.34). Consequently,

\[T=\frac{G}{1-GH}\]

If we let \(G=10\),000 as before and \(H=0.9\times 10^{-4}\), then

\[T=\frac{10,000}{1-0.9(10^{4})(10^{-4})}=100,000\]

Suppose that because of aging or replacement of some transistors, the gain of the forward amplifier changes to 11,000. The new gain of the feedback amplifier is

\[T=\frac{11,000}{1-0.9(11,000)(10^{-4})}=1,100,000\]

Observe that in this case, a mere 10% increase in the forward gain \(G\) caused 1000% increase in the gain \(T\) (from 100,000 to 1,100,000). Clearly, the amplifier is very sensitive to parameter variations. This behavior is exactly opposite of what was observed earlier, when the signal fed back was subtracted from the input.

What is the difference between the two situations? Crudely speaking, the former case is called the _negative feedback_ and the latter is the _positive feedback_. The positive feedback increases system gain but tends to make the system more sensitive to parameter variations. It can also lead to instability. In our example, if \(G\) were to be 111,111, then \(GH=1\), \(T=\infty\), and the system would become unstable because the signal fed back was exactly equal to the input signal itself, since \(GH=1\). Hence, once a signal has been applied, no matter how small and how short in duration, it comes back to reinforce the input undiminished, which further passes to the output, and is fed back again and again. In essence, the signal perpetuates itself forever. This perpetuation, even when the input ceases to exist, is precisely the symptom of instability.

Generally speaking, a feedback system cannot be described in black and white terms, such as positive or negative. Usually \(H\) is a frequency-dependent component, more accurately represented by \(H(s)\); hence it varies with frequency. Consequently, what was negative feedback at lower frequencies can turn into positive feedback at higher frequencies and may give rise to instability. This is one of the serious aspects of feedback systems, which warrants a designer's careful attention.

### Analysis of a Simple Control System

Figure 4.35a represents an automatic position control system, which can be used to control the angular position of a heavy object (e.g., a tracking antenna, an anti-aircraft gun mount, or the position of a ship). The input \(\theta_{i}\) is the desired angular position of the object, which can be set at any given value. The actual angular position \(\theta_{o}\) of the object (the output) is measured by a potentiometer whose wiper is mounted on the output shaft. The difference between the input

(set at the desired output position) and the output \(\theta_{o}\) (actual position) is amplified; the amplified output, which is proportional to \(\theta_{i}-\theta_{o}\), is applied to the motor input. If \(\theta_{i}-\theta_{o}=0\) (the output being equal to the desired angle), there is no input to the motor, and the motor stops. But if \(\theta_{o}\neq\theta_{i}\), there will be a nonzero input to the motor, which will turn the shaft until \(\theta_{o}=\theta_{i}\). It is evident that by setting the input potentiometer at a desired position in this system, we can control the angular position of a heavy remote object.

The block diagram of this system is shown in Fig. 4.35b. The amplifier gain is \(K\), where \(K\) is adjustable. Let the motor (with load) transfer function that relates the output angle \(\theta_{o}\) to the motor input voltage be \(G(s)\) [for a starting point, see Eq. (1.32)]. This feedback arrangement is identical to that in Fig. 4.18d with \(H(s)=1\). Hence, \(T(s)\), the (closed-loop) system transfer function relating the output \(\theta_{o}\) to the input \(\theta_{i}\), is

\[\frac{\Theta_{o}(s)}{\Theta_{i}(s)}=T(s)=\frac{KG(s)}{1+KG(s)}\]

From this equation, we shall investigate the behavior of the automatic position control system in Fig. 4.35a for a step and a ramp input.

### Step Input

If we desire to change the angular position of the object instantaneously, we need to apply a step input. We may then want to know how long the system takes to position itself at the desired angle, whether it reaches the desired angle, and whether it reaches the desired position smoothly (monotonically) or oscillates about the final position. If the system oscillates, we may want to know how long it takes for the oscillations to settle down. All these questions can be readily answered by finding the output \(\theta_{o}(t)\) when the input \(\theta_{i}(t)=u(t)\). A step input implies instantaneous change in the angle. This input would be one of the most difficult to follow; if the system can perform well for this input, it is likely to give a good account of itself under most other expected situations. This is why we test control systems for a step input.

For the step input \(\theta_{i}(t)=u(t)\), \(\Theta_{i}(s)=1/s\) and

\[\Theta_{o}(s)=\frac{1}{s}T(s)=\frac{KG(s)}{s[1+KG(s)]}\]

Let the motor (with load) transfer function relating the load angle \(\theta_{o}(t)\) to the motor input voltage be \(G(s)=1/(s(s+8))\). This yields

\[\Theta_{o}(s)=\frac{\frac{K}{s(s+8)}}{s[1+\frac{K}{s(s+8)}]}=\frac{K}{s(s^{2} +8s+K)}\]

Let us investigate the system behavior for three different values of gain \(K\).

For \(K=7\),

\[\Theta_{o}(s)=\frac{7}{s(s^{2}+8s+7)}=\frac{7}{s(s+1)(s+7)}=\frac{1}{s}-\frac {\frac{7}{6}}{s+1}+\frac{\frac{1}{6}}{s+7}\]Figure 4.35: **(a)** An automatic position control system. **(b)** Its block diagram. **(c)** The unit step response. **(d)** The unit ramp response.

and

\[\theta_{o}(t)=\left(1-\tfrac{7}{6}e^{-t}+\tfrac{1}{6}e^{-7t}\right)u(t)\]

This response, illustrated in Fig. 4.35c, shows that the system reaches the desired angle, but at a rather leisurely pace. To speed up the response let us increase the gain to, say, 80.

For \(K=80\),

\[\Theta_{o}(s) =\frac{80}{s(s^{2}+8s+80)}=\frac{80}{s(s+4-j8)(s+4+j8)}\] \[=\frac{1}{s}+\frac{\frac{\sqrt{5}}{4}e^{j153^{\circ}}}{s+4-j8}+ \frac{\frac{\sqrt{5}}{4}e^{-j153^{\circ}}}{s+4+j8}\]

and

\[\theta_{o}(t)=\left[1+\tfrac{\sqrt{5}}{2}e^{-4t}\cos{(8t+153^{\circ})}\right] u(t)\]

This response, also depicted in Fig. 4.35c, achieves the goal of reaching the final position at a faster rate than that in the earlier case (\(K=7\)). Unfortunately the improvement is achieved at the cost of ringing (oscillations) with high overshoot. In the present case, the _percent overshoot_ (PO) is 21%. The response reaches its peak value at _peak time_\(t_{p}=0.393\) second. The _rise time_, defined as the time required for the response to rise from 10% to 90% of its steady-state value, indicates the speed of response.2 In the present case \(t_{r}=0.175\) second. The steady-state value of the response is unity so that the _steady-state error_ is zero. Theoretically it takes infinite time for the response to reach the desired value of unity. In practice, however, we may consider the response to have settled to the final value if it closely approaches the final value. A widely accepted measure of closeness is within 2% of the final value. The time required for the response to reach and stay within 2% of the final value is called the settling time \(t_{s}\).3 In Fig. 4.35c, we find \(t_{s}\approx 1\) second (when \(K=80\)). A good system has a small overshoot, small \(t_{r}\) and \(t_{s}\) and a small steady-state error.

Footnote 3: _Delay time \(t_{d}\)_, defined as the time required for the response to reach 50% of its steady-state value, is another indication of speed. For the present case, \(t_{d}=0.141\) second.

Footnote 3: Typical percentage values used are 2 to 5% for \(t_{s}\).

A large overshoot, as in the present case, may be unacceptable in many applications. Let us try to determine \(K\) (the gain) that yields the fastest response without oscillations. Complex characteristic roots lead to oscillations; to avoid oscillations, the characteristic roots should be real. In the present case, the characteristic polynomial is \(s^{2}+8s+K\). For \(K>16\), the characteristic roots are complex; for \(K<16\), the roots are real. The fastest response without oscillations is obtained by choosing \(K=16\). We now consider this case.

For \(K=16\),

\[\Theta_{o}(s)=\frac{16}{s(s^{2}+8s+16)}=\frac{16}{s(s+4)^{2}}=\frac{1}{s}- \frac{1}{s+4}-\frac{4}{(s+4)^{2}}\]

and

\[\theta_{o}(t)=[1-(4t+1)e^{-4t}]u(t)\]

This response also appears in Fig. 4.35c. The system with \(K>16\) is said to be _underdamped_ (oscillatory response), whereas the system with \(K<16\) is said to be _overdamped_. For \(K=16\), the system is said to be _critically damped_.

There is a trade-off between undesirable overshoot and rise time. Reducing overshoots leads to higher rise time (sluggish system). In practice, a small overshoot, which is still faster than the critical damping, may be acceptable. Note that percent overshoot PO and peak time \(t_{p}\) are meaningless for the overdamped or critically damped cases. In addition to adjusting gain \(K\), we may need to augment the system with some type of compensator if the specifications on overshoot and the speed of response are too stringent.

### Ramp Input

If the anti-aircraft gun in Fig. 4.35a is tracking an enemy plane moving with a uniform velocity, the gun-position angle must increase linearly with \(t\). Hence, the input in this case is a ramp; that is, \(\theta_{i}(t)=tu(t)\). Let us find the response of the system to this input when \(K=80\). In this case, \(\Theta_{i}(s)=1/s^{2}\), and

\[\Theta_{o}(s)=\frac{80}{s^{2}(s^{2}+8s+80)}=-\frac{0.1}{s}+\frac{1}{s^{2}}+ \frac{0.1(s-2)}{s^{2}+8s+80}\]

Use of Table 4.1 yields

\[\theta_{o}(t)=\left[-0.1+t+\tfrac{1}{8}e^{-8t}\cos{(8t+36.87^{\circ})}\right]u (t)\]

This response, sketched in Fig. 4.35d, shows that there is a steady-state error \(e_{r}=0.1\) radian. In many cases such a small steady-state error may be tolerable. If, however, a zero steady-state error to a ramp input is required, this system in its present form is unsatisfactory. We must add some form of compensator to the system.

**Example 4.26**: **Step and Ramp Responses of Feedback Systems Using MATLAB**

Using the feedback system of Fig. 4.18d with \(G(s)=K/(s(s+8))\) and \(H(s)=1\), determine the step response for each of the following cases: **(a)**\(K=7\), **(b)**\(K=16\), and **(c)**\(K=80\). Additionally, find the unit ramp response when **(d)**\(K=80\).

Example 4.21 computes the transfer functions of these feedback systems in a simple way. In this example, the conv command is used to demonstrate polynomial multiplication of the two denominator factors of \(G(s)\). Step responses are computed by using the step command.

**(a-c)**

>> H = tf(1,1); K = 7; G = tf([K],conv([1 0],[1 8])); Ha = feedback(G,H); >> H = tf(1,1); K = 16; G = tf([K],conv([1 0],[1 8])); Hb = feedback(G,H); >> H = tf(1,1); K = 80; G = tf([K],conv([1 0],[1 8])); Hc = feedback(G,H); >> clf; step(Ha,'k-',Hb,'k-'-',Hc,'k-.'); >> legend('K = 7','K = 16','K = 80','Location','best');

**(d)** The unit ramp response is equivalent to the integral of the unit step response. We can obtain the ramp response by taking the step response of the system in cascade with an integrator. To help highlight waveform detail, we compute the ramp response over the short time interval of \(0\leq t\leq 1.5\).

>> t = 0:.001:1.5; Hd = series(Hc,tf([1],[1 0])); >> step(Hd,'k-',t); title('Unit Ramp Response');

### 4.7 Application to Feedback and Controls

Now the reader has some idea of the various specifications a control system might require. Generally, a control system is designed to meet given transient specifications, steady-state error specifications, and sensitivity specifications. Transient specifications include overshoot, rise time, and settling time of the response to step input. The steady-state error is the difference between

Figure 4.36: Step responses for Ex. 4.26.

Figure 4.37: Ramp response for Ex. 4.26 with \(K=80\).

the desired response and the actual response to a test input in steady state. The system should also satisfy a specified sensitivity specifications to some system parameter variations, or to certain disturbances. Above all, the system must remain stable under operating conditions. Discussion of design procedures used to realize given specifications is beyond the scope of this book.

### 4.8 Frequency Response of an LTIC System

Filtering is an important area of signal processing. Filtering characteristics of a system are indicated by its response to sinusoids of various frequencies varying from 0 to \(\infty\). Such characteristics are called the frequency response of the system. In this section, we shall find the frequency response of LTIC systems.

In Sec. 2.4-4 we showed that an LTIC system response to an everlasting exponential input \(x(t)=e^{st}\) is also an everlasting exponential \(H(s)e^{st}\). As before, we use an arrow directed from the input to the output to represent an input-output pair:

\[e^{st}\Longrightarrow H(s)e^{st} \tag{4.40}\]

Setting \(s=j\omega\) in this relationship yields

\[e^{i\omega t}\Longrightarrow H(j\omega)e^{i\omega t} \tag{4.41}\]

Noting that \(\cos\omega t\) is the real part of \(e^{j\omega t}\), use of Eq. (2.31) yields

\[\cos\omega t\Longrightarrow\text{Re}[H(j\omega)e^{j\omega t}] \tag{4.42}\]

We can express \(H(j\omega)\) in the polar form as

\[H(j\omega)=|H(j\omega)|e^{j\angle H(j\omega)}\]

With this result, Eq. (4.42) becomes

\[\cos\omega t\Longrightarrow|H(j\omega)|\cos\left[\omega t+\angle H(j\omega)\right]\]

In other words, the system response \(y(t)\) to a sinusoidal input \(\cos\omega t\) is given by

\[y(t)=|H(j\omega)|\cos\left[\omega t+\angle H(j\omega)\right]\]

Using a similar argument, we can show that the system response to a sinusoid \(\cos\left(\omega t+\theta\right)\) is

\[y(t)=|H(j\omega)|\cos\left[\omega t+\theta+\angle H(j\omega)\right] \tag{4.43}\]

This result is valid only for BIBO-stable systems. The frequency response is meaningless for BIBO-unstable systems. This follows from the fact that the frequency response in Eq. (4.41) is obtained by setting \(s=j\omega\) in Eq. (4.40). But, as shown in Sec. 2.4-4 [Eqs. (2.38) and (2.39)], Eq. (4.40) applies only for the values of \(s\) for which \(H(s)\) exists. For BIBO-unstable systems, the ROC for \(H(s)\) does not include the \(\omega\) axis where \(s=j\omega\) [see Eq. (4.10)]. This means that \(H(s)\) when \(s=j\omega\) is meaningless for BIBO-unstable systems.+

Footnote †: \({}^{\dagger}\) This may also be argued as follows. For BIBO-unstable systems, the zero-input response contains nondecaying natural mode terms of the form \(\cos\omega_{0}t\) or \(e^{at}\cos\omega_{0}t\) (\(a>0\)). Hence, the response of such a system to a sinusoid \(\cos\omega t\) will contain not just the sinusoid of frequency \(\omega\), but also nondecaying natural modes, rendering the concept of frequency response meaningless.

Equation (4.43) shows that for a sinusoidal input of radian frequency \(\omega\), the system response is also a sinusoid of the same frequency \(\omega\). _The amplitude of the output sinusoid is \(|H(j\omega)|\) times the input amplitude, and the phase of the output sinusoid is shifted by \(\angle H(j\omega)\) with respect to the input phase_ (see later Fig. 4.38 in Ex. 4.27). For instance, a certain system with \(|H(j10)|=3\) and \(\angle H(j10)=-30^{\circ}\) amplifies a sinusoid of frequency \(\omega=10\) by a factor of 3 and delays its phase by \(30^{\circ}\). The system response to an input \(5\cos\left(10t+50^{\circ}\right)\) is \(3\times 5\cos\left(10t+50^{\circ}-30^{\circ}\right)=15\cos\left(10t+20^{\circ}\right)\).

Clearly \(|H(j\omega)|\) is the amplitude _gain_ of the system, and a plot of \(|H(j\omega)|\) versus \(\omega\) shows the amplitude gain as a function of frequency \(\omega\). We shall call \(|H(j\omega)|\) the _amplitude response_. It also goes under the name _magnitude response_.+ Similarly, \(\angle H(j\omega)\) is the _phase response_, and a plot of \(\angle H(j\omega)\) versus \(\omega\) shows how the system modifies or changes the phase of the input sinusoid. Plots of the magnitude response \(|H(j\omega)|\) and phase response \(\angle H(j\omega)\) show at a glance how a system responds to sinusoids of various frequencies. Observe that \(H(j\omega)\) has the information of \(|H(j\omega)|\) and \(\angle H(j\omega)\) and is therefore termed the _frequency response_ of the system. Clearly, the frequency response of a system represents its filtering characteristics.

Footnote †: \({}^{\dagger}\) This may also be argued as follows. For BIBO-unstable systems, the zero-input response contains nondecaying natural mode terms of the form \(\cos\omega_{0}t\) or \(e^{at}\cos\omega_{0}t\) (\(a>0\)). Hence, the response of such a system to a sinusoid \(\cos\omega t\) will contain not just the sinusoid of frequency \(\omega\), but also nondecaying natural modes, rendering the concept of frequency response meaningless.

Footnote †: \({}^{\ddagger}\) Strictly speaking, \(|H(\omega)|\) is magnitude response. There is a fine distinction between amplitude and magnitude. Amplitude \(A\) can be positive and negative. In contrast, the magnitude \(|A|\) is always nonnegative. We refrain from relying on this useful distinction between amplitude and magnitude in the interest of avoiding proliferation of essentially similar entities. This is also why we shall use the “amplitude” (instead of “magnitude”) spectrum for \(|H(\omega)|\).

## 10 Example 4.27 Frequency Response

Find the frequency response (amplitude and phase responses) of a system whose transfer function is

\[H(s)=\frac{s+0.1}{s+5}\]

Also, find the system response \(y(t)\) if the input \(x(t)\) is

**(a)**: \(\cos 2t\)
**(b)**: \(\cos\left(10t-50^{\circ}\right)\)

In this case,

\[H(j\omega)=\frac{j\omega+0.1}{j\omega+5}\]Therefore, \[|H(j\omega)|=\frac{\sqrt{\omega^{2}+0.01}}{\sqrt{\omega^{2}+25}}\qquad\text{and} \qquad\angle H(j\omega)=\tan^{-1}\left(\frac{\omega}{0.1}\right)-\tan^{-1}\left( \frac{\omega}{5}\right)\] Both the amplitude and the phase response are depicted in Fig. 4.38a as functions of \(\omega\). These plots furnish the complete information about the frequency response of the system to sinusoidal inputs.

**(a)** For the input \(x(t)=\cos 2t\), \(\omega=2\), and

\[|H(j2)| =\frac{\sqrt{(2)^{2}+0.01}}{\sqrt{(2)^{2}+25}}=0.372\] \[\angle H(j2) =\tan^{-1}\left(\frac{2}{0.1}\right)-\tan^{-1}\left(\frac{2}{5} \right)=87.1^{\circ}-21.8^{\circ}=65.3^{\circ}\]

Figure 4.38: Responses for the system of Ex. 4.27.

We also could have read these values directly from the frequency response plots in Fig. 4.38a corresponding to \(\omega=2\). This result means that for a sinusoidal input with frequency \(\omega=2\), the amplitude gain of the system is 0.372, and the phase shift is 65.3\({}^{\circ}\). In other words, the output amplitude is 0.372 times the input amplitude, and the phase of the output is shifted with respect to that of the input by 65.3\({}^{\circ}\). Therefore, the system response to the input \(\cos 2t\) is

\[y(t)=0.372\cos{(2t+65.3^{\circ})}\]

The input \(\cos 2t\) and the corresponding system response \(0.372\cos{(2t+65.3^{\circ})}\) are illustrated in Fig. 4.38b.

**(b)** For the input \(\cos{(10t-50^{\circ})}\), instead of computing the values \(|H(j\omega)|\) and \(\angle H(j\omega)\) as in part (a), we shall read them directly from the frequency response plots in Fig. 4.38a corresponding to \(\omega=10\). These are

\[|H(j10)|=0.894\qquad\mbox{and}\qquad\angle H(j10)=26^{\circ}\]

Therefore, for a sinusoidal input of frequency \(\omega=10\), the output sinusoid amplitude is 0.894 times the input amplitude, and the output sinusoid is shifted with respect to the input sinusoid by 26\({}^{\circ}\). Therefore, the system response \(y(t)\) to an input \(\cos{(10t-50^{\circ})}\) is

\[y(t)=0.894\cos{(10t-50^{\circ}+26^{\circ})}=0.894\cos{(10t-24^{\circ})}\]

If the input were \(\sin{(10t-50^{\circ})}\), the response would be \(0.894\sin{(10t-50^{\circ}+26^{\circ})}=0.894\sin{(10t-24^{\circ})}\).

The frequency response plots in Fig. 4.38a show that the system has highpass filtering characteristics; it responds well to sinusoids of higher frequencies (\(\omega\) well above 5), and suppresses sinusoids of lower frequencies (\(\omega\) well below 5).

Plotting Frequency Response with MATLAB

It is simple to use MATLAB to create magnitude and phase response plots. Here, we consider two methods. In the first method, we use an anonymous function to define the transfer function \(H(s)\) and then obtain the frequency response plots by substituting _j\(\omega\)_ for \(s\).

>> H = @(s) (s+0.1)./(s+5); omega = 0:.01:20; >> subplot(1,2,1); plot(omega,abs(H(1j*omega)),'k-'); >> subplot(1,2,2); plot(omega,angle(H(1j*omega))*180/pi,'k-'); In the second method, we define vectors that contain the numerator and denominator coefficients of \(H(s)\) and then use the freqs command to compute frequency response.

>> B = [1 0.1]; A = [1 5]; H = freqs(B,A,omega); omega = 0:.01:20; >> subplot(1,2,1); plot(omega,abs(H),'k-'); >> subplot(1,2,2); plot(omega,angle(H)*180/pi,'k-'); Both approaches generate plots that match Fig. 4.38a.

**Example 4.28**: **Frequency Responses of Delay, Differentiator, and Integrator Systems**

Find and sketch the frequency responses (magnitude and phase) for **(a)** an ideal delay of \(T\) seconds, **(b)** an ideal differentiator, and **(c)** an ideal integrator.

**(a) Ideal delay of \(T\) seconds.** The transfer function of an ideal delay is [see Eq. (4.30)]

\[H(s)=e^{-sT}\]

Therefore,

\[H(j\omega)=e^{-j\omega T}\]

Consequently,

\[|H(j\omega)|=1\qquad\mbox{and}\qquad\angle H(j\omega)=-\omega T\]

These amplitude and phase responses are shown in Fig. 4.39a. The amplitude response is constant (unity) for all frequencies. The phase shift increases linearly with frequency with a slope of \(-T\). This result can be explained physically by recognizing that if a sinusoid \(\cos\omega t\) is passed through an ideal delay of \(T\) seconds, the output is \(\cos\omega(t-T)\). The output sinusoid amplitude is the same as that of the input for all values of \(\omega\). Therefore, the amplitude response (gain) is unity for all frequencies. Moreover, the output \(\cos\omega(t-T)=\cos\left(\omega t-\omega T\right)\) has a phase shift \(-\omega T\) with respect to the input \(\cos\omega t\). Therefore, the phase response is linearly proportional to the frequency \(\omega\) with a slope \(-T\).

**(b) An ideal differentiator.** The transfer function of an ideal differentiator is [see Eq. (4.31)]

\[H(s)=s\]

Therefore,

\[H(j\omega)=j\omega=\omega e^{j\pi/2}\]

Consequently,

\[|H(j\omega)|=\omega\qquad\mbox{and}\qquad\angle H(j\omega)=\frac{\pi}{2}\]

These amplitude and phase responses are depicted in Fig. 4.39b. The amplitude response increases linearly with frequency, and phase response is constant (\(\pi/2\)) for all frequencies. This result can be explained physically by recognizing that if a sinusoid \(\cos\omega t\) is passed through an ideal differentiator, the output is \(-\omega\sin\omega t=\omega\cos[\omega t+(\pi/2)]\). Therefore, the output sinusoid amplitude is \(\omega\) times the input amplitude; that is, the amplitude response (gain) increases linearly with frequency \(\omega\). Moreover, the output sinusoid undergoes a phase shift \(\pi/2\) with respect to the input \(\cos\omega t\). Therefore, the phase response is constant (\(\pi/2\)) with frequency.

In an ideal differentiator, the amplitude response (gain) is proportional to frequency [\(|H(j\omega)|=\omega\)] so that the higher-frequency components are enhanced (see Fig. 4.39b). All practical signals are contaminated with noise, which, by its nature, is a broadband (rapidly varying) signal containing components of very high frequencies. A differentiator can increase the noise disproportionately to the point of drowning out the desired signal. This is why ideal differentiators are avoided in practice.

**(c) An ideal integrator.** The transfer function of an ideal integrator is [see Eq. (4.32)]

\[H(s)=\frac{1}{s}\]

Therefore,

\[H(j\omega)=\frac{1}{j\omega}=\frac{-j}{\omega}=\frac{1}{\omega}e^{-j\pi/2}\]

Consequently,

\[|H(j\omega)|=\frac{1}{\omega}\qquad\text{and}\qquad\angle H(j\omega)=-\frac{ \pi}{2}\]

These amplitude and phase responses are illustrated in Fig. 4.39c. The amplitude response is inversely proportional to frequency, and the phase shift is constant (\(-\pi/2\)) with frequency. This result can be explained physically by recognizing that if a sinusoid \(\cos\omega t\) is passed through an ideal integrator, the output is \((1/\omega)\sin\omega t=(1/\omega)\cos[\omega t-(\pi/2)]\). Therefore, the amplitude response is inversely proportional to \(\omega\), and the phase response is constant (\(-\pi/2\)

Figure 4.39: Frequency response of an ideal **(a)** delay, **(b)** differentiator, and **(c)** integrator.

with frequency.1 Because its gain is \(1/\omega\), the ideal integrator suppresses higher-frequency components but enhances lower-frequency components with \(\omega<1\). Consequently, noise signals (if they do not contain an appreciable amount of very-low-frequency components) are suppressed (smoothed out) by an integrator.

Footnote 1: A puzzling aspect of this result is that in deriving the transfer function of the integrator in Eq. (4.32), we have assumed that the input starts at \(t=0\). In contrast, in deriving its frequency response, we assume that the everlasting exponential input \(e^{i\omega t}\) starts at \(t=-\infty\). There appears to be a fundamental contradiction between the everlasting input, which starts at \(t=-\infty\), and the integrator, which opens its gates only at \(t=0\). Of what use is everlasting input, since the integrator starts integrating at \(t=0\)? The answer is that the integrator gates are always open, and integration begins whenever the input starts. We restricted the input to start at \(t=0\) in deriving Eq. (4.32) because we were finding the transfer function using the unilateral transform, where the inputs begin at \(t=0\). So the integrator starting to integrate at \(t=0\) is restricted because of the limitations of the unilateral transform method, not because of the limitations of the integrator itself. If we were to find the integrator transfer function using Eq. (2.40), where there is no such restriction on the input, we would still find the transfer function of an integrator as \(1/s\). Similarly, even if we were to use the bilateral Laplace transform, where \(t\) starts at \(-\infty\), we would find the transfer function of an integrator to be \(1/s\). The transfer function of a system is the property of the system and does not depend on the method used to find it.

### 4.8.1 Steady-State Response to Causal Sinusoidal Inputs

So far we have discussed the LTIC system response to everlasting sinusoidal inputs (starting at \(t=-\infty\)). In practice, we are more interested in causal sinusoidal inputs (sinusoids starting at \(t=0\)). Consider the input \(e^{i\omega t}u(t)\), which starts at \(t=0\) rather than at \(t=-\infty\). In this case \(X(s)=1/(s+j\omega)\). Moreover, according to Eq. (4.27), \(H(s)=P(s)/Q(s)\), where \(Q(s)\) is the characteristic polynomial given by \(Q(s)=(s-\lambda_{1})(s-\lambda_{2})\cdot\cdot\cdot(s-\lambda_{N})\).2 Hence,

Footnote 2: For simplicity, we have assumed nonrepeating characteristic roots. The procedure is readily modified for repeated roots, and the same conclusion results.

\[Y(s)=X(s)H(s)=\frac{P(s)}{(s-\lambda_{1})(s-\lambda_{2})\cdot\cdot\cdot(s- \lambda_{N})(s-j\omega)}\]

In the partial fraction expansion of the right-hand side, let the coefficients corresponding to the \(N\) terms \((s-\lambda_{1})\), \((s-\lambda_{2})\), \(\ldots\), \((s-\lambda_{N})\) be \(k_{1}\), \(k_{2}\), \(\ldots\), \(k_{N}\). The coefficient corresponding to the last term \((s-j\omega)\) is \(P(s)/Q(s)|_{s=j\omega}=H(j\omega)\). Hence,

\[Y(s)=\sum_{i=1}^{n}\frac{k_{i}}{s-\lambda_{i}}+\frac{H(j\omega)}{s-j\omega}\]

and

\[y(t)=\underbrace{\sum_{i=1}^{n}k_{i}e^{\lambda_{i}t}u(t)}_{\text{ transient component }y_{\text{tr}}(t)}+\underbrace{H(j\omega)e^{j\omega t}u(t)}_{\text{ steady-state component }y_{\text{ss}}(t)}\]

For an asymptotically stable system, the characteristic mode terms \(e^{j\omega t}\) decay with time, and, therefore, constitute the so-called _transient_ component of the response. The last term \(H(j\omega)e^{j\omega t}\) persists forever, and is the _steady-state_ component of the response given by

\[y_{\text{ss}}(t)=H(j\omega)e^{j\omega t}u(t)\]

This result also explains why an everlasting exponential input \(e^{j\omega t}\) results in the total response \(H(j\omega)e^{j\omega t}\) for BIBO systems. Because the input started at \(t=-\infty\), at any finite time the decaying transient component has long vanished, leaving only the steady-state component. Hence, the total response appears to be \(H(j\omega)e^{j\omega t}\).

From the argument that led to Eq. (4.43), it follows that for a causal sinusoidal input \(\cos\omega t\), the steady-state response \(y_{ss}(t)\) is given by

\[y_{\text{ss}}(t)=|H(j\omega)|\cos\left[\omega t+\angle H(j\omega)\right]u(t)\]

In summary, \(|H(j\omega)|\cos\left[\omega t+\angle H(j\omega)\right]\) is the total response to everlasting sinusoid \(\cos\omega t\). In contrast, it is the steady-state response to the same input applied at \(t=0\).

### 4.9 Bode Plots

Sketching frequency response plots (\(|H(j\omega)|\) and \(\angle H(j\omega)\) versus \(\omega\)) is considerably facilitated by the use of logarithmic scales. The amplitude and phase response plots as a function of \(\omega\) on a logarithmic scale are known as _Bode plots_. By using the asymptotic behavior of the amplitude and the phase responses, we can sketch these plots with remarkable ease, even for higher-order transfer functions.

Let us consider a system with the transfer function

\[H(s)=\frac{K(s+a_{1})(s+a_{2})}{s(s+b_{1})(s^{2}+b_{2}s+b_{3})} \tag{4.44}\]

where the second-order factor (\(s^{2}+b_{2}s+b_{3}\)) is assumed to have complex conjugate roots.2 We shall rearrange Eq. (4.44) in the form

Footnote 2: Coefficients \(a_{1}\), \(a_{2}\) and \(b_{1}\), \(b_{2}\), \(b_{3}\) used in this section are not to be confused with those used in the representation of \(N\)th-order LTIC system equations given earlier [Eqs. (2.1) or (4.26)].

\[H(s)=\frac{Ka_{1}a_{2}}{b_{1}b_{3}}\frac{\biggl{(}\frac{s}{a_{1}}+1\biggr{)} \biggl{(}\frac{s}{a_{2}}+1\biggr{)}}{s\biggl{(}\frac{s}{b_{1}}+1\biggr{)} \biggl{(}\frac{s^{2}}{b_{3}}+\frac{b_{2}}{b_{3}}s+1\biggr{)}}\]

and

\[H(j\omega)=\frac{Ka_{1}a_{2}}{b_{1}b_{3}}\frac{\biggl{(}1+\frac{j\omega}{a_{1} }\biggr{)}\biggl{(}1+\frac{j\omega}{a_{2}}\biggr{)}}{j\omega\biggl{(}1+\frac{ j\omega}{b_{1}}\biggr{)}\biggl{[}1+j\frac{b_{2}\omega}{b_{3}}+\frac{(j\omega)^{2}}{b_ {3}}\biggr{]}}\]

This equation shows that \(H(j\omega)\) is a complex function of \(\omega\). The amplitude response \(|H(j\omega)|\) and the phase response \(\angle H(j\omega)\) are given by

\[|H(j\omega)|=\biggl{|}\frac{Ka_{1}a_{2}}{b_{1}b_{3}}\biggr{|}\frac{\biggl{|}1+ \frac{j\omega}{a_{1}}\biggr{|}\biggl{|}1+\frac{j\omega}{a_{2}}\biggr{|}}{j \omega\biggl{|}\biggl{|}1+j\frac{b_{2}\omega}{b_{1}}+\frac{(j\omega)^{2}}{b_ {3}}\biggr{|}} \tag{4.45}\]

and

\[\angle H(j\omega)=\] \[\qquad\qquad-\angle j\omega-\angle\biggl{(}1+\frac{j\omega}{b_{1 }}\biggr{)}-\angle\biggl{[}1+\frac{jb_{2}\omega}{b_{3}}+\frac{(j\omega)^{2}}{b _{3}}\biggr{]} \tag{4.46}\]

From Eq. (4.46) we see that the phase function consists of the addition of terms of four kinds: (i) the phase of a constant, (ii) the phase of \(j\omega\), which is \(90^{\circ}\) for all values of \(\omega\), (iii) the phase for the first-order term of the form \(1+j\omega/a\), and (iv) the phase of the second-order term

\[\biggl{[}1+\frac{jb_{2}\omega}{b_{3}}+\frac{(j\omega)^{2}}{b_{3}}\biggr{]}\]

We can plot these basic phase functions for \(\omega\) in the range 0 to \(\infty\) and then, using these plots, we can construct the phase function of any transfer function by properly adding these basic responses. Note that if a particular term is in the numerator, its phase is added, but if the term is in the denominator, its phase is subtracted. This makes it easy to plot the phase function \(\angle H(j\omega)\) as a function of \(\omega\). Computation of \(|H(j\omega)|\), unlike that of the phase function, however, involves the multiplication and division of various terms. This is a formidable task, especially when we have to plot this function for the entire range of \(\omega\) (0 to \(\infty\)).

We know that a log operation converts multiplication and division to addition and subtraction. So, instead of plotting \(|H(j\omega)|\), why not plot log \(|H(j\omega)|\) to simplify our task? We can take advantage of the fact that logarithmic units are desirable in several applications, where the variables considered have a very large range of variation. This is particularly true in frequency response plots, where we may have to plot frequency response over a range from a very low frequency, near 0, to a very high frequency, in the range of \(10^{10}\) or higher. A plot on a linear scale of frequencies for such a large range will bury much of the useful information at lower frequencies. Also, the amplitude response may have a very large dynamic range from a low of \(10^{-6}\) to a high of \(10^{6}\). A linear plot would be unsuitable for such a situation. Therefore, logarithmic plots not only simplify our task of plotting, but, fortunately, they are also desirable in this situation.

There is another important reason for using logarithmic scale. The Weber-Fechner law (first observed by Weber in 1834) states that human senses (sight, touch, hearing, etc.) generally respond in a logarithmic way. For instance, when we hear sound at two different power levels, we judge one sound twice as loud when the ratio of the two sound powers is 10. Human senses respond to equal ratios of power, not equal increments in power [10]. This is clearly a logarithmic response.2

Footnote 2: Observe that the frequencies of musical notes are spaced logarithmically (not linearly). The octave is a ratio of 2. The frequencies of the same note in the successive octaves have a ratio of 2. On the Western musical scale, there are 12 distinct notes in each octave. The frequency of each note is about 6% higher than the frequency of the preceding note. Thus, the successive notes are separated not by some constant frequency, but by constant ratio of 1.06.

The logarithmic unit is the _decibel_ and is equal to 20 times the logarithm of the quantity (log to the base 10). Therefore, \(20\log_{10}\)\(|H(j\omega)|\) is simply the log amplitude in decibels (dB).3 Thus, instead of plotting \(|H(j\omega)|\), we shall plot \(20\log_{10}\)\(|H(j\omega)|\) as a function of \(\omega\). These plots (log amplitude and phase) are called _Bode plots_. For the transfer function in Eq. (4.45), the _log amplitude_ is

Footnote 3: Originally, the unit _bel_ (after the inventor of telephone, Alexander Graham Bell) was introduced to represent power ratio as \(\log_{10}P_{2}/P_{1}\) bels. A tenth of this unit is a decibel, as in \(10\log_{10}P_{2}/P_{1}\) decibels. Since the power ratio of two signals is proportional to the amplitude ratio squared, or \(|H(j\omega)|^{2}\), we have \(10\log_{10}P_{2}/P_{1}=10\log_{10}|H(j\omega)|^{2}=20\log_{10}|H(j\omega)|\) dB.

\[20\log|H(j\omega)| = 20\log\left|\frac{Ka_{1}a_{2}}{b_{1}b_{3}}\right|+20\log\left|1 +\frac{j\omega}{a_{1}}\right|+20\log\left|1+\frac{j\omega}{a_{2}}\right|-20 \log|j\omega| \tag{4.47}\] \[-20\log\left|1+\frac{j\omega}{b_{1}}\right|-20\log\left|1+\frac{ jb_{2}\omega}{b_{3}}+\frac{(j\omega)^{2}}{b_{3}}\right|\]

The term \(20\log(Ka_{1}a_{2}/b_{1}b_{3})\) is a constant. We observe that the log amplitude is a sum of four basic terms corresponding to a constant, a pole or zero at the origin (\(20\log|j\omega|\)), a first-order pole or zero (\(20\log|1+j\omega/a|\)), and complex-conjugate poles or zeros (\(20\log|1+j\omega b_{2}/b_{3}+(j\omega)^{2}/b_{3}|\)).

We can sketch these four basic terms as functions of \(\omega\) and use them to construct the log-amplitude plot of any desired transfer function. Let us discuss each of the terms.

### 4.9-1 Constant \(Ka_{1}a_{2}/b_{1}b_{3}\)

The log amplitude of the constant \(Ka_{1}a_{2}/b_{1}b_{2}\) term is also a constant, \(20\log|Ka_{1}a_{2}/b_{1}b_{3}|\). The phase contribution from this term is zero for positive value and \(\pi\) for negative value of the constant (complex constants can have different phases).

### 4.9-2 Pole (or Zero) at the Origin

Log Magnitude

A pole at the origin gives rise to the term \(-20\log|j\omega|\), which can be expressed as

\[-20\log|j\omega|=-20\log\omega\]

This function can be plotted as a function of \(\omega\). However, we can effect further simplification by using the logarithmic scale for the variable \(\omega\) itself. Let us define a new variable \(u\) such that

\[u=\log\omega\]

Hence,

\[-20\log\omega=-20u\]

The log-amplitude function \(-20u\) is plotted as a function of \(u\) in Fig. 4.40a. This is a straight line with a slope of \(-20\). It crosses the \(u\) axis at \(u=0\). The \(\omega\)-scale (\(u=\log\omega\)) also appears in Fig. 4.40a. Semilog graphs can be conveniently used for plotting, and we can directly plot \(\omega\) on semilog paper. A ratio of 10 is a _decade_, and a ratio of 2 is known as an _octave_. Furthermore, a decade along the \(\omega\) scale is equivalent to 1 unit along the \(u\) scale. We can also show that a ratio of 2 (an octave) along the \(\omega\) scale equals to 0.3010 (which is \(\log_{10}2\)) along the \(u\) scale.1Note that equal increments in \(u\) are equivalent to equal ratios on the \(\omega\) scale. Thus, 1 unit along the \(u\) scale is the same as one decade along the \(\omega\) scale. This means that the amplitude plot has a slope of \(-20\) dB\(/\)decade or \(-20(0.3010)=-6.02\) dB\(/\)octave (commonly stated as \(-6\) dB/octave). Moreover, the amplitude plot crosses the \(\omega\) axis at \(\omega=1\), since \(u=\log_{10}\omega=0\) when \(\omega=1\).

For the case of a zero at the origin, the log-amplitude term is 20 log \(\omega\). This is a straight line passing through \(\omega=1\) and having a slope of 20 dB\(/\)decade (or 6 dB\(/\)octave). This plot is a mirror image about the \(\omega\) axis of the plot for a pole at the origin and is shown dashed in Fig. 4.40a.

### Phase

The phase function corresponding to the pole at the origin is \(-\angle jo\) [see Eq. (4.46)]. Thus,

\[\angle H(jo)=-\angle jo\omega=-90^{\circ}\]

The phase is constant (\(-90^{\circ}\)) for all values of \(\omega\), as depicted in Fig. 4.40b. For a zero at the origin, the phase is \(\angle jo\omega=90^{\circ}\). This is a mirror image of the phase plot for a pole at the origin and is shown dashed in Fig. 4.40b.

Figure 4.40: **(a)** Amplitude and **(b)** phase responses of a pole or a zero at the origin.

[MISSING_PAGE_FAIL:95]

The actual plot along with the asymptotes is depicted in Fig. 4.41b. In this case, we use a three-line segment asymptotic plot for greater accuracy. The asymptotes are a phase angle of \(0^{\circ}\) for \(\omega\leq a/10\), a phase angle of \(-90^{\circ}\) for \(\omega\geq 10a\), and a straight line with a slope \(-45^{\circ}\)/decade connecting these two asymptotes (from \(\omega=a/10\) to \(10a\)) crossing the \(\omega\) axis at \(\omega=a/10\). It can be seen from Fig. 4.41b that the asymptotes are very close to the curve and the maximum error is \(5.7^{\circ}\). Figure 4.42b plots the error as a function of \(\omega\); the actual plot can be obtained by adding the error to the asymptotic plot.

Figure 4.41: **(a)** Amplitude and **(b)** phase responses of a first-order pole or zero at \(s=-a\).

The phase for a zero at \(-a\) (shown dotted in Fig. 4.41b) is identical to that of the pole at \(-a\) with a sign change, and therefore is the mirror image (about the \(0^{\circ}\) line) of the phase plot for a pole at \(-a\).

### 4.9-4 Second-Order Pole (or Zero)

Let us consider the second-order pole in Eq. (4.44). The denominator term is \(s^{2}+b_{2}s+b_{3}\). We shall introduce the often-used standard form \(s^{2}+2\zeta\omega_{n}s+\omega_{n}^{2}\) instead of \(s^{2}+b_{2}s+b_{3}\). With this form, the log amplitude function for the second-order term in Eq. (4.47) becomes

\[-20\log\left|1+2j\zeta\,\frac{\omega}{\omega_{n}}+\left(\frac{j\omega}{\omega_ {n}}\right)^{2}\right|\]

and the phase function is

\[-\angle\left[1+2j\zeta\,\frac{\omega}{\omega_{n}}+\left(\frac{j\omega}{\omega _{n}}\right)^{2}\right] \tag{4.48}\]

Figure 4.42: Errors in asymptotic approximation of a first-order pole at \(s=-a\).

The Log Magnitude

The log amplitude is given by

\[\log\,\mbox{amplitude}=-20\log\left|1+2j\zeta\left(\frac{\omega}{\omega_{n}} \right)+\left(\frac{j\omega}{\omega_{n}}\right)^{2}\right| \tag{4.49}\]

For \(\omega\ll\omega_{n}\), the log amplitude becomes

\[\log\,\mbox{amplitude}\approx-20\log 1=0\]

For \(\omega\gg\omega_{n}\), the log amplitude is

\[\log\,\mbox{amplitude} \approx-20\log\left|\left(-\frac{\omega}{\omega_{n}}\right)^{2} \right|=-40\log\left(\frac{\omega}{\omega_{n}}\right)\] \[=-40\log\omega-40\log\omega_{n}=-40u-40\log\omega_{n} \tag{4.50}\]

The two asymptotes are zero for \(\omega<\omega_{n}\) and \(-40u-40\log\omega_{n}\) for \(\omega>\omega_{n}\). The second asymptote is a straight line with a slope of \(-40\) dB/decade (or \(-12\) dB/octave) when plotted against the log \(\omega\) scale. It begins at \(\omega=\omega_{n}\) [see Eq. (4.50)]. The asymptotes are depicted in Fig. 4.43a. The exact log amplitude is given by [see Eq. (4.49)]

\[\log\,\mbox{amplitude}=-20\log\left\{\left[1-\left(\frac{\omega}{\omega_{n}} \right)^{2}\right]^{2}+4\zeta^{2}\left(\frac{\omega}{\omega_{n}}\right)^{2} \right\}^{1/2} \tag{4.51}\]

The log amplitude in this case involves a parameter \(\zeta\), resulting in a different plot for each value of \(\zeta\). For complex-conjugate poles,2\(\zeta<1\). Hence, we must sketch a family of curves for a number of values of \(\zeta\) in the range 0 to 1. This is illustrated in Fig. 4.43a. The error between the actual plot and the asymptotes is shown in Fig. 4.44. The actual plot can be obtained by adding the error to the asymptotic plot.

Footnote 2: For \(\zeta\geq 1\), the two poles in the second-order factor are no longer complex but real, and each of these two real poles can be dealt with as a separate first-order factor.

For second-order zeros (complex-conjugate zeros), the plots are mirror images (about the 0 dB line) of the plots depicted in Fig. 4.43a. Note the resonance phenomenon of the complex-conjugate poles. This phenomenon is barely noticeable for \(\zeta>0.707\) but becomes pronounced as \(\zeta\to 0\).

### Phase

The phase function for second-order poles, as apparent in Eq. (4.48), is

\[\angle H(j\omega)=-\tan^{-1}\left[\frac{2\zeta\left(\frac{\omega}{\omega_{n}} \right)}{1-\left(\frac{\omega}{\omega_{n}}\right)^{2}}\right] \tag{4.52}\]

For \(\omega\ll\omega_{n}\),

\[\angle H(j\omega)\approx 0\]For \(\omega\gg\omega_{n}\),

\[\angle H(j\omega)\simeq-180^{\circ}\]

Hence, the phase \(\rightarrow-180^{\circ}\) as \(\omega\rightarrow\infty\). As in the case of amplitude, we also have a family of phase plots for various values of \(\zeta\), as illustrated in Fig. 4.43b. A convenient asymptote for the phase of complex-conjugate poles is a step function that is \(0^{\circ}\) for \(\omega<\omega_{n}\) and \(-180^{\circ}\) for \(\omega>\omega_{n}\).

Figure 4.43: Amplitude and phase response of a second-order pole.

Error plots for such an asymptote are shown in Fig. 4.44 for various values of \(\zeta\). The exact phase is the asymptotic value plus the error.

For complex-conjugate zeros, the amplitude and phase plots are mirror images of those for complex conjugate-poles.

We shall demonstrate the application of these techniques with two examples.

Figure 4.44: Errors in the asymptotic approximation of a second-order pole.

**Example 4.29**: **Bode Plots for Second-Order Transfer Function with Real Roots**

Sketch Bode plots for the transfer function

\[H(s)=\frac{20s(s+100)}{(s+2)(s+10)}\]

Magnitude Plot

First, we write the transfer function in normalized form

\[H(s)=\frac{20\times 100}{2\times 10}\frac{s\biggl{(}1+\frac{s}{100}\biggr{)}}{ \biggl{(}1+\frac{s}{2}\biggr{)}\biggl{(}1+\frac{s}{10}\biggr{)}}=100\frac{s \biggl{(}1+\frac{s}{100}\biggr{)}}{\biggl{(}1+\frac{s}{2}\biggr{)}\biggl{(}1+ \frac{s}{10}\biggr{)}}\]

Here, the constant term is 100; that is, 40 dB (\(20\,\log 100=40\)). This term can be added to the plot by simply relabeling the horizontal axis (from which the asymptotes begin) as the 40 dB line (see Fig. 4.45a). Such a step implies shifting the horizontal axis upward by 40 dB. This is precisely what is desired.

In addition, we have two first-order poles at \(-2\) and \(-10\), one zero at the origin, and one zero at \(-100\).

**Step 1.**: For each of these terms, we draw an asymptotic plot as follows (shown in Fig. 4.45a by dashed lines):

* For the zero at the origin, draw a straight line with a slope of \(20\,\,\mathrm{dB}/\mathrm{decade}\) passing through \(\omega=1\).
* For the pole at \(-2\), draw a straight line with a slope of \(-20\,\,\mathrm{dB}/\mathrm{decade}\) (for \(\omega>2\)) beginning at the corner frequency \(\omega=2\).
* For the pole at \(-10\), draw a straight line with a slope of \(-20\,\,\mathrm{dB}/\mathrm{decade}\) beginning at the corner frequency \(\omega=10\).
* For the zero at \(-100\), draw a straight line with a slope of \(20\,\,\mathrm{dB}/\mathrm{decade}\) beginning at the corner frequency \(\omega=100\).
**Step 2.**: Add all the asymptotes, as depicted in Fig. 4.45a by solid line segments.
**Step 3.**: Apply the following corrections (see Fig. 4.42a):

* The correction at \(\omega=1\) because of the corner frequency at \(\omega=2\) is \(-1\) dB. The correction at \(\omega=1\) because of the corner frequencies at \(\omega=10\) and \(\omega=100\) is quite small (see Fig. 4.42a) and may be ignored. Hence, the net correction at \(\omega=1\) is \(-1\) dB.

* The correction at \(\omega=2\) because of the corner frequency at \(\omega=2\) is \(-3\) dB, and the correction because of the corner frequency at \(\omega=10\) is \(-0.17\) dB. The correction because of the corner frequency \(\omega=100\) can be safely ignored. Hence the net correction at \(\omega=2\) is \(-3.17\) dB.
* The correction at \(\omega=10\) because of the corner frequency at \(\omega=10\) is \(-3\) dB, and the correction because of the corner frequency at \(\omega=2\) is \(-0.17\) dB. The correction because of \(\omega=100\) can be ignored. Hence the net correction at \(\omega=10\) is \(-3.17\) dB.

Figure 4.45: **(a)** Amplitude and **(b)** phase responses of the second-order system.

* The correction at \(\omega=100\) because of the corner frequency at \(\omega=100\) is 3 dB, and the corrections because of the other corner frequencies may be ignored.
* In addition to the corrections at corner frequencies, we may consider corrections at intermediate points for more accurate plots. For instance, the corrections at \(\omega=4\) because of corner frequencies at \(\omega=2\) and \(10\) are \(-1\) and about \(-0.65\), totaling \(-1.65\) dB. In the same way, the corrections at \(\omega=5\) because of corner frequencies at \(\omega=2\) and \(10\) are \(-0.65\) and \(-1\), totaling \(-1.65\) dB. With these corrections, the resulting amplitude plot is illustrated in Fig. 4.45a.

### Phase Plot

We draw the asymptotes corresponding to each of the four factors:

* The zero at the origin causes a \(90^{\circ}\) phase shift.
* The pole at \(s=-2\) has an asymptote with a zero value for \(-\infty<\omega<0.2\) and a slope of \(-45^{\circ}/\)decade beginning at \(\omega=0.2\) and going up to \(\omega=20\). The asymptotic value for \(\omega>20\) is \(-90^{\circ}\).
* The pole at \(s=-10\) has an asymptote with a zero value for \(-\infty<\omega<1\) and a slope of \(-45^{\circ}/\)decade beginning at \(\omega=1\) and going up to \(\omega=100\). The asymptotic value for \(\omega>100\) is \(-90^{\circ}\).
* The zero at \(s=-100\) has an asymptote with a zero value for \(-\infty<\omega<10\) and a slope of \(45^{\circ}/\)decade beginning at \(\omega=10\) and going up to \(\omega=1000\). The asymptotic value for \(\omega>1000\) is \(90^{\circ}\). All the asymptotes are added, as shown in Fig. 4.45b. The appropriate corrections are applied from Fig. 4.42b, and the exact phase plot is depicted in Fig. 4.45b.

**Example 4.30**: **Bode Plots for Second-Order Transfer Function with Complex Poles**

Sketch the amplitude and phase response (Bode plots) for the transfer function

\[H(s)=\frac{10(s+100)}{s^{2}+2s+100}=10\frac{1+\frac{s}{100}}{1+\frac{s}{50}+ \frac{s^{2}}{100}}\]

### Magnitude Plot

Here, the constant term is 10: that is, 20 dB(20 \(\log 10=20\)). To add this term, we simply label the horizontal axis (from which the asymptotes begin) as the 20 dB line, as before (see Fig. 4.46a).

In addition, we have a real zero at \(s=-100\) and a pair of complex conjugate poles. When we express the second-order factor in standard form,

\[s^{2}+2s+100=s^{2}+2\zeta\omega_{n}s+\omega_{n}^{2}\]

Figure 4.46: **(a)** Amplitude and **(b)** phase responses of the second-order system.

we have \[\omega_{n}=10\qquad\text{and}\qquad\zeta=0.1\] Step 1. Draw an asymptote of \(-40\)\(\mathrm{dB/decade}\) (\(-12\)\(\mathrm{dB/octave}\)) starting at \(\omega=10\) for the complex conjugate poles, and draw another asymptote of \(20\)\(\mathrm{dB/decade}\) starting at \(\omega=100\) for the (real) zero. Step 2. Add both asymptotes. Step 3. Apply the correction at \(\omega=100\), where the correction because of the corner frequency \(\omega=100\) is \(3\)\(\mathrm{dB}\). The correction because of the corner frequency \(\omega=10\), as seen from Fig. 4.44a for \(\zeta=0.1\), can be safely ignored. Next, the correction at \(\omega=10\) because of the corner frequency \(\omega=10\) is \(13.90\)\(\mathrm{dB}\) (see Fig. 4.44a for \(\zeta=0.1\)). The correction because of the real zero at \(-100\) can be safely ignored at \(\omega=10\). We may find corrections at a few more points. The resulting plot is illustrated in Fig. 4.46a.

### Phase Plot

The asymptote for the complex conjugate poles is a step function with a jump of \(-180^{\circ}\) at \(\omega=10\). The asymptote for the zero at \(s=-100\) is zero for \(\omega\leq 10\) and is a straight line with a slope of \(45^{\circ}\)/decade, starting at \(\omega=10\) and going to \(\omega=1000\). For \(\omega\geq 1000\), the asymptote is \(90^{\circ}\). The two asymptotes add to give the sawtooth shown in Fig. 4.46b. We now apply the corrections from Figs. 4.42b and 4.44b to obtain the exact plot.

Figure 4.47: MATLAB-generated Bode plots for Ex. 4.30.

### Bode Plots with MATLAB

Bode plots make it relatively simple to hand-draw straight-line approximations to a system's magnitude and frequency responses. To produce exact Bode plots, we turn to MATLAB and its bode command.

>> bode(tf([10 1000],[1 2 100]),'k-'); The resulting MATLAB plots, shown in Fig. 4.47, match the plots shown in Fig. 4.46.

### Comment.

These two examples demonstrate that actual frequency response plots are very close to asymptotic plots, which are so easy to construct. Thus, by mere inspection of \(H(s)\) and its poles and zeros, one can rapidly construct a mental image of the frequency response of a system. This is the principal virtue of Bode plots.

### Poles and Zeros in the Right Half-Plane

In our discussion so far, we have assumed the poles and zeros of the transfer function to be in the left half-plane. What if some of the poles and/or zeros of \(H(s)\) lie in the RHP? If there is a pole in the RHP, the system is unstable. Such systems are useless for any signal-processing application. For this reason, we shall consider only the case of the RHP zero. The term corresponding to RHP zero at \(s=a\) is \((s/a)-1\), and the corresponding frequency response is \((j\omega/a)-1\). The amplitude response is

\[\left|\frac{j\omega}{a}-1\right|=\left(\frac{\omega^{2}}{a^{2}}+1\right)^{1/2}\]

This shows that the amplitude response of an RHP zero at \(s=a\) is identical to that of an LHP zero or \(s=-a\). Therefore, the log amplitude plots remain unchanged whether the zeros are in the LHP or the RHP. However, the phase corresponding to the RHP zero at \(s=a\) is

\[\angle\left(\frac{j\omega}{a}-1\right)=\angle-\left(1-\frac{j\omega}{a}\right) =\pi+\tan^{-1}\left(\frac{-\omega}{a}\right)=\pi-\tan^{-1}\left(\frac{\omega} {a}\right)\]

whereas the phase corresponding to the LHP zero at \(s=-a\) is \(\tan^{-1}(\omega/a)\).

The complex-conjugate zeros in the RHP give rise to a term \(s^{2}-2\zeta\omega_{n}s+\omega_{n}^{2}\), which is identical to the term \(s^{2}+2\zeta\omega_{n}s+\omega_{n}^{2}\) with a sign change in \(\zeta\). Hence, from Eqs. (4.51) and (4.52), it follows that the amplitudes are identical, but the phases are of opposite signs for the two terms.

Systems whose poles and zeros are restricted to the LHP are classified as _minimum phase_ systems. Minimum phase systems are particularly desirable because the system _and its inverse_ are both stable.

### The Transfer Function from the Frequency Response

In the preceding section we were given the transfer function of a system. From a knowledge of the transfer function, we developed techniques for determining the system response to sinusoidal inputs. We can also reverse the procedure to determine the transfer function of a minimum phasesystem from the system's response to sinusoids. This application has significant practical utility. If we are given a system in a black box with only the input and output terminals available, the transfer function has to be determined by experimental measurements at the input and output terminals. The frequency response to sinusoidal inputs is one of the possibilities that is very attractive because the measurements involved are so simple. One needs only to apply a sinusoidal signal at the input and observe the output. We find the amplitude gain \(|H(j\omega)|\) and the output phase shift \(\angle H(j\omega)\) (with respect to the input sinusoid) for various values of \(\omega\) over the entire range from 0 to \(\infty\). This information yields the frequency response plots (Bode plots) when plotted against log \(\omega\). From these plots we determine the appropriate asymptotes by taking advantage of the fact that the slopes of all asymptotes must be multiples of \(\pm 20\) dB\(/\)decade if the transfer function is a rational function (function that is a ratio of two polynomials in \(s\)). From the asymptotes, the corner frequencies are obtained. Corner frequencies determine the poles and zeros of the transfer function. Because of the ambiguity about the location of zeros since LHP and RHP zeros (zeros at \(s=\pm a\)) have identical magnitudes, this procedure works only for minimum phase systems.

## 4.10 Filter Design by Placement of Poles and Zeros of \(H(s)\)

In this section we explore the strong dependence of frequency response on the location of poles and zeros of \(H(s)\). This dependence points to a simple intuitive procedure to filter design.

### 4.10-1 Dependence of Frequency Response on Poles and Zeros of \(H(s)\)

Frequency response of a system is basically the information about the filtering capability of the system. A system transfer function can be expressed as

\[H(s)=\frac{P(s)}{Q(s)}=b_{0}\frac{(s-z_{1})(s-z_{2})\cdot\cdot\cdot(s-z_{N})}{ (s-\lambda_{1})(s-\lambda_{2})\cdot\cdot\cdot(s-\lambda_{N})}\]

where \(z_{1}\), \(z_{2}\), \(\ldots\), \(z_{N}\) are \(\lambda_{1}\), \(\lambda_{2}\), \(\ldots\), \(\lambda_{N}\) are the poles of \(H(s)\). Now the value of the transfer function \(H(s)\) at some frequency \(s=p\) is

\[H(s)|_{s=p}=b_{0}\frac{(p-z_{1})(p-z_{2})\cdot\cdot\cdot(p-z_{N})}{(p-\lambda_ {1})(p-\lambda_{2})\cdot\cdot\cdot(p-\lambda_{N})} \tag{4.53}\]

This equation consists of factors of the form \(p-z_{i}\) and \(p-\lambda_{i}\). The factor \(p-z_{i}\) is a complex number represented by a vector drawn from point \(z\) to the point \(p\) in the complex plane, as illustrated in Fig. 4.48a. The length of this line segment is \(|p-z_{i}|\), the magnitude of \(p-z_{i}\). The angle of this directed line segment (with the horizontal axis) is \(\angle(p-z_{i})\). To compute \(H(s)\) at \(s=p\), we draw line segments from all poles and zeros of \(H(s)\) to the point \(p\), as shown in Fig. 4.48b. The vector connecting a zero \(z_{i}\) to the point \(p\) is \(p-z_{i}\). Let the length of this vector be \(r_{i}\), and let its angle with the horizontal axis be \(\phi_{i}\). Then \(p-z_{i}=r_{i}e^{i\phi_{i}}\). Similarly, the vector connecting a pole \(\lambda_{i}\) to the point \(p\) is \(p-\lambda_{i}=d_{i}e^{i\theta_{i}}\), where \(d_{i}\) and \(\theta_{i}\) are the length and the angle (with the horizontal axis),respectively, of the vector \(p-\lambda_{i}\). Now from Eq. (4.53) it follows that

\[H(s)|_{s=p} =b_{0}\frac{(r_{1}e^{i\phi_{1}})(r_{2}e^{i\phi_{2}})\cdot\cdot\cdot (r_{N}e^{i\phi_{N}})}{(d_{1}e^{i\phi_{1}})(d_{2}e^{i\phi_{2}})\cdot\cdot\cdot(d_ {N}e^{i\phi_{N}})}\] \[=b_{0}\frac{r_{1}r_{2}\cdot\cdot\cdot r_{N}}{d_{1}d_{2}\cdot\cdot \cdot d_{N}}e^{i(\phi_{1}+\phi_{2}+\cdot\cdot\cdot+\phi_{N})-(\theta_{1}+ \theta_{2}+\cdot\cdot\cdot+\theta_{N}))}\]

Therefore

\[|H(s)|_{s=p}=b_{0}\frac{r_{1}r_{2}\cdot\cdot\cdot r_{N}}{d_{1}d_{2}\cdot\cdot \cdot d_{N}}=b_{0}\frac{\text{product of distances of zeros to }p}{\text{product of distances of poles to }p} \tag{4.54}\]

and

\[\angle H(s)|_{s=p} =(\phi_{1}+\phi_{2}+\cdot\cdot\cdot+\phi_{N})-(\theta_{1}+\theta _{2}+\cdot\cdot\cdot+\theta_{N})\] \[=\text{sum of angles of zeros to }p-\text{sum of angles of poles to }p \tag{4.55}\]

Here, we have assumed positive \(b_{0}\). If \(b_{0}\) is negative, there is an additional phase \(\pi\). Using this procedure, we can determine \(H(s)\) for any value of \(s\). To compute the frequency response \(H(j\omega)\), we use \(s=j\omega\) (a point on the imaginary axis), connect all poles and zeros to the point \(j\omega\), and determine \(|H(j\omega)|\) and \(\angle H(j\omega)\) from Eqs. (4.54) and (4.55). We repeat this procedure for all values of \(\omega\) from \(0\) to \(\infty\) to obtain the frequency response.

### Gain Enhancement by a Pole

To understand the effect of poles and zeros on the frequency response, consider a hypothetical case of a single pole \(-\alpha+j\omega_{0}\), as depicted in Fig. 4.49a. To find the amplitude response \(|H(j\omega)|\) for a certain value of \(\omega\), we connect the pole to the point \(j\omega\) (Fig. 4.49a). If the length of this line is \(d\), then \(|H(j\omega)|\) is proportional to \(1/d\),

\[|H(j\omega)|=\frac{K}{d} \tag{4.56}\]

where the exact value of constant \(K\) is not important at this point. As \(\omega\) increases from zero, \(d\) decreases progressively until \(\omega\) reaches the value \(\omega_{0}\). As \(\omega\) increases beyond \(\omega_{0}\), \(d\) increases

Figure 4.48: Vector representations of **(a)** complex numbers and **(b)** factors of \(H(s)\).

progressively. Therefore, according to Eq. (4.56), the amplitude response \(|H(j\omega)|\) increases from \(\omega=0\) until \(\omega=\omega_{0}\), and it decreases continuously as \(\omega\) increases beyond \(\omega_{0}\), as illustrated in Fig. 4.49b. Therefore, a pole at \(-\alpha+j\omega_{0}\) results in a frequency-selective behavior that enhances the gain at the frequency \(\omega_{0}\) (resonance). Moreover, as the pole moves closer to the imaginary axis (as \(\alpha\) is reduced), this enhancement (resonance) becomes more pronounced. This is because \(\alpha\), the distance between the pole and \(j\omega_{0}\) (\(d\) corresponding to \(j\omega_{0}\)), becomes smaller, which increases the gain \(K/d\). In the extreme case, when \(\alpha=0\) (pole on the imaginary axis), the gain at \(\omega_{0}\) goes to infinity. Repeated poles further enhance the frequency-selective effect. To summarize, we can enhance a gain at a frequency \(\omega_{0}\) by placing a pole opposite the point \(j\omega_{0}\). The closer the pole is to \(j\omega_{0}\), the higher is the gain at \(\omega_{0}\), and the gain variation is more rapid (more frequency selective) in the vicinity of frequency \(\omega_{0}\). Note that a pole must be placed in the LHP for stability.

Here we have considered the effect of a single complex pole on the system gain. For a real system, a complex pole \(-\alpha+j\omega_{0}\) must accompany its conjugate \(-\alpha-j\omega_{0}\). We can readily show that the presence of the conjugate pole does not appreciably change the frequency-selective

Figure 4.49: The role of poles and zeros in determining the frequency response of an LTIC system.

behavior in the vicinity of \(\omega_{0}\). This is because the gain in this case is \(K/dd^{\prime}\), where \(d^{\prime}\) is the distance of a point \(j\omega\) from the conjugate pole \(-\alpha-j\omega_{0}\). Because the conjugate pole is far from \(j\omega_{0}\), there is no dramatic change in the length \(d^{\prime}\) as \(\omega\) varies in the vicinity of \(\omega_{0}\). There is a gradual increase in the value of \(d^{\prime}\) as \(\omega\) increases, which leaves the frequency-selective behavior as it was originally, with only minor changes.

### Gain Suppression by a Zero

Using the same argument, we observe that zeros at \(-\alpha\pm j\omega_{0}\) (Fig. 4.49d) will have exactly the opposite effect of suppressing the gain in the vicinity of \(\omega_{0}\), as shown in Fig. 4.49e). A zero on the imaginary axis at \(j\omega_{0}\) will totally suppress the gain (zero gain) at frequency \(\omega_{0}\). Repeated zeros will further enhance the effect. Also, a closely placed pair of a pole and a zero (dipole) tend to cancel out each other's influence on the frequency response. Clearly, a proper placement of poles and zeros can yield a variety of frequency-selective behavior. We can use these observations to design lowpass, highpass, bandpass, and bandstop (or notch) filters.

Phase response can also be computed graphically. In Fig. 4.49a, angles formed by the complex conjugate poles \(-\alpha\pm j\omega_{0}\) at \(\omega=0\) (the origin) are equal and opposite. As \(\omega\) increases from 0 up, the angle \(\theta_{1}\) (due to the pole \(-\alpha+j\omega_{0}\)), which has a negative value at \(\omega=0\), is reduced in magnitude; the angle \(\theta_{2}\) because of the pole \(-\alpha-j\omega_{0}\), which has a positive value at \(\omega=0\), increases in magnitude. As a result, \(\theta_{1}+\theta_{2}\), the sum of the two angles, increases continuously, approaching a value \(\pi\) as \(\omega\to\infty\). The resulting phase response \(\angle H(j\omega)=-(\theta_{1}+\theta_{2})\) is illustrated in Fig. 4.49c. Similar arguments apply to zeros at \(-\alpha\pm j\omega_{0}\). The resulting phase response \(\angle H(j\omega)=(\phi_{1}+\phi_{2})\) is depicted in Fig. 4.49f.

We now focus on simple filters, using the intuitive insights gained in this discussion. The discussion is essentially qualitative.

### Lowpass Filters

A typical lowpass filter has a maximum gain at \(\omega=0\). Because a pole enhances the gain at frequencies in its vicinity, we need to place a pole (or poles) on the real axis opposite the origin (\(j\omega=0\)), as shown in Fig. 4.50a. The transfer function of this system is

\[H(s)=\frac{\omega_{c}}{s+\omega_{c}}\]

We have chosen the numerator of \(H(s)\) to be \(\omega_{c}\) to normalize the dc gain \(H(0)\) to unity. If \(d\) is the distance from the pole \(-\omega_{c}\) to a point \(j\omega\) (Fig. 4.50a), then

\[|H(j\omega)|=\frac{\omega_{c}}{d}\]

with \(H(0)=1\). As \(\omega\) increases, \(d\) increases and \(|H(j\omega)|\) decreases monotonically with \(\omega\), as illustrated in Fig. 4.50d with label \(N=1\). This is clearly a lowpass filter with gain enhanced in the vicinity of \(\omega=0\).

### Wall of Poles

An ideal lowpass filter characteristic (shaded in Fig. 4.50d) has a constant gain of unity up to frequency \(\omega_{c}\). Then the gain drops suddenly to 0 for \(\omega>\omega_{c}\). To achieve the ideal lowpasscharacteristic, we need enhanced gain over the entire frequency band from 0 to \(\omega_{c}\). We know that to enhance a gain at any frequency \(\omega\), we need to place a pole opposite \(\omega\). To achieve an enhanced gain for all frequencies over the band (0 to \(\omega_{c}\)), we need to place a pole opposite every frequency in this band. In other words, we need a _continuous wall of poles_ facing the imaginary axis opposite the frequency band 0 to \(\omega_{c}\) (and from 0 to \(-\omega_{c}\) for conjugate poles), as depicted in Fig. 4.50b. At this point, the optimum shape of this wall is not obvious because our arguments are qualitative and intuitive. Yet, it is certain that to have enhanced gain (constant gain) at every frequency over this range, we need an infinite number of poles on this wall. We can show that for a maximally flat+ response over the frequency range (0 to \(\omega_{c}\)), the wall is a semicircle with an infinite number of poles uniformly distributed along the wall [11]. In practice, we compromise by using a finite number (\(N\)) of poles with less-than-ideal characteristics. Figure 4.50c shows the pole configuration for a fifth-order (\(N=5\)) filter. The amplitude response for various values of \(N\) is illustrated in Fig. 4.50d. As \(N\rightarrow\infty\), the filter response approaches the ideal. This family of filters is known as the _Butterworth_ filters. There are also other families. In _Chebyshev_ filters, the wall shape is a semiellipse rather than a semicircle. The characteristics of a Chebyshev filter are inferior to those of Butterworth over the passband \((0,\omega_{c})\), where the characteristics show a rippling effect

Figure 4.50: Pole-zero configuration and the amplitude response of a lowpass (Butterworth) filter.

instead of the maximally flat response of Butterworth. But in the stopband (\(\omega>\omega_{c}\)), Chebyshev behavior is superior in the sense that Chebyshev filter gain drops faster than that of the Butterworth.

### Bandpass Filters

The shaded characteristic in Fig. 4.51b shows the ideal bandpass filter gain. In the bandpass filter, the gain is enhanced over the entire passband. Our earlier discussion indicates that this can be realized by a wall of poles opposite the imaginary axis in front of the passband centered at \(\omega_{0}\). (There is also a wall of conjugate poles opposite \(-\omega_{0}\).) Ideally, an infinite number of poles is required. In practice, we compromise by using a finite number of poles and accepting less-than-ideal characteristics (Fig. 4.51).

### Notch (Bandstop) Filters

An ideal notch filter amplitude response (shaded in Fig. 4.52b) is a complement of the amplitude response of an ideal bandpass filter. Its gain is zero over a small band centered at some frequency \(\omega_{0}\) and is unity over the remaining frequencies. Realization of such a characteristic requires an infinite number of poles and zeros. Let us consider a practical second-order notch filter to obtain zero gain at a frequency \(\omega=\omega_{0}\). For this purpose, we must have zeros at \(\pm ja\omega_{0}\). The requirement of unity gain at \(\omega=\infty\) requires the number of poles to be equal to the number of zeros (\(M=N\)). This ensures that for very large values of \(\omega\), the product of the distances of poles from \(\omega\) will be equal to the product of the distances of zeros from \(\omega\). Moreover, unity gain at \(\omega=0\) requires a pole and the corresponding zero to be equidistant from the origin. For example, if we use two (complex-conjugate) zeros, we must have two poles; the distance from the origin of the poles and of the zeros should be the same. This requirement can be met by placing the two conjugate poles on the semicircle of radius \(\omega_{0}\), as depicted in Fig. 4.52a. The poles can be anywhere on the semicircle to satisfy the equidistance condition. Let the two conjugate poles be at angles \(\pm\theta\) with respect to the negative real axis. Recall that a pole and a zero in the same vicinity tend to cancel out

Figure 4.51: **(a)** Pole-zero configuration and **(b)** the amplitude response of a bandpass filter.

each other's influences. Therefore, placing poles closer to zeros (selecting \(\theta\) closer to \(\pi/2\)) results in a rapid recovery of the gain from value 0 to 1 as we move away from \(\omega_{0}\) in either direction. Figure 4.52b shows the gain \(|H(j\omega)|\) for three different values of \(\theta\).

**Example 4.31**: **Notch Filter Design**

Design a second-order notch filter to suppress 60 Hz hum in a radio receiver.

We use the poles and zeros in Fig. 4.52a with \(\omega_{0}=120\pi\). The zeros are at \(s=\pm j\omega_{0}\). The two poles are at \(-\omega_{0}\cos\theta\pm j\omega_{0}\sin\theta\). The filter transfer function is (with \(\omega_{0}=120\pi\))

\[H(s) =\frac{(s-j\omega_{0})(s+j\omega_{0})}{(s+\omega_{0}\cos\theta+j \omega_{0}\sin\theta)(s+\omega_{0}\cos\theta-j\omega_{0}\sin\theta)}\] \[=\frac{s^{2}+\omega_{0}^{2}}{s^{2}+(2\omega_{0}\cos\theta)s+ \omega_{0}^{2}}=\frac{s^{2}+142122.3}{s^{2}+(753.98\cos\theta)s+142122.3}\]

and

\[|H(j\omega)|=\frac{-\omega^{2}+142122.3}{\sqrt{(-\omega^{2}+142122.3)^{2}+(75 3.98\omega\cos\theta)^{2}}}\]

The closer the poles are to the zeros (the closer \(\theta\) is to \(\pi/2\)), the faster the gain recovery from 0 to 1 on either side of \(\omega_{0}=120\pi\). Figure 4.52b shows the amplitude response for three different values of \(\theta\). This example is a case of very simple design. To achieve zero gain over a band, we need an infinite number of poles as well as an infinite number of zeros.

Figure 4.52: **(a)** Pole-zero configuration and **(b)** the amplitude response of a bandstop (notch) filter.

[MISSING_PAGE_EMPTY:114]

### 4.10-5 Practical Filters and Their Specifications

For ideal filters, everything is black and white; the gains are either zero or unity over certain bands. As we saw earlier, real life does not permit such a worldview. Things have to be gray or shades of gray. In practice, we can realize a variety of filter characteristics that can only approach ideal characteristics.

An ideal filter has a passband (unity gain) and a stopband (zero gain) with a sudden transition from the passband to the stopband. There is no transition band. For practical (or realizable) filters, on the other hand, the transition from the passband to the stopband (or vice versa) is gradual and takes place over a finite band of frequencies. Moreover, for realizable filters, the gain cannot be zero over a finite band (Paley-Wiener condition). As a result, there can be no true stopband for practical filters. We therefore define a _stopband_ to be a band over which the gain is below some small number \(G_{s}\), as illustrated in Fig. 4.55. Similarly, we define a _passband_ to be a band over which the gain is between \(1\) and some number \(G_{p}\) (\(G_{p}<1\)), as shown in Fig. 4.55. We have selected the passband gain of unity for convenience. It could be any constant. Usually the gains are specified in terms of decibels. This is simply \(20\) times the log (to base \(10\)) of the gain. Thus,

\[\hat{G}(\text{dB})=20\log_{10}G\]

A gain of unity is \(0\) dB and a gain of \(\sqrt{2}\) is \(3.01\) dB, usually approximated by \(3\) dB. Sometimes the specification may be in terms of attenuation, which is the negative of the gain in dB. Thus, a gain of \(1/\sqrt{2}\), that is, \(0.707\), is \(-3\) dB, but is an attenuation of \(3\) dB.

Figure 4.55: Passband, stopband, and transition band in filters of various types.

In a typical design procedure, \(G_{p}\) (_minimum passband gain_) and \(G_{s}\) (_maximum stopband gain_) are specified. Figure 4.55 shows the passband, the stopband, and the transition band for typical lowpass, bandpass, highpass, and bandstop filters. Fortunately, the highpass, bandpass, and bandstop filters can be obtained from a basic lowpass filter by simple frequency transformations. For example, replacing \(s\) with \(\omega_{c}/s\) in the lowpass filter transfer function results in a highpass filter. Similarly, other frequency transformations yield the bandpass and bandstop filters. Hence, it is necessary to develop a design procedure only for a basic lowpass filter. Then, by using appropriate transformations, we can design filters of other types. The design procedures are beyond our scope here and will not be discussed. The interested reader is referred to [1].

### 4.11 The Bilateral Laplace Transform

Situations involving noncausal signals and/or systems cannot be handled by the (unilateral) Laplace transform discussed so far. These cases can be analyzed by the _bilateral_ (or _two-sided_) Laplace transform defined by

\[X(s)=\int_{-\infty}^{\infty}x(t)e^{-st}\,dt\]

and \(x(t)\) can be obtained from \(X(s)\) by the inverse transformation

\[x(t)=\frac{1}{2\pi j}\int_{c-j\infty}^{c+j\infty}X(s)e^{st}\,ds\]

Observe that the unilateral Laplace transform discussed so far is a special case of the bilateral Laplace transform, where the signals are restricted to the causal type. Basically, the two transforms are the same. For this reason we use the same notation for the bilateral Laplace transform.

Earlier we showed that the Laplace transforms of \(e^{-at}u(t)\) and of \(-e^{-at}u(-t)\) are identical. The only difference is in their regions of convergence (ROC). The ROC for the former is \(\mathrm{Re}\,s>-a\); that for the latter is \(\mathrm{Re}\,s<-a\), as illustrated in Fig. 4.1. Clearly, the inverse Laplace transform of \(X(s)\) is not unique unless the ROC is specified. If we restrict all our signals to the causal type, however, this ambiguity does not arise. The inverse transform of \(1/(s+a)\) is \(e^{-at}u(t)\). Thus, in the unilateral Laplace transform, we can ignore the ROC in determining the inverse transform of \(X(s)\).

We now show that any bilateral transform can be expressed in terms of two unilateral transforms. It is, therefore, possible to evaluate bilateral transforms from a table of unilateral transforms.

Consider the function \(x(t)\) appearing in Fig. 4.56a. We separate \(x(t)\) into two components, \(x_{1}(t)\) and \(x_{2}(t)\), representing the positive time (_causal_) component and the negative time (_anticausal_) component of \(x(t)\), respectively (Figs. 4.56b and 4.56c):

\[x_{1}(t)=x(t)u(t)\qquad\text{and}\qquad x_{2}(t)=x(t)u(-t)\]The bilateral Laplace transform of \(x(t)\) is given by

\[X(s) =\int_{-\infty}^{\infty}x(t)e^{-st}\,dt\] \[=\int_{-\infty}^{0^{-}}x_{2}(t)e^{-st}\,dt+\int_{0^{-}}^{\infty}x_{ 1}(t)e^{-st}\,dt\] \[=X_{2}(s)+X_{1}(s) \tag{4.57}\]

Figure 4.56: Expressing a signal as a sum of causal and anticausal components.

where \(X_{1}(s)\) is the Laplace transform of the causal component \(x_{1}(t)\), and \(X_{2}(s)\) is the Laplace transform of the anticausal component \(x_{2}(t)\). Consider \(X_{2}(s)\), given by

\[X_{2}(s)=\int_{-\infty}^{0^{-}}\!x_{2}(t)e^{-st}\,dt=\int_{0^{+}}^{\infty}\!x_{ 2}(-t)e^{st}\,dt\]

Therefore,

\[X_{2}(-s)=\int_{0^{+}}^{\infty}\!x_{2}(-t)e^{-st}\,dt\]

If \(x(t)\) has any impulse or its derivative(s) at the origin, they are included in \(x_{1}(t)\). Consequently, \(x_{2}(t)=0\) at the origin; that is, \(x_{2}(0)=0\). Hence, the lower limit on the integration in the preceding equation can be taken as \(0^{-}\) instead of \(0^{+}\). Therefore,

\[X_{2}(-s)=\int_{0^{-}}^{\infty}\!x_{2}(-t)e^{-st}\,dt\]

Because \(x_{2}(-t)\) is causal (Fig. 4.56d), \(X_{2}(-s)\) can be found from the unilateral transform table. Changing the sign of \(s\) in \(X_{2}(-s)\) yields \(X_{2}(s)\).

To summarize, the bilateral transform \(X(s)\) in Eq. (4.57) can be computed from the unilateral transforms in two steps:

1. Split \(x(t)\) into its causal and anticausal components, \(x_{1}(t)\) and \(x_{2}(t)\), respectively.
2. Since the signals \(x_{1}(t)\) and \(x_{2}(-t)\) are both causal, take the (unilateral) Laplace transform of \(x_{1}(t)\) and add to it the (unilateral) Laplace transform of \(x_{2}(-t)\), with \(s\) replaced by \(-s\). This procedure gives the (bilateral) Laplace transform of \(x(t)\).

Since \(x_{1}(t)\) and \(x_{2}(-t)\) are both causal, \(X_{1}(s)\) and \(X_{2}(-s)\) are both unilateral Laplace transforms. Let \(\sigma_{c1}\) and \(\sigma_{c2}\) be the abscissas of convergence of \(X_{1}(s)\) and \(X_{2}(-s)\), respectively. This statement implies that \(X_{1}(s)\) exists for all \(s\) with \(\mbox{Re}\,s>\sigma_{c1}\), and \(X_{2}(-s)\) exists for all \(s\) with \(\mbox{Re}\,s>\sigma_{c2}\). Therefore, \(X_{2}(s)\) exists for all \(s\) with \(\mbox{Re}\,s<-\sigma_{c2}\).2 Therefore, \(X(s)=X_{1}(s)+X_{2}(s)\) exists for all \(s\) such that

Footnote 2: For instance, if \(x(t)\) exists for all \(t>10\), then \(x(-t)\), its time-inverted form, exists for \(t<-10\).

\[\sigma_{c1}<\mbox{Re}\,s<-\sigma_{c2}\]

The regions of convergence of \(X_{1}(s)\), \(X_{2}(s)\), and \(X(s)\) are shown in Fig. 4.57. Because \(X(s)\) is finite for all values of \(s\) lying in the strip of convergence (\(\sigma_{c1}<\mbox{Re}\,s<-\sigma_{c2}\)), poles of \(X(s)\) must lie outside this strip. The poles of \(X(s)\) arising from the causal component \(x_{1}(t)\) lie to the left of the _strip_ (region) _of convergence_, and those arising from its anticausal component \(x_{2}(t)\) lie to its right (see Fig. 4.57). This fact is of crucial importance in finding the inverse bilateral transform.

This result can be generalized to left-sided and right-sided signals. We define a signal \(x(t)\) as a _right-sided_ signal if \(x(t)=0\) for \(t<T_{1}\) for some finite positive or negative number \(T_{1}\). A causal signal is always a right-sided signal, but the converse is not necessarily true. A signal is said to _left-sided_ if it is zero for \(t>T_{2}\) for some finite, positive, or negative number \(T_{2}\). An anticausal signal is always a left-sided signal, but the converse is not necessarily true. A _two-sided_ signal is of infinite duration on both positive and negative sides of \(t\) and is neither right-sided nor left-sided.

We can show that the conclusions for ROC for causal signals also hold for right-sided signals, and those for anticausal signals hold for left-sided signals. In other words, if \(x(t)\) is causal or right-sided, the poles of \(X(s)\) lie to the left of the ROC, and if \(x(t)\) is anticausal or left-sided, the poles of \(X(s)\) lie to the right of the ROC.

To prove this generalization, we observe that a right-sided signal can be expressed as \(x(t)+x_{f}(t)\), where \(x(t)\) is a causal signal and \(x_{f}(t)\) is some finite-duration signal. The ROC of any finite-duration signal is the entire \(s\)-plane (no finite poles). Hence, the ROC of the right-sided signal \(x(t)+x_{f}(t)\) is the region common to the ROCs of \(x(t)\) and \(x_{f}(t)\), which is same as the ROC for \(x(t)\). This proves the generalization for right-sided signals. We can use a similar argument to generalize the result for left-sided signals. Let us find the bilateral Laplace transform of

\[x(t)=e^{bt}u(-t)+e^{at}u(t) \tag{4.58}\]

We already know the Laplace transform of the causal component

\[e^{at}u(t)\Longleftrightarrow\frac{1}{s-a}\qquad\operatorname{Re}s>a \tag{4.59}\]

For the anticausal component, \(x_{2}(t)=e^{bt}u(-t)\), we have

\[x_{2}(-t)=e^{-bt}u(t)\Longleftrightarrow\frac{1}{s+b}\qquad\operatorname{Re} s>-b\]

so that

\[X_{2}(s)=\frac{1}{-s+b}=\frac{-1}{s-b}\qquad\operatorname{Re}s<b\]

Therefore,

\[e^{bt}u(-t)\Longleftrightarrow\frac{-1}{s-b}\qquad\operatorname{Re}s<b \tag{4.60}\]

[MISSING_PAGE_EMPTY:120]

**Example 4.32**: **Inverse Bilateral Laplace Transform**

Find the inverse bilateral Laplace transform of

\[X(s)=\frac{-3}{(s+2)(s-1)}\]

if the ROC is **(a)**\(-2<\operatorname{Re}s<1\), **(b)**\(\operatorname{Re}s>1\), and **(c)**\(\operatorname{Re}s<-2\).

**(a)**

\[X(s)=\frac{1}{s+2}-\frac{1}{s-1}\]

Now, \(X(s)\) has poles at \(-2\) and \(1\). The strip of convergence is \(-2<\operatorname{Re}s<1\). The pole at \(-2\), being to the left of the strip of convergence, corresponds to a causal signal. The pole at \(1\), being to the right of the strip of convergence, corresponds to an anticausal signal. Equations (4.59) and (4.60) yield

\[x(t)=e^{-2t}u(t)+e^{t}u(-t)\]

**(b)** Both poles lie to the left of the ROC, so both poles correspond to causal signals. Therefore,

\[x(t)=(e^{-2t}-e^{t})u(t)\]

**Figure 4.59**: Three possible inverse transforms of \(-3/((s+2)(s-1))\).

[MISSING_PAGE_FAIL:122]

## Chapter 4 Continuous-time system analysis

### 4.11-2 Using the Bilateral Transform for Linear System Analysis

Since the bilateral Laplace transform can handle noncausal signals, we can analyze noncausal LTIC systems using the bilateral Laplace transform. We have shown that the (zero-state) output \(y(t)\) is given by

\[y(t)=\mathcal{L}^{-1}[X(s)H(s)]\]

This expression is valid only if \(X(s)H(s)\) exists. The ROC of \(X(s)H(s)\) is the region in which both \(X(s)\) and \(H(s)\) exist. In other words, the ROC of \(X(s)H(s)\) is the region common to the regions of convergence of both \(X(s)\) and \(H(s)\). These ideas are clarified in the following examples.

The transfer function \(H(s)\) of the circuit is given by

\[H(s)=\frac{s}{s+1}\qquad\operatorname{Re}s>-1\]

Because \(h(t)\) is a causal function, the ROC of \(H(s)\) is \(\operatorname{Re}s>-1\). Next, the bilateral Laplace transform of \(x(t)\) is given by

\[X(s)=\frac{1}{s-1}-\frac{1}{s-2}=\frac{-1}{(s-1)(s-2)}\qquad 1<\operatorname{ Re}s<2\]

The response \(y(t)\) is the inverse transform of \(X(s)H(s)\):

\[y(t)=\mathcal{L}^{-1}\left[\frac{-s}{(s+1)(s-1)(s-2)}\right]=\mathcal{L}^{-1} \left[\frac{1}{6}\frac{1}{s+1}+\frac{1}{2}\frac{1}{s-1}-\frac{2}{3}\frac{1}{s- 2}\right]\]

The ROC of \(X(s)H(s)\) is that ROC common to both \(X(s)\) and \(H(s)\). This is \(1<\operatorname{Re}s<2\). The poles \(s=\pm 1\) lie to the left of the ROC and, therefore, correspond to causal signals; the pole

Figure 4.60: Response of a circuit to a noncausal input.

\(s=2\) lies to the right of the ROC and thus represents an anticausal signal. Hence,

\[y(t)=\tfrac{1}{6}e^{-t}u(t)+\tfrac{1}{2}e^{t}u(t)+\tfrac{2}{3}e^{2t}u(-t)\]

Figure 4.60c shows \(y(t)\). Note that in this example, if

\[x(t)=e^{-4t}u(t)+e^{-2t}u(-t)\]

then the ROC of \(X(s)\) is \(-4<\operatorname{Re}s<-2\). Here no region of convergence exists for \(X(s)H(s)\). Hence, the response \(y(t)\) goes to infinity.

**Example 4.34**: **Response of a Noncausal System**

Find the response \(y(t)\) of a noncausal system with the transfer function

\[H(s)=\frac{-1}{s-1}\qquad\operatorname{Re}s<1\]

to the input \(x(t)=e^{-2t}u(t)\).

We have

\[X(s)=\frac{1}{s+2}\qquad\operatorname{Re}s>-2\]

and

\[Y(s)=X(s)H(s)=\frac{-1}{(s-1)(s+2)}\]

The ROC of \(X(s)H(s)\) is the region \(-2<\operatorname{Re}s<1\). By partial fraction expansion,

\[Y(s)=\frac{-1/3}{s-1}+\frac{1/3}{s+2}\qquad-2<\operatorname{Re}s<1\]

and

\[y(t)=\tfrac{1}{3}[e^{t}u(-t)+e^{-2t}u(t)]\]

Note that the pole of \(H(s)\) lies in the RHP at \(1\). Yet the system is not unstable. The pole(s) in the RHP may indicate instability or noncausality, depending on its location with respect to the region of convergence of \(H(s)\). For example, if \(H(s)=-1/(s-1)\) with \(\operatorname{Re}s>1\), the system is causal and unstable, with \(h(t)=-e^{t}u(t)\). In contrast, if \(H(s)=-1/(s-1)\) with \(\operatorname{Re}s<1\), the system is noncausal and stable, with \(h(t)=e^{t}u(-t)\).

**Example 4.35**: **System Response to a Noncausal Input**

Find the response \(y(t)\) of a system with the transfer function

\[H(s)=\frac{1}{s+5}\qquad\operatorname{Re}s>-5\]

and the input

\[x(t)=e^{-t}u(t)+e^{-2t}u(-t)\]

The input \(x(t)\) is of the type depicted in Fig. 4.58f, and the region of convergence for \(X(s)\) does not exist. In this case, we must determine separately the system response to each of the two input components, \(x_{1}(t)=e^{-t}u(t)\) and \(x_{2}(t)=e^{-2t}u(-t)\).

\[X_{1}(s)=\frac{1}{s+1}\qquad\operatorname{Re}s>-1\]

\[X_{2}(s)=\frac{-1}{s+2}\qquad\operatorname{Re}s<-2\]

If \(y_{1}(t)\) and \(y_{2}(t)\) are the system responses to \(x_{1}(t)\) and \(x_{2}(t)\), respectively, then

\[Y_{1}(s)=\frac{1}{(s+1)(s+5)}=\frac{1/4}{s+1}-\frac{1/4}{s+5}\qquad \operatorname{Re}s>-1\]

so that

\[y_{1}(t)=\tfrac{1}{4}(e^{-t}-e^{-5t})u(t)\]

and

\[Y_{2}(s)=\frac{-1}{(s+2)(s+5)}=\frac{-1/3}{s+2}+\frac{1/3}{s+5}\qquad-5< \operatorname{Re}s<-2\]

so that

\[y_{2}(t)=\tfrac{1}{3}[e^{-2t}u(-t)+e^{-5t}u(t)]\]

Therefore,

\[y(t)=y_{1}(t)+y_{2}(t)=\tfrac{1}{3}e^{-2t}u(-t)+\left(\tfrac{1}{4}e^{-t}+ \tfrac{1}{12}e^{-5t}\right)u(t)\]

### 4.12 MATLAB: Continuous-Time Filters

Continuous-time filters are essential to many if not most engineering systems, and MATLAB is an excellent assistant for filter design and analysis. Although a comprehensive treatment of continuous-time filter techniques is outside the scope of this book, quality filters can be designed and realized with minimal additional theory.

A simple yet practical example demonstrates basic filtering concepts. Telephone voice signals are often lowpass-filtered to eliminate frequencies above a cutoff of 3 kHz, or \(\omega_{c}=3000(2\pi)\approx 18,850\) rad/s. Filtering maintains satisfactory speech quality and reduces signal bandwidth, thereby increasing the phone company's call capacity. How, then, do we design and realize an acceptable 3 kHz lowpass filter?

### 4.12-1 Frequency Response and Polynomial Evaluation

Magnitude response plots help assess a filter's performance and quality. The magnitude response of an ideal filter is a brick-wall function with unity passband gain and perfect stopband attenuation. For a lowpass filter with cutoff frequency \(\omega_{c}\), the ideal magnitude response is

\[|H_{\text{ideal}}(j\omega)|=\begin{cases}1&|\omega|\leq\omega_{c}\\ 0&|\omega|>\omega_{c}\end{cases}\]

Unfortunately, ideal filters cannot be implemented in practice. Realizable filters require compromises, although good designs will closely approximate the desired brick-wall response.

A realizable LTIC system often has a rational transfer function that is represented in the \(s\)-domain as

\[H(s)=\frac{Y(s)}{X(s)}=\frac{B(s)}{A(s)}=\frac{\sum\limits_{k=0}^{M}b_{k+N-M}s ^{M-k}}{\sum\limits_{k=0}^{N}a_{k}s^{N-k}}\]

Frequency response \(H(j\omega)\) is obtained by letting \(s=j\omega\), where frequency \(\omega\) is in radians per second.

MATLAB is ideally suited to evaluate frequency response functions. Defining a length-(\(N+1\)) coefficient vector \(\mathbf{A}=[a_{0},a_{1},\ldots,a_{N}]\) and a length-(\(M+1\)) coefficient vector \(\mathbf{B}=[b_{N-M},b_{N-M+1},\ldots,\)\(b_{N}]\), program CH4MP1 computes \(H(j\omega)\) for each frequency in the input vector \(\boldsymbol{\omega}\).

function [H] = CH4MP1(B,A,omega); % CH4MP1.m : Chapter 4, MATLAB Program 1 % Function M-file computes frequency response for LTIC system % INPUTS: B = vector of feedforward coefficients % A = vector of feedback coefficients % omega = vector of frequencies [rad/s]. % OUTPUS: H = frequency response H = polyval(B,j*omega)./polyval(A,j*omega); The function polyval efficiently evaluates simple polynomials and makes the program nearly trivial. For example, when A is the vector of coefficients \([a_{0},a_{1},\ldots,a_{N}]\), polyval (A,j*omega) computes

\[\sum\limits_{k=0}^{N}a_{k}(j\omega)^{N-k}\]for each value of the frequency vector omega. It is also possible to compute frequency responses by using the signal-processing toolbox function freqs.

### Design and Evaluation of a Simple _Rc_ Filter

One of the simplest lowpass filters is realized by using an _RC_ circuit, as shown in Fig. 4.61. This one-pole system has transfer function \(H_{RC}(s)=(RCs+1)^{-1}\) and magnitude response \(|H_{RC}(j\omega)|=|(j\omega RC+1)^{-1}|=1/\sqrt{1+(RC\omega)^{2}}\). Independent of component values \(R\) and \(C\), this circuit has many desirable characteristics, such as unity gain at \(\omega=0\) and magnitude response that monotonically decreases to zero as \(\omega\rightarrow\infty\).

Components \(R\) and \(C\) are chosen to set the desired 3 kHz cutoff frequency. For many filter types, the cutoff frequency corresponds to the half-power point, or \(|H_{RC}(j\omega_{c})|=1/\sqrt{2}\). Assign \(C\) a realistic capacitance of 1 nF, then the required resistance is computed by \(R=1/\sqrt{C^{2}\omega_{c}^{2}}=1/\sqrt{(10^{-9})^{2}(2\pi 3000)^{2}}\).

>> omega_c = 2*pi*3000; C = 1e-9; R = 1/sqrt(C^2*omega_c^2)  R = 5.3052e+004 The root of this first-order _RC_ filter is directly related to the cutoff frequency, \(\lambda=-1/RC=-18,850=-\omega_{c}\).

To evaluate the _RC_ filter performance, the magnitude response is plotted over the mostly audible frequency range (\(0\leq\)\(f\leq 20\) kHz).

>> f = linspace(0,20000,200); Hmag_RC = abs(CH4MP1([1],[R*C 1],f*2*pi)); >> plot(f,abs(f*2*pi)<=omega_c,'k-',f,Hmag_RC,'k-'); >> axis([0 20000 -0.05 1.05]); xlabel('f [Hz]'); ylabel('|H(j2\(\backslash\)pi f)|'); >> legend('Ideal','First-order RC','location','best'); The linspace(X1,X2,N) command generates an N-length vector of linearly spaced points between X1 and X2.

As shown in Fig. 4.62, the first-order _RC_ response is indeed lowpass with a half-power cutoff frequency equal to 3 kHz. It rather poorly approximates the desired brick-wall response: the passband is not very flat, and stopband attenuation increases very slowly to less than 20 dB at 20 kHz.

Figure 4.61: An _RC_ filter.

[MISSING_PAGE_EMPTY:129]

#### Butterworth Filters and the Find Command

The pole location of a first-order lowpass filter is necessarily fixed by the cutoff frequency. There is little reason, however, to place all the poles of a 10th-order filter at one location. Better pole placement will improve our filter's magnitude response. One strategy, discussed in Sec. 4.10, is to place a wall of poles opposite the passband frequencies. A semicircular wall of poles leads to the Butterworth family of filters, and a semi-elliptical shape leads to the Chebyshev family of filters. Butterworth filters are considered first.

To begin, notice that a transfer function \(H(s)\) with real coefficients has a squared magnitude response given by \(|H(j\omega)|^{2}=H(j\omega)H^{*}(j\omega)=H(j\omega)H(-j\omega)=H(s)H(-s)|_{s=j \omega}\). Thus, half the poles of \(|H(j\omega)|^{2}\) correspond to the filter \(H(s)\) and the other half correspond to \(H(-s)\). Filters that are both stable and causal require \(H(s)\) to include only left-half-plane poles.

The squared magnitude response of a Butterworth filter is

\[|H_{\rm BW}(j\omega)|^{2}=\frac{1}{1+(j\omega/j\omega_{c})^{2N}}\]

This function has the same appealing characteristics as the first-order \(RC\) filter: a gain that is unity at \(\omega=0\) and monotonically decreases to zero as \(\omega\to\infty\). By construction, the half-power gain

Figure 4.64: Magnitude response \(|H_{\rm cascade}(j2\pi f)|\) of a tenth-order \(RC\) cascade.

occurs at \(\omega_{c}\). Perhaps most importantly, however, the first \(2N-1\) derivatives of \(|H_{\rm BW}(j\omega)|\) with respect to \(\omega\) are zero at \(\omega=0\). Put another way, the passband is constrained to be very flat for low frequencies. For this reason, Butterworth filters are sometimes called maximally flat filters.

As discussed in Sec. B.7, the roots of minus 1 must lie equally spaced on a circle centered at the origin. Thus, the \(2N\) poles of \(|H_{\rm BW}(j\omega)|^{2}\) naturally lie equally spaced on a circle of radius \(\omega_{c}\) centered at the origin. Figure 4.65 displays the 20 poles corresponding to the case \(N=10\) and \(\omega_{c}=3000(2\pi)\) rad/s. An \(N\)th-order Butterworth filter that is both causal and stable uses the \(N\) left-half-plane poles of \(|H_{\rm BW}(j\omega)|^{2}\).

To design a 10th-order Butterworth filter, we first compute the 20 poles of \(|H_{\rm BW}(j\omega)|^{2}\):

>> N=10; poles = roots([(1j*omega_c)^(-2*N),zeros(1,2*N-1),1]); The find command is a powerful and useful function that returns the indices of a vector's nonzero elements. Combined with relational operators, the find command allows us to extract the 10 left-half-plane roots that correspond to the poles of our Butterworth filter.

>> BW_poles = poles(find(real(poles)<0)); To compute the magnitude response, these roots are converted to coefficient vector \(\mathbf{A}\).

>> A = poly(BW_poles); A = A/A(end); Hmag_BW = abs(CH4MP1(B,A,f*2*pi)); >> plot(f,abs(f*2*pi)<omega_c,'k-',f,Hmag_BW,'k--'); >> axis([0 20000 -0.05 1.05]); xlabel('f [Hz]'); ylabel('|H(j2\pi f)|'); >> legend('Ideal','Tenth-order Butterworth','location','best');

The magnitude response plot of the Butterworth filter is shown in Fig. 4.66. The Butterworth response closely approximates the brick-wall function and provides excellent filter characteristics: flat passband, rapid transition to the stopband, and excellent stopband attenuation (\(>\)40 dB at 5 kHz).

### 4.12 MATLAB: Continuous-Time Filters

#### Using Cascaded Second-Order Sections

for Butterworth Filter Realization

For our \(RC\) filters, realization preceded design. For our Butterworth filter, however, design has preceded realization. For our Butterworth filter to be useful, we must be able to implement it.

Since the transfer function \(H_{\mathrm{BW}}(s)\) is known, the differential equation is also known. Therefore, it is possible to try to implement the design by using op-amp integrators, summers, and scalar multipliers. Unfortunately, this approach will not work well. To understand why, consider the denominator coefficients \(a_{0}=1.766\times 10^{-43}\) and \(a_{10}=1\). The smallest coefficient is 43 orders of magnitude smaller than the largest coefficient! It is practically impossible to accurately realize such a broad range in scale values. To understand this, skeptics should try to find realistic resistors such that \(R_{f}/R=1.766\times 10^{-43}\). Additionally, small component variations will cause large changes in actual pole location.

A better approach is to cascade five second-order sections, where each section implements one complex conjugate pair of poles. By pairing poles in complex conjugate pairs, each of the resulting second-order sections has real coefficients. With this approach, the smallest coefficients are only about nine orders of magnitude smaller than the largest coefficients. Furthermore, pole placement is typically less sensitive to component variations for cascaded structures.

The Sallen-Key circuit shown in Fig. 4.67 provides a good way to realize a pair of complex-conjugate poles.1 The transfer function of this circuit is

Footnote 1: A more general version of the Sallen–Key circuit has a resistor \(R_{a}\) from the negative terminal to ground and a resistor \(R_{b}\) between the negative terminal and the output. In Fig. 4.67, \(R_{a}=\infty\) and \(R_{b}=0\).

\[H_{\mathrm{SK}}(s)=\frac{\frac{1}{R_{1}R_{2}C_{1}C_{2}}}{s^{2}+\left(\frac{1} {R_{1}C_{1}}+\frac{1}{R_{2}C_{1}}\right)s+\frac{1}{R_{1}R_{2}C_{1}C_{2}}}= \frac{\omega_{0}^{2}}{s^{2}+\left(\frac{\omega_{0}}{Q}\right)s+\omega_{0}^{2}}\]

Geometrically, \(\omega_{0}\) is the distance from the origin to the poles and \(Q=1/2\cos\psi\), where \(\psi\) is the angle between the negative real axis and the pole. Termed the "quality factor" of a circuit, \(Q\)

Figure 4.66: Magnitude response \(|H_{\mathrm{BW}}(j2\pi f)|\) of a tenth-order Butterworth filter.

provides a measure of the peakedness of the response. High-\(Q\) filters have poles close to the \(\omega\) axis, which boost the magnitude response near those frequencies.

Although many ways exist to determine suitable component values, a simple method is to assign \(R_{1}\) a realistic value and then let \(R_{2}=R_{1}\), \(C_{1}=2Q/\omega_{0}R_{1}\), and \(C_{2}=1/2Q\omega_{0}R_{2}\). Butterworth poles are a distance \(\omega_{c}\) from the origin, so \(\omega_{0}=\omega_{c}\). For our 10th-order Butterworth filter, the angles \(\psi\) are regularly spaced at 9, 27, 45, 63, and 81 degrees. MATLAB program CH4MP2 automates the task of computing component values and magnitude responses for each stage.

 % CH4MP2.m : Chapter 4, MATLAB Program 2  % Script M-file computes Sallen-Key component values and magnitude  % responses for each of the five cascaded second-order filter sections.

 omega_0 = 3000*2*pi; % Filter cut-off frequency  psi = [9 27 45 63 81]*pi/180; % Butterworth pole angles  f = linspace(0,6000,200); % Frequency range for magnitude response calculations  Hmag_SK = zeros(5,200); % Pre-allocate array for magnitude responses  for stage = 1:5,  Q = 1/(2*cos(psi(stage))); % Compute Q for current stage  % Compute and display filter components to the screen:  disp(['Stage ',num2str(stage),... '(Q = ',num2str(Q),...  '): R1 = R2 = ',num2str(56000),...  ', C1 = ',num2str(24Q/(omega_0*56000)),...  ', C2 = ',num2str(1/(2*Q*omega_0*56000))]);  B = omega_0^2; A = [1 omega_0/Q omega_0^2]; % Compute filter coefficients  Hmag_SK(stage,:) = abs(CH4MP1(B,A,2*pi*f)); % Compute magnitude response  end plot(f,Hmag_SK,'k',f,prod(Hmag_SK),'k':')  xlabel('f [Hz]'); ylabel('Magnitude Response') The disp command displays a character string to the screen. Character strings must be enclosed in single quotation marks. The num2str command converts numbers to character strings and facilitates the formatted display of information. The prod command multiplies along the columns of a matrix; it computes the total magnitude response as the product of the magnitude responses of the five stages.

 Executing the program produces the following output:

 >> CH4MP2  Stage 1 (Q = 0.50623): R1 = R2 = 56000, C1 = 9.5916e-10, C2 = 9.3569e-10  Stage 2 (Q = 0.56116): R1 = R2 = 56000, C1 = 1.0632e-09, C2 = 8.441e-10  Stage 3 (Q = 0.70711): R1 = R2 = 56000, C1 = 1.3398e-09, C2 = 6.6988e-10  Stage 4 (Q = 1.1013): R1 = R2 = 56000, C1 = 2.0867e-09, C2 = 4.3009e-10  Stage 5 (Q = 3.1962): R1 = R2 = 56000, C1 = 6.0559e-09, C2 = 1.482e-10

Figure 4.67: Sallen–Key filter stage.

Since all the component values are practical, this filter is possible to implement. Figure 4.68 displays the magnitude responses for all five stages (solid lines). The total response (dotted line) confirms a 10th-order Butterworth response. Stage 5, which has the largest \(Q\) and implements the pair of conjugate poles nearest the \(\omega\) axis, is the most peaked response. Stage 1, which has the smallest \(Q\) and implements the pair of conjugate poles furthest from the \(\omega\) axis, is the least peaked response. In practice, it is best to order high-\(Q\) stages last; this reduces the risk that the high gains will saturate the filter hardware.

### 4.12-4 Chebyshev Filters

Like an order-\(N\) Butterworth lowpass filter (LPF), an order-\(N\) Chebyshev LPF is an all-pole filter that possesses many desirable characteristics. Compared with an equal-order Butterworth filter, the Chebyshev filter achieves better stopband attenuation and reduced transition bandwidth by allowing an adjustable amount of ripple within the passband.

The squared magnitude response of a Chebyshev filter is

\[|H_{\mathrm{C}}(j\omega)|^{2}=\frac{1}{1+\epsilon^{2}C_{N}^{2}(\omega/\omega_{ c})}\]

where \(\epsilon\) controls the passband ripple, \(C_{N}(\omega/\omega_{c})\) is a degree-\(N\) Chebyshev polynomial, and \(\omega_{c}\) is the radian cutoff frequency. Several characteristics of Chebyshev LPFs are noteworthy:

* An order-\(N\) Chebyshev LPF is equi-ripple in the passband (\(|\omega|\leq\omega_{c}\)), has a total of \(N\) maxima and minima over (\(0\leq\omega\leq\omega_{c}\)), and is monotonic decreasing in the stopband (\(|\omega|>\omega_{c}\)).

Figure 4.68: Magnitude responses for Sallen–Key filter stages.

* In the passband, the maximum gain is 1 and the minimum gain is \(1/\sqrt{1+\epsilon^{2}}\). For odd-valued \(N\), \(|H(j0)|=1\). For even-valued \(N\), \(|H_{\rm C}(j0)|=1/\sqrt{1+\epsilon^{2}}\).
* Ripple is controlled by setting \(\epsilon=\sqrt{10^{R/10}-1}\), where \(R\) is the allowable passband ripple expressed in decibels. Reducing \(\epsilon\) adversely affects filter performance (see Prob. 4.12-10).
* Unlike Butterworth filters, the cutoff frequency \(\omega_{c}\) rarely specifies the 3 dB point. For \(\epsilon\neq 1\), \(|H_{\rm C}(j\omega_{c})|^{2}=1/(1+\epsilon^{2})\neq 0.5\). The cutoff frequency \(\omega_{c}\) simply indicates the frequency after which \(|H_{\rm C}(j\omega)|<1/\sqrt{1+\epsilon^{2}}\).

The Chebyshev polynomial \(C_{N}(x)\) is defined as

\[C_{N}(x)=\cos[N\cos^{-1}(x)]=\cosh[N\cosh^{-1}(x)]\]

In this form, it is difficult to verify that \(C_{N}(x)\) is a degree-\(N\) polynomial in \(x\). A recursive form of \(C_{N}(x)\) makes this fact more clear (see Prob. 4.12-13).

\[C_{N}(x)=2xC_{N-1}(x)-C_{N-2}(x)\]

With \(C_{0}(x)=1\) and \(C_{1}(x)=x\), the recursive form shows that any \(C_{N}\) is a linear combination of degree-\(N\) polynomials and is therefore a degree-\(N\) polynomial itself. For \(N\geq 2\), MATLAB program CH4MP3 generates the \((N+1)\) coefficients of Chebyshev polynomial \(C_{N}(x)\).

function [C_N] = CH4MP3(N); % CH4MP3.m : Chapter 4, MATLAB Program 3 % Function M-file computes Chebyshev polynomial coefficients % using the recursion relation C_N(x) = 2xC_{N-1}(x) - C_{N-2}(x) % INPUTS: N = degree of Chebyshev polynomial % OUTPUTS: C_N = vector of Chebyshev polynomial coefficients

C_Nm2 = 1; C_Nm1 = [1 0]; % Initial polynomial coefficients: for t = 2:N;  C_N = 2*conv([1 0],C_Nm1)-[zeros(1,length(C_Nm1)-length(C_Nm2)+1),C_Nm2];  C_Nm2 = C_Nm1; C_Nm1 = C_N; end As examples, consider \(C_{2}(x)=2xC_{1}(x)-C_{0}(x)=2x(x)-1=2x^{2}-1\) and \(C_{3}(x)=2xC_{2}(x)-C_{1}(x)=2x(2x^{2}-1)-x=4x^{3}-3x\). CH4MP3 easily confirms these cases.

>> CH4MP3(2)  ans = 2 0 -1 >> CH4MP3(3)  ans = 4 0 -3 0 Since \(C_{N}(\omega/\omega_{c})\) is a degree-\(N\) polynomial, \(|H_{\rm C}(j\omega)|^{2}\) is an all-pole rational function with \(2N\) finite poles. Similar to the Butterworth case, the \(N\) poles specifying a causal and stable Chebyshev filter can be found by selecting the \(N\) left-half-plane roots of \(1+\epsilon^{2}C_{N}^{2}[s/(j\omega_{c})]\).

Root locations and dc gain are sufficient to specify a Chebyshev filter for a given \(N\) and \(\epsilon\). To demonstrate, consider the design of an order-8 Chebyshev filter with cutoff frequency \(f_{c}=1\) kHz and allowable passband ripple \(R=1\) dB. First, filter parameters are specified.

>> omega_c = 2*pi*1000; R = 1; N = 8; >> epsilon = sqrt(10^(R/10)-1); The coefficients of \(C_{N}[s/(j\omega_{c})]\) are obtained with the help of CH4MP3, and then the coefficients of \([1+\epsilon^{2}C_{N}^{2}(s/(j\omega_{c}))]\) are computed by using convolution to perform polynomial multiplication.

>> CN = CH4MP3(N).*((1/(1j*omega_c)).^[N:-1:0]); >> CP = epsilon^2*conv(CN,CN); CP(end) = CP(end)+1; Next, the polynomial roots are found, and the left-half-plane poles are retained and plotted.

>> poles = roots(CP); i = find(real(poles)<0); C_poles = poles(i); >> plot(real(C_poles),imag(C_poles),'kx'); axis equal; >> axis(omega_c*[-1.1 1.1 -1.1 1.1]); >> xlabel('Real'); ylabel('Imaginary'); As shown in Fig. 4.69, the roots of a Chebyshev filter lie on an ellipse+ (see Prob. 4.12-14).

Footnote †: E. A. Guillemin demonstrates a wonderful relationship between the Chebyshev ellipse and the Butterworth circle in his book _Synthesis of Passive Networks_ (Wiley, New York, 1957).

To compute the filter's magnitude response, the poles are expanded into a polynomial, the dc gain is set based on the even value of \(N\), and CH4MP1 is used.

>> A = poly(C_poles); B = A(end)/sqrt(1+epsilon^2); >> omega = linspace(0,2*pi*2000,2001); H_C = CH4MP1(B,A,omega); >> plot(omega/2/pi,abs(H_C),'k'); axis([0 2000 0 1.1]); >> xlabel('f [Hz]'); ylabel('|H_C(j2\pi f)|');As seen in Fig. 4.70, the magnitude response exhibits correct Chebyshev filter characteristics: passband ripples are equal in height and never exceed \(R=1\) dB; there are a total of \(N=8\) maxima and minima in the passband; and the gain rapidly and monotonically decreases after the cutoff frequency of \(f_{c}=1\) kHz.

For higher-order filters, polynomial rooting may not provide reliable results. Fortunately, Chebyshev roots can also be determined analytically. For

\[\phi_{k}=\frac{2k+1}{2N}\pi\qquad\text{and}\qquad\xi=\frac{1}{N}\sinh^{-1} \left(\frac{1}{\epsilon}\right)\]

the Chebyshev poles are

\[p_{k}=\omega_{c}\sinh\left(\xi\right)\sin\left(\phi_{k}\right)+j\omega_{c} \cosh\left(\xi\right)\cos\left(\phi_{k}\right)\]

Continuing the same example, the poles are recomputed and again plotted. The result is identical to Fig. 4.69.

>> k = [1:N]; xi = 1/N*asinh(1/epsilon); phi = (k*2-1)/(2*N)*pi; >> C_poles = omega_c*(-sinh(xi)*sin(phi)+1j*cosh(xi)*cos(phi)); >> plot(real(C_poles),imag(C_poles),'kx'); axis equal; >> axis(omega_c*[-1.1 1.1 -1.1 1.1]); >> xlabel('Real'); ylabel('Imaginary');

As in the case of high-order Butterworth filters, a cascade of second-order filter sections facilitates practical implementation of Chebyshev filters. Problems 4.12-5 and 4.12-8 use second-order Sallen-Key circuit stages to investigate such implementations.

### 4.13 Summary

This chapter discusses analysis of LTIC (linear, time-invariant, continuous-time) systems by the Laplace transform, which transforms integro-differential equations of such systems into algebraic

Figure 4.70: Magnitude responses for an order-8 Chebyshev LPF with \(f_{c}=1\) kHz and \(R=1\) dB.

equations. Therefore solving these integro-differential equations reduces to solving algebraic equations. The Laplace transform method cannot be used for time-varying-parameter systems or for nonlinear systems in general.

The transfer function \(H(s)\) of an LTIC system is the Laplace transform of its impulse response. It may also be defined as a ratio of the Laplace transform of the output to the Laplace transform of the input when all initial conditions are zero (system in zero state). If \(X(s)\) is the Laplace transform of the input \(x(t)\) and \(Y(s)\) is the Laplace transform of the corresponding output \(y(t)\) (when all initial conditions are zero), then \(Y(s)=X(s)H(s)\). For an LTIC system described by an \(N\)th-order differential equation \(Q(D)y(t)=P(D)x(t)\), the transfer function \(H(s)=P(s)/Q(s)\). Like the impulse response \(h(t)\), the transfer function \(H(s)\) is also an external description of the system.

Electrical circuit analysis can also be carried out by using a transformed circuit method, in which all signals (voltages and currents) are represented by their Laplace transforms, all elements by their impedances (or admittances), and initial conditions by their equivalent sources (initial condition generators). In this method, a network can be analyzed as if it were a resistive circuit.

Large systems can be depicted by suitably interconnected subsystems represented by blocks. Each subsystem, being a smaller system, can be readily analyzed and represented by its input-output relationship, such as its transfer function. Analysis of large systems can be carried out with the knowledge of input-output relationships of its subsystems and the nature of interconnection of various subsystems.

LTIC systems can be realized by scalar multipliers, adders, and integrators. A given transfer function can be synthesized in many different ways, such as canonic, cascade, and parallel. Moreover, every realization has a transpose, which also has the same transfer function. In practice, all the building blocks (scalar multipliers, adders, and integrators) can be obtained from operational amplifiers.

The system response to an everlasting exponential \(e^{st}\) is also an everlasting exponential \(H(s)e^{st}\). Consequently, the system response to an everlasting exponential \(e^{int}\) is \(H(j\omega)\,e^{int}\). Hence, \(H(j\omega)\) is the frequency response of the system. For a sinusoidal input of unit amplitude and having frequency \(\omega\), the system response is also a sinusoid of the same frequency (\(\omega\)) with amplitude \(|H(j\omega)|\), and its phase is shifted by \(\angle H(j\omega)\) with respect to the input sinusoid. For this reason \(|H(j\omega)|\) is called the amplitude response (gain) and \(\angle H(j\omega)\) is called the phase response of the system. Amplitude and phase response of a system indicate the filtering characteristics of the system. The general nature of the filtering characteristics of a system can be quickly determined from a knowledge of the location of poles and zeros of the system transfer function.

Most of the input signals and practical systems are causal. Consequently we are required most of the time to deal with causal signals. When all signals must be causal, the Laplace transform analysis is greatly simplified; the region of convergence of a signal becomes irrelevant to the analysis process. This special case of the Laplace transform (which is restricted to causal signals) is called the unilateral Laplace transform. Much of the chapter deals with this variety of Laplace transform. Section 4.11 discusses the general Laplace transform (the bilateral Laplace transform), which can handle causal and noncausal signals and systems. In the bilateral transform, the inverse transform of \(X(s)\) is not unique but depends on the region of convergence of \(X(s)\). Thus, the region of convergence plays a very crucial role in the bilateral Laplace transform.