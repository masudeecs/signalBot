{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34e74406-0eed-4260-bdc7-23805eb04d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "919a68b5-bea2-40c8-aa26-8b2bf7fa4367",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, UnstructuredMarkdownLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import MarkdownTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cb05564-0f45-458b-bf68-b5416fbe11d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]Need to load profiles.\n",
      "Need to load profiles.\n",
      "Need to load profiles.\n",
      "Need to load profiles.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:35<00:00,  3.53s/it]\n"
     ]
    }
   ],
   "source": [
    "text_loader_kwargs={'autodetect_encoding': True}\n",
    "loader = DirectoryLoader('../mmd', glob=\"*.mmd\", loader_cls=UnstructuredMarkdownLoader, loader_kwargs=text_loader_kwargs, use_multithreading=True, show_progress=True)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90c502b7-1b04-44dd-9ff6-c22c62c2d610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split using Markdown rules\n",
    "markdown_splitter = MarkdownTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "split_docs = markdown_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efb84b18-e475-458c-9ae5-f26a945eba8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Chapter Time-Domain Analysis\\n\\nof Continuous-Time Systems\\n\\nIn this book we consider two methods of analysis of linear time-invariant (LTI) systems: the time-domain method and the frequency-domain method. In this chapter we discuss the time-domain analysis of linear, time-invariant, continuous-time (LTIC) systems.\\n\\n11 Introduction', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='For the purpose of analysis, we shall consider linear differential systems. This is the class of LTIC systems introduced in Ch. 1, for which the input (x(t)) and the output (y(t)) are related by linear differential equations of the form\\n\\n[\\\\frac{d^{N}y(t)}{dt^{N}}+a_{1}\\\\frac{d^{N-1}y(t)}{dt^{N-1}}+\\\\cdot \\\\cdot\\\\cdot+a_{N-1}\\\\frac{dy(t)}{dt}+a_{N}y(t)] [=b_{N-M}\\\\frac{d^{M}x(t)}{dt^{M}}+b_{N-M+1}\\\\frac{d^{M-1}x(t)}{dt^{M- 1}}+\\\\cdot\\\\cdot\\\\cdot+b_{N-1}\\\\frac{dx(t)}{dt}+b_{N}x(t) \\\\tag{1}]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='where all the coefficients (a_{i}) and (b_{i}) are constants. Using operator notation (D) to represent (d/dt), we can express this equation as\\n\\n[(D^{N}+a_{1}D^{N-1}+\\\\cdot\\\\cdot\\\\cdot+a_{N-1}D+a_{N})y(t)] [=(b_{N-M}D^{M}+b_{N-M+1}D^{M-1}+\\\\cdot\\\\cdot\\\\cdot+b_{N-1}D+b_{N})\\\\,x(t)]\\n\\nor\\n\\n[Q(D)y(t)=P(D)x(t) \\\\tag{2}]\\n\\nwhere the polynomials (Q(D)) and (P(D)) are\\n\\n[Q(D) =D^{N}+a_{1}D^{N-1}+\\\\cdot\\\\cdot\\\\cdot+a_{N-1}D+a_{N}] [P(D) =b_{N-M}D^{M}+b_{N-M+1}D^{M-1}+\\\\cdot\\\\cdot\\\\cdot+b_{N-1}D+b_{N}]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Theoretically the powers (M) and (N) in the foregoing equations can take on any value. However, practical considerations make (M>N) undesirable for two reasons. In Sec. 11-11, we shall show thatan LTIC system specified by Eq. (2.1) acts as an ((M-N))th-order differentiator. A differentiator represents an unstable system because a bounded input like the step input results in an unbounded output, (\\\\delta(t)). Second, noise is enhanced by a differentiator. Noise is a wideband signal containing', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='components of all frequencies from 0 to a very high frequency approaching (\\\\infty).2 Hence, noise contains a significant amount of rapidly varying components. We know that the derivative of any rapidly varying signal is high. Therefore, any system specified by Eq. (2.1) in which (M>N) will magnify the high-frequency components of noise through differentiation. It is entirely possible for noise to be magnified so much that it swamps the desired system output even if the noise signal at the', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"system's input is tolerably small. Hence, practical systems generally use (M\\\\leq N). For the rest of this text we assume implicitly that (M\\\\leq N). For the sake of generality, we shall assume (M=N) in Eq. (2.1).\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Footnote 2: Notice is any undesirable signal, natural or manufactured, that interferes with the desired signals in the system. Some of the sources of noise are the electromagnetic radiation from stars, the random motion of electrons in system components, interference from nearby radio and television stations, transients produced by automobile ignition systems, and fluorescent lighting.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='In Ch. 1, we demonstrated that a system described by Eq. (2.2) is linear. Therefore, its response can be expressed as the sum of two components: the zero-input response and the zero-state response (decomposition property).3 Therefore,\\n\\nFootnote 3: We can verify readily that the system described by Eq. (2.2) has the decomposition property. If (y_{0}(t)) is the zero-input response, then, by definition,\\n\\n[Q(D)y_{0}(t)=0]\\n\\nIf (y(t)) is the zero-state response, then (y(t)) is the solution of', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[Q(D)y(t)=P(D)x(t)]\\n\\nsubject to zero initial conditions (zero-state). Adding these two equations, we have\\n\\n[Q(D)[y_{0}(t)+y(t)]=P(D)x(t)]\\n\\nClearly, (y_{0}(t)+y(t)) is the general solution of Eq. (2.2).\\nor\\n\\n[(D^{N}+a_{1}D^{N-1}+\\\\cdot\\\\cdot\\\\cdot+a_{N-1}D+a_{N})y_{0}(t)=0 \\\\tag{2.3}]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='A solution to this equation can be obtained systematically [1]. However, we will take a shortcut by using heuristic reasoning. Equation (2.3) shows that a linear combination of (y_{0}(t)) and its (N) successive derivatives is zero, not at some values of (t), but for all (t). Such a result is possible if and only if (y{0}(t))_ and all its (N) successive derivatives are of the same form. Otherwise their sum can never add to zero for all values of (t). We know that only an exponential function', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='(e^{\\\\lambda t}) has this property. So let us assume that', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[y_{0}(t)=ce^{\\\\lambda t}]\\n\\nis a solution to Eq. (2.3). Then\\n\\n[Dy_{0}(t) =\\\\frac{dy_{0}(t)}{dt}=c\\\\lambda e^{\\\\lambda t}] [D^{2}y_{0}(t) =\\\\frac{d^{2}y_{0}(t)}{dt^{2}}=c\\\\lambda^{2}e^{\\\\lambda t}] [\\\\vdots] [D^{N}y_{0}(t) =\\\\frac{d^{N}y_{0}(t)}{dt^{N}}=c\\\\lambda^{N}e^{\\\\lambda t}]\\n\\nSubstituting these results in Eq. (2.3), we obtain\\n\\n[c(\\\\lambda^{N}+a_{1}\\\\lambda^{N-1}+\\\\cdot\\\\cdot\\\\cdot+a_{N-1}\\\\lambda+a_{N})e^{ \\\\lambda t}=0]\\n\\nFor a nontrivial solution of this equation,', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[\\\\lambda^{N}+a_{1}\\\\lambda^{N-1}+\\\\cdot\\\\cdot\\\\cdot+a_{N-1}\\\\lambda+a_{N}=0 \\\\tag{2.4}]\\n\\nThis result means that (ce^{\\\\lambda t}) is indeed a solution of Eq. (2.3), provided (\\\\lambda) satisfies Eq. (2.4). Note that the polynomial in Eq. (2.4) is identical to the polynomial (Q(D)) in Eq. (2.3), with (\\\\lambda) replacing (D). Therefore, Eq. (2.4) can be expressed as\\n\\n[Q(\\\\lambda)=0]\\n\\nExpressing (Q(\\\\lambda)) in factorized form, we obtain', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[Q(\\\\lambda)=(\\\\lambda-\\\\lambda_{1})(\\\\lambda-\\\\lambda_{2})\\\\cdot\\\\cdot\\\\cdot(\\\\lambda- \\\\lambda_{N})=0 \\\\tag{2.5}]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Clearly, (\\\\lambda) has (N) solutions: (\\\\lambda_{1}), (\\\\lambda_{2}), (\\\\ldots), (\\\\lambda_{N}), assuming that all (\\\\lambda_{i}) are distinct. Consequently, Eq. (2.3) has (N) possible solutions: (c_{1}e^{\\\\lambda_{1}t}), (c_{2}e^{\\\\lambda_{2}t}), (\\\\ldots), (c_{N}e^{\\\\lambda_{N}t}), with (c_{1}), (c_{2}),(\\\\ldots),(c_{N}) as arbitrary constants. Wecan readily show that a general solution is given by the sum of these (N) solutions+ so that', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Footnote †: ({}^{\\\\ddagger})Eigenvalue is German for “characteristic value.”\\n\\n[y_{0}(t)=c_{1}e^{\\\\lambda_{1}t}+c_{2}e^{\\\\lambda_{2}t}+\\\\cdot\\\\cdot\\\\cdot+c_{N}e^{ \\\\lambda_{N}t} \\\\tag{6}]\\n\\nwhere (c_{1}), (c_{2}), (\\\\ldots), (c_{N}) are arbitrary constants determined by (N) constraints (the auxiliary conditions) on the solution.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Observe that the polynomial (Q(\\\\lambda)), which is characteristic of the system, has nothing to do with the input. For this reason the polynomial (Q(\\\\lambda)) is called the characteristic polynomial of the system. The equation\\n\\n[Q(\\\\lambda)=0]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='is called the characteristic equation of the system. Equation (5) clearly indicates that (\\\\lambda_{1}), (\\\\lambda_{2}), (\\\\cdot\\\\cdot\\\\cdot), (\\\\lambda_{N}) are the roots of the characteristic equation; consequently, they are called the characteristic roots of the system. The terms characteristic values, eigenvalues, and natural frequencies are also used for characteristic roots.+ The exponentials (e^{\\\\lambda_{i}t}) ((i=1,2,\\\\ldots,n)) in the zero-input response are the characteristic modes (also', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='known as natural modes or simply as modes) of the system. There is a characteristic mode for each characteristic root of the system, and the zero-input response is a linear combination of the characteristic modes of the system.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Footnote †: ({}^{\\\\ddagger})Eigenvalue is German for “characteristic value.”', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"An LTIC system's characteristic modes comprise its single most important attribute. Characteristic modes not only determine the zero-input response but also play an important role in determining the zero-state response. In other words, the entire behavior of a system is dictated primarily by its characteristic modes. In the rest of this chapter we shall see the pervasive presence of characteristic modes in every aspect of system behavior.\\n\\nRepeated Roots\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='The solution of Eq. (3) as given in Eq. (6) assumes that the (N) characteristic roots (\\\\lambda_{1}), (\\\\lambda_{2}), (\\\\ldots), (\\\\lambda_{N}) are distinct. If there are repeated roots (same root occurring more than once), the form of the solution is modified slightly. By direct substitution we can show that the solution of the equation\\n\\n[(D-\\\\lambda)^{2}y_{0}(t)=0]\\n\\nis given by', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[y_{0}(t)=(c_{1}+c_{2}t)e^{\\\\lambda_{1}t}]In this case the root (\\\\lambda) repeats twice. Observe that the characteristic modes in this case are (e^{\\\\lambda t}) and (te^{\\\\lambda t}). Continuing this pattern, we can show that for the differential equation\\n\\n[(D-\\\\lambda)^{r}y_{0}(t)=0]\\n\\nthe characteristic modes are (e^{\\\\lambda t}), (te^{\\\\lambda t}), (t^{2}e^{\\\\lambda t}), (\\\\cdot\\\\cdot\\\\cdot), (t^{r-1}e^{\\\\lambda t}), and that the solution is', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[y_{0}(t)=(c_{1}+c_{2}t+\\\\cdot\\\\cdot\\\\cdot+c_{r}t^{r-1})e^{\\\\lambda t}]\\n\\nConsequently, for a system with the characteristic polynomial\\n\\n[Q(\\\\lambda)=(\\\\lambda-\\\\lambda_{1})^{r}(\\\\lambda-\\\\lambda_{r+1})\\\\cdot\\\\cdot\\\\cdot( \\\\lambda-\\\\lambda_{N})]\\n\\nthe characteristic modes are (e^{\\\\lambda_{1}t}), (te^{\\\\lambda_{1}t}), (\\\\cdot\\\\cdot\\\\cdot), (t^{r-1}e^{\\\\lambda_{1}t}), (e^{\\\\lambda_{r+1}t}), (\\\\cdot\\\\cdot\\\\cdot), (e^{\\\\lambda_{N}t}) and the solution is', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[y_{0}(t)=(c_{1}+c_{2}t+\\\\cdot\\\\cdot\\\\cdot+c_{r}t^{r-1})e^{\\\\lambda_{1}t}+c_{r+1}e ^{\\\\lambda_{r+1}t}+\\\\cdot\\\\cdot\\\\cdot+c_{N}e^{\\\\lambda_{N}t}]\\n\\n2.4 Complex Roots\\n\\nThe procedure for handling complex roots is the same as that for real roots. For complex roots, the usual procedure leads to complex characteristic modes and the complex form of solution. However, it is possible to avoid the complex form altogether by selecting a real-form of solution, as described next.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='For a real system, complex roots must occur in pairs of conjugates if the coefficients of the characteristic polynomial (Q(\\\\lambda)) are to be real. Therefore, if (\\\\alpha+j\\\\beta) is a characteristic root, (\\\\alpha-j\\\\beta) must also be a characteristic root. The zero-input response corresponding to this pair of complex conjugate roots is\\n\\n[y_{0}(t)=c_{1}e^{(\\\\alpha+j\\\\beta)t}+c_{2}e^{(\\\\alpha-j\\\\beta)t} \\\\tag{2.7}]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='For a real system, the response (y_{0}(t)) must also be real. This is possible only if (c_{1}) and (c_{2}) are conjugates. Let\\n\\n[c_{1}=\\\\frac{c}{2}e^{j\\\\theta}\\\\qquad\\\\text{and}\\\\qquad c_{2}=\\\\frac{c}{2}e^{-j\\\\theta}]\\n\\nThis yields\\n\\n[y_{0}(t) =\\\\frac{c}{2}e^{j\\\\theta}e^{(\\\\alpha+j\\\\beta)t}+\\\\frac{c}{2}e^{-j \\\\theta}e^{(\\\\alpha-j\\\\beta)t}] [=\\\\frac{c}{2}e^{at}\\\\big{[}e^{j(\\\\beta t+\\\\theta)}+e^{-j(\\\\beta t+ \\\\theta)}\\\\big{]}] [=ce^{at}\\\\cos{(\\\\beta t+\\\\theta)} \\\\tag{2.8}]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Therefore, the zero-input response corresponding to complex conjugate roots (\\\\alpha\\\\pm j\\\\beta) can be expressed in a complex form [Eq. (2.7)] or a real form [Eq. (2.8)].\\n\\n2.1 Finding the Zero-Input Response\\n\\nFind (y_{0}(t)), the zero-input response of the response for an LTIC system described by\\n\\nthe simple-root system ((D^{2}+3D+2)y(t)=Dx(t)) with initial conditions (y_{0}(0)=0) and (\\\\dot{y}_{0}(0)=-5).', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='the repeated-root system ((D^{2}+6D+9)y(t)=(3D+5)x(t)) with initial conditions (y_{0}(0)=3) and (\\\\dot{y}_{0}(0)=-7).\\n\\nthe complex-root system ((D^{2}+4D+40)y(t)=(D+2)x(t)) with initial conditions (y_{0}(0)=2) and (\\\\dot{y}_{0}(0)=16.78).', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='(a) Note that (y_{0}(t)), being the zero-input response ((x(t)=0)), is the solution of ((D^{2}+3D+2)y_{0}(t)=0). The characteristic polynomial of the system is (\\\\lambda^{2}+3\\\\lambda+2). The characteristic equation of the system is therefore (\\\\lambda^{2}+3\\\\lambda+2=(\\\\lambda+1)(\\\\lambda+2)=0). The characteristic roots of the system are (\\\\lambda_{1}=-1) and (\\\\lambda_{2}=-2), and the characteristic modes of the system are (e^{-t}) and (e^{-2t}). Consequently, the zero-input response is', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[y_{0}(t)=c_{1}e^{-t}+c_{2}e^{-2t}]\\n\\nDifferentiating this expression, we obtain\\n\\n[\\\\dot{y}{0}(t)=-c{1}e^{-t}-2c_{2}e^{-2t}]\\n\\nTo determine the constants (c_{1}) and (c_{2}), we set (t=0) in the equations for (y_{0}(t)) and (\\\\dot{y}{0}(t)) and substitute the initial conditions (y{0}(0)=0) and (\\\\dot{y}_{0}(0)=-5), yielding\\n\\n[0 =c_{1}+c_{2}] [-5 =-c_{1}-2c_{2}]\\n\\nSolving these two simultaneous equations in two unknowns for (c_{1}) and (c_{2}) yields\\n\\n[c_{1}=-5\\\\qquad\\\\mbox{and}\\\\qquad c_{2}=5]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Therefore,\\n\\n[y_{0}(t)=-5e^{-t}+5e^{-2t} \\\\tag{2.9}]\\n\\nThis is the zero-input response of (y(t)). Because (y_{0}(t)) is present at (t=0^{-}), we are justified in assuming that it exists for (t\\\\geq 0).1\\n\\nFootnote 1: (y_{0}(t)) may be present even before (t=0^{-}). However, we can be sure of its presence only from (t=0^{-}) onward.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='(b) The characteristic polynomial is (\\\\lambda^{2}+6\\\\lambda+9=(\\\\lambda+3)^{2}), and its characteristic roots are (\\\\lambda_{1}=-3), (\\\\lambda_{2}=-3) (repeated roots). Consequently, the characteristic modes of the system are (e^{-3t}) and (te^{-3t}). The zero-input response, being a linear combination of the characteristic modes, is given by', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[y_{0}(t)=(c_{1}+c_{2}t)e^{-3t}]We can find the arbitrary constants (c_{1}) and (c_{2}) from the initial conditions (y_{0}(0)=3) and (\\\\dot{y}{0}(0)=-7) following the procedure in part (a). The reader can show that (c{1}=3) and (c_{2}=2). Hence,\\n\\n[y_{0}(t)=(3+2t)e^{-3t}\\\\qquad t\\\\geq 0]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='(c) The characteristic polynomial is (\\\\lambda^{2}+4\\\\lambda+40=(\\\\lambda+2-j6)(\\\\lambda+2+j6)). The characteristic roots are (-2\\\\pm j6).2 The solution can be written either in the complex form [Eq. (2.7)] or in the real form [Eq. (2.8)]. The complex form is (y_{0}(t)=c_{1}e^{\\\\dot{\\\\lambda}{1}t}+c{2}e^{\\\\dot{\\\\lambda}{2}t}), where (\\\\lambda{1}=-2+j6) and (\\\\lambda_{2}=-2-j6). Since (\\\\alpha=-2) and (\\\\beta=6), the real-form solution is [see Eq. (2.8)]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Footnote 2: The complex conjugate roots of a second-order polynomial can be determined by using the formula in Sec. B.8-10 or by expressing the polynomial as a sum of two squares. The latter can be accomplished by completing the square with the first two terms, as follows:\\n\\n[\\\\lambda^{2}+4\\\\lambda+40=(\\\\lambda^{2}+4\\\\lambda+4)+36=(\\\\lambda+2)^{2}+(6)^{2}=( \\\\lambda+2-j6)(\\\\lambda+2+j6)]\\n\\nDifferentiating this expression, we obtain', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[\\\\dot{y}_{0}(t)=-2ce^{-2t}\\\\cos{(6t+\\\\theta)}-6ce^{-2t}\\\\sin{(6t+\\\\theta)}]\\n\\nTo determine the constants (c) and (\\\\theta), we set (t=0) in the equations for (y_{0}(t)) and (\\\\dot{y}{0}(t)) and substitute the initial conditions (y{0}(0)=2) and (\\\\dot{y}_{0}(0)=16.78), yielding\\n\\n[2 =c\\\\cos{\\\\theta}] [16.78 =-2c\\\\cos{\\\\theta}-6c\\\\sin{\\\\theta}]\\n\\nSolution of these two simultaneous equations in two unknowns (c\\\\cos{\\\\theta}) and (c\\\\sin{\\\\theta}) yields\\n\\n[c\\\\cos{\\\\theta}=2\\\\qquad\\\\text{and}\\\\qquad c\\\\sin{\\\\theta}=-3.463]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Squaring and then adding these two equations yield\\n\\n[c^{2}=(2)^{2}+(-3.464)^{2}=16\\\\Longrightarrow c=4]\\n\\nNext, dividing (c\\\\sin{\\\\theta}=-3.463) by (c\\\\cos{\\\\theta}=2) yields\\n\\n[\\\\tan{\\\\theta}=\\\\frac{-3.463}{2}]\\n\\nand\\n\\n[\\\\theta=\\\\tan^{-1}\\\\left(\\\\frac{-3.463}{2}\\\\right)=-\\\\frac{\\\\pi}{3}]\\n\\nTherefore,\\n\\n[y_{0}(t)=4e^{-2t}\\\\cos{\\\\left(6t-\\\\frac{\\\\pi}{3}\\\\right)}]\\n\\nFor the plot of (y_{0}(t)), refer again to Fig. B.11c.\\n\\n2.2 Using MATLAB to Find Polynomial Roots', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"Find the roots (\\\\lambda_{1}) and (\\\\lambda_{2}) of the polynomial (\\\\lambda^{2}+4\\\\lambda+k) for three values of (k): (a)(k=3), (b)(k=4), and (c)(k=40).\\n\\n(a) >> r = roots([1 4 3]).' r = -3 -1 For (k=3), the polynomial roots are therefore (\\\\lambda_{1}=-3) and (\\\\lambda_{2}=-1).\\n\\n(b) >> r = roots([1 4 4]).' r = -2 -2 For (k=4), the polynomial roots are therefore (\\\\lambda_{1}=\\\\lambda_{2}=-2).\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"(c) >> r = roots([1 4 40]).' r = -2.00+6.00i -2.00-6.00i For (k=40), the polynomial roots are therefore (\\\\lambda_{1}=-2+j6) and (\\\\lambda_{2}=-2-j6).\\n\\n(a) >> y_0 = dsolve('D2y+4Dy+3y=0','y(0)=3','Dy(0)=-7','t') y_0 = 1/exp(t) + 2/exp(3*t) For (k=3), the zero-input response is therefore (y_{0}(t)=e^{-t}+2e^{-3t}).\\n\\n(b) >> y_0 = dsolve('D2y+4Dy+4y=0','y(0)=3','Dy(0)=-7','t') y_0 = 3/exp(2t) - t/exp(2t)For (k=4), the zero-input response is therefore (y_{0}(t)=3e^{-2t}-te^{-2t}).\\n\\n(c)\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"y_0 = dsolve('D2y+4Dy+40y=0','y(0)=3','Dy(0)=-7','t') y_0 = (3cos(6t))/exp(2t) - sin(6t)/(6exp(2t)) For (k=40), the zero-input response is therefore (y_{0}(t)=3e^{-2t}\\\\cos{(6t)}-\\\\frac{1}{6}e^{-2t}\\\\sin{(6t)}).\\n\\n2.1 Finding the Zero-Input Response of a First-Order System\\n\\nFind the zero-input response of an LTIC system described by ((D+5)y(t)=x(t)) if the initial condition is (y(0)=5).\\n\\nANSWER\\n\\n(y_{0}(t)=5e^{-5t})(t\\\\geq 0)\\n\\n2.2 Finding the Zero-Input Response of a Second-Order System\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Letting (y_{0}(0)=1) and (\\\\dot{y}_{0}(0)=4), solve\\n\\n[(D^{2}+2D)y_{0}(t)=0]\\n\\nANSWER\\n\\n(y_{0}(t)=3-2e^{-2t})(t\\\\geq 0)\\n\\n2.3 Practical Initial Conditions and the Meaning of (0^{-}) and (0^{+})\\n\\nIn Ex. 2.1 the initial conditions (y_{0}(0)) and (\\\\dot{y}_{0}(0)) were supplied. In practical problems, we must derive such conditions from the physical situation. For instance, in an (RLC) circuit, we may be given the conditions (initial capacitor voltages, initial inductor currents, etc.).', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='From this information, we need to derive (y_{0}(0)), (\\\\dot{y}_{0}(0)), (\\\\ldots) for the desired variable as demonstrated in the next example.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='In much of our discussion, the input is assumed to start at (t=0), unless otherwise mentioned. Hence, (t=0) is the reference point. The conditions immediately before (t=0) (just before the input is applied) are the conditions at (t=0^{-}), and those immediately after (t=0) (just after the input is applied) are the conditions at (t=0^{+}) (compare this with the historical time frames bce and ce). Inpractice, we are likely to know the initial conditions at (t=0^{-}) rather than at (t=0^{+}). The', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='two sets of conditions are generally different, although in some cases they may be identical.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='The total response (y(t)) consists of two components: the zero-input response (y_{0}(t)) [response due to the initial conditions alone with (x(t)=0)] and the zero-state response resulting from the input alone with all initial conditions zero. At (t=0^{-}), the total response (y(t)) consists solely of the zero-input response (y_{0}(t)) because the input has not started yet. Hence the initial conditions on (y(t)) are identical to those of (y_{0}(t)). Thus, (y(0^{-})=y_{0}(0^{-})),', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='(\\\\dot{y}(0^{-})=\\\\dot{y}{0}(0^{-})), and so on. Moreover, (y{0}(t)) is the response due to initial conditions alone and does not depend on the input (x(t)). Hence, application of the input at (t=0) does not affect (y_{0}(t)). This means the initial conditions on (y_{0}(t)) at (t=0^{-}) and (0^{+}) are identical; that is, (y_{0}(0^{-})), (\\\\dot{y}{0}(0^{-})), (\\\\ldots) are identical to (y{0}(0^{+})), (\\\\dot{y}{0}(0^{+})), (\\\\ldots), respectively. It is clear that for (y{0}(t)), there is no', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='distinction between the initial conditions at (t=0^{-}), (0), and (0^{+}). They are all the same. But this is not the case with the total response (y(t)), which consists of both the zero-input and zero-state responses. Thus, in general, (y(0^{-})\\\\neq y(0^{+})), (\\\\dot{y}(0^{-})\\\\neq\\\\dot{y}(0^{+})), and so on.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='EXAMPLE 2.4: Consideration of Initial Conditions\\n\\nA voltage (x(t)=10e^{-3t}u(t)) is applied at the input of the (RLC) circuit illustrated in Fig. 2.2a. Find the loop current (y(t)) for (t\\\\geq 0) if the initial inductor current is zero [(y(0^{-})=0)] and the initial capacitor voltage is 5 volts [(v_{C}(0^{-})=5)].\\n\\nThe differential (loop) equation relating (y(t)) to (x(t)) was derived in Eq. (1.29) as\\n\\n[(D^{2}+3D+2)y(t)=Dx(t)]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='The zero-state component of (y(t)) resulting from the input (x(t)), assuming that all initial conditions are zero, that is, (y(0^{-})=v_{C}(0^{-})=0), will be obtained later in Ex. 2.9. In this example we shall find the zero-input reponse (y_{0}(t)). For this purpose, we need two initial conditions, (y_{0}(0)) and (\\\\dot{y}{0}(0)). These conditions can be derived from the given initial conditions, (y(0^{-})=0) and (v{C}(0^{-})=5), as follows. Recall that (y_{0}(t)) is the loop current when the', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='input terminals are shorted so that the input (x(t)=0) (zero-input), as depicted in Fig. 2.2b. We now compute (y_{0}(0)) and (\\\\dot{y}_{0}(0)), the values of the loop current and its derivative at (t=0), from the initial values of the inductor current and the capacitor voltage. Remember that the inductor current cannot change instantaneously in the absence of an impulsive voltage. Similarly, the capacitor voltage cannot change instantaneously in the absence of an impulsive current. Therefore,', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='when the input terminals are shorted at (t=0), the inductor current is still zero and the capacitor voltage is still 5 volts. Thus,', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[y_{0}(0)=0]To determine (\\\\dot{y}{0}(0)), we use the loop equation for the circuit in Fig. 2.2b. Because the voltage across the inductor is (L(dy{0}/dt)) or (\\\\dot{y}_{0}(t)), this equation can be written as follows:\\n\\n[\\\\dot{y}{0}(t)+3y{0}(t)+v_{C}(t)=0]\\n\\nSetting (t=0), we obtain\\n\\n[\\\\dot{y}{0}(0)+3y{0}(0)+v_{C}(0)=0]\\n\\nBut (y_{0}(0)=0) and (v_{C}(0)=5). Consequently,\\n\\n[\\\\dot{y}_{0}(0)=-5]\\n\\nTherefore, the desired initial conditions are\\n\\n[y_{0}(0)=0\\\\qquad\\\\text{and}\\\\qquad\\\\dot{y}_{0}(0)=-5]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Thus, the problem reduces to finding (y_{0}(t)), the zero-input component of (y(t)) of the system specified by the equation ((D^{2}+3D+2)y(t)=Dx(t)), when the initial conditions are (y_{0}(0)=0) and (\\\\dot{y}_{0}(0)=-5). We have already solved this problem in Ex. 2.1a, where we found\\n\\n[y_{0}(t)=-5e^{-t}+5e^{-2t}\\\\qquad t\\\\geq 0]\\n\\nThis is the zero-input component of the loop current (y(t)).', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='It is interesting to find the initial conditions at (t=0^{-}) and (0^{+}) for the total response (y(t)). Let us compare (y(0^{-})) and (\\\\dot{y}(0^{-})) with (y(0^{+})) and (\\\\dot{y}(0^{+})). The two pairs can be compared by writing the loop equation for the circuit in Fig. 2.2a at (t=0^{-}) and (t=0^{+}). The only difference between the two situations is that at (t=0^{-}), the input (x(t)=0), whereas at (t=0^{+}), the input (x(t)=10) [because (x(t)=10e^{-3t})]. Hence, the two loop equations are', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[\\\\dot{y}(0^{-})+3y(0^{-})+v_{C}(0^{-})=0] [\\\\dot{y}(0^{+})+3y(0^{+})+v_{C}(0^{+})=10]\\n\\nFigure 2.1: Circuits for Ex. 2.4.\\n\\nThe loop current (y(0^{+})=y(0^{-})=0) because it cannot change instantaneously in the absence of impulsive voltage. The same is true of the capacitor voltage. Hence, (v_{C}(0^{+})=v_{C}(0^{-})=5). Substituting these values in the foregoing equations, we obtain (\\\\dot{y}(0^{-})=-5) and (\\\\dot{y}(0^{+})=5). Thus,', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[y(0^{-})=0,\\\\,\\\\dot{y}(0^{-})=-5\\\\quad\\\\mbox{and}\\\\quad y(0^{+})=0,\\\\,\\\\dot{y}(0^{+})=5 \\\\tag{2.10}]\\n\\n2.3 Zero-Input Response of an RC Circuit\\n\\nIn the circuit in Fig. 2.2a, the inductance (L=0) and the initial capacitor voltage (v_{C}(0)=30) volts. Show that the zero-input component of the loop current is given by (y_{0}(t)=-10e^{-2t/3}) for (t\\\\geq 0).\\n\\nIndependence of the Zero-Input and Zero-State Responses', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='In Ex. 2.4 we computed the zero-input component without using the input (x(t)). The zero-state response can be computed from the knowledge of the input (x(t)) alone; the initial conditions are assumed to be zero (system in zero state). The two components of the system response (the zero-input and zero-state responses) are independent of each other. The two worlds of zero-input response and zero-state response coexist side by side, neither one knowing or caring what the other is doing. For each', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='component, the other is totally irrelevant.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Role of Auxiliary Conditions in Solution of Differential Equations\\n\\nThe solution of a differential equation requires additional pieces of information (the auxiliary conditions). Why? We now show heuristically why a differential equation does not, in general, have a unique solution unless some additional constraints (or conditions) on the solution are known.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Differentiation operation is not invertible unless one piece of information about (y(t)) is given. To get back (y(t)) from (dy/dt), we must know one piece of information, such as (y(0)). Thus, differentiation is an irreversible (noninvertible) operation during which certain information is lost. To invert this operation, one piece of information about (y(t)) must be provided to restore the original (y(t)). Using a similar argument, we can show that, given (d^{2}y/dt^{2}), we can determine (y(t))', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='uniquely only if two additional pieces of information (constraints) about (y(t)) are given. In general, to determine (y(t)) uniquely from its (N)th derivative, we need (N) additional pieces of information (constraints) about (y(t)). These constraints are also called auxiliary conditions. When these conditions are given at (t=0), they are called initial conditions.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Some Insights into the Zero-Input Behavior of a System', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='By definition, the zero-input response is the system response to its internal conditions, assuming that its input is zero. Understanding this phenomenon provides interesting insight into system behavior. If a system is disturbed momentarily from its rest position and if the disturbance is then removed, the system will not come back to rest instantaneously. In general, it will come back to rest over a period of time and only through a special type of motion that is characteristic of the system.2', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='For example, if we press on an automobile fender momentarily and then release it at (t=0), there is no external force on the automobile for (t>0).3 The auto body will eventually come back to its rest (equilibrium) position, but not through any arbitrary motion. It must do so by using only a form of response that is sustainable by the system on its own without any external source, since the input is zero. Only characteristic modes satisfy this condition. The system uses a proper combination of', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='characteristic modes to come back to the rest position while satisfying appropriate boundary (or initial) conditions.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Footnote 2: This assumes that the system will eventually come back to its original rest (or equilibrium) position.\\n\\nFootnote 3: We ignore the force of gravity, which merely causes a constant displacement of the auto body without affecting the other motion.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='If the shock absorbers of the automobile are in good condition (high damping coefficient), the characteristic modes will be monotonically decaying exponentials, and the auto body will come to rest rapidly without oscillation. In contrast, for poor shock absorbers (low damping coefficients), the characteristic modes will be exponentially decaying sinusoids, and the body will come to rest through oscillatory motion. When a series (RC) circuit with an initial charge on the capacitor is shorted,', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='the capacitor will start to discharge exponentially through the resistor. This response of the (RC) circuit is caused entirely by its internal conditions and is sustained by this system without the aid of any external input. The exponential current waveform is therefore the characteristic mode of the (RC) circuit.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Mathematically we know that any combination of characteristic modes can be sustained by the system alone without requiring an external input. This fact can be readily verified for the series (RL) circuit shown in Fig. 2.2. The loop equation for this system is\\n\\n[(D+2)y(t)=x(t)]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='It has a single characteristic root (\\\\lambda=-2), and the characteristic mode is (e^{-2t}). We now verify that a loop current (y(t)=ce^{-2t}) can be sustained through this circuit without any input voltage. The input voltage (x(t)) required to drive a loop current (y(t)=ce^{-2t}) is given by\\n\\n[x(t) =L\\\\frac{dy(t)}{dt}+Ry(t)] [=\\\\frac{d}{dt}(ce^{-2t})+2ce^{-2t}] [=-2ce^{-2t}+2ce^{-2t}=0]\\n\\nFigure 2.2: Modes always get a free ride.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Clearly, the loop current (y(t)=ce^{-2t}) is sustained by the (RL) circuit on its own, without the necessity of an external input.\\n\\nThe Resonance Phenomenon', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"We have seen that any signal consisting of a system's characteristic mode is sustained by the system on its own; the system offers no obstacle to such signals. Imagine what would happen if we were to drive the system with an external input that is one of its characteristic modes. This would be like pouring gasoline on a fire in a dry forest or hiring a child to eat ice cream. A child would gladly do the job without pay. Think what would happen if he were paid by the amount of ice cream he ate!\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='He would work overtime. He would work day and night, until he became sick. The same thing happens with a system driven by an input of the form of characteristic mode. The system response grows without limit, until it burns out.2 We call this behavior the resonance phenomenon. An intelligent discussion of this important phenomenon requires an understanding of the zero-state response; for this reason we postpone this topic until Sec. 2.6-7.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Footnote 2: In practice, the system in resonance is more likely to go in saturation because of high amplitude levels.\\n\\nThe Unit Impulse Response (h(t))', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='In Ch. 1 we explained how a system response to an input (x(t)) may be found by breaking this input into narrow rectangular pulses, as illustrated earlier in Fig. 1.27a, and then summing the system response to all the components. The rectangular pulses become impulses in the limit as their widths approach zero. Therefore, the system response is the sum of its responses to various impulse components. This discussion shows that if we know the system response to an impulse input, we can determine', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='the system response to an arbitrary input (x(t)). We now discuss a method of determining (h(t)), the unit impulse response of an LTIC system described by the (N)th-order differential equation [Eq. (2.1)]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[\\\\frac{d^{N}y(t)}{dt^{N}}+a_{1}\\\\frac{d^{N-1}y(t)}{dt^{N-1}}+\\\\cdot \\\\cdot\\\\cdot+a_{N-1}\\\\frac{dy(t)}{dt}+a_{N}y(t)] [\\\\quad=b_{N-M}\\\\frac{d^{M}x(t)}{dt^{M}}+b_{N-M+1}\\\\frac{d^{M-1}x(t)} {dt^{M-1}}+\\\\cdot\\\\cdot\\\\cdot+b_{N-1}\\\\frac{dx(t)}{dt}+b_{N}x(t)]\\n\\nRecall that noise considerations restrict practical systems to (M\\\\leq N). Under this constraint, the most general case is (M=N). Therefore, Eq. (2.1) can be expressed as', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[(D^{N}+a_{1}D^{N-1}+\\\\cdot\\\\cdot\\\\cdot+a_{N-1}D+a_{N})y(t)=(b_{0}D^{N}+b_{1}D^{N- 1}+\\\\cdot\\\\cdot\\\\cdot+b_{N-1}D+b_{N})x(t) \\\\tag{2.11}]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Before deriving the general expression for the unit impulse response (h(t)), it is illuminating to understand qualitatively the nature of (h(t)). The impulse response (h(t)) is the system response to an impulse input (\\\\delta(t)) applied at (t=0) with all the initial conditions zero at (t=0^{-}). An impulse input (\\\\delta(t)) is like lightning, which strikes instantaneously and then vanishes. But in its wake, in that single moment, objects that have been struck are rearranged. Similarly, an', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='impulse input (\\\\delta(t)) appears momentarily at (t=0), and then it is gone forever. But in that moment it generates energy storages; that is, it creates nonzero initial conditions instantaneously within the system at (t=0^{+}). Although the impulse input (\\\\delta(t)) vanishes for (t>0) so that the system has no input after the impulse has been applied, the system will still have a response generated by these newly created initial conditions. The impulse response (h(t)), therefore, must consist', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"of the system's characteristic modes for (t\\\\geq 0^{+}). As a result,\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[h(t)=\\\\mbox{characteristic mode terms}\\\\qquad t\\\\geq 0^{+}]\\n\\nThis response is valid for (t>0). But what happens at (t=0)? At a single moment (t=0), there can at most be an impulse,+ so the form of the complete response (h(t)) is', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Footnote †: ({}^{\\\\dagger}) It might be possible for the derivatives of (\\\\delta(t)) to appear at the origin. However, if (M\\\\leq N), it is impossible for (h(t)) to have any derivatives of (\\\\delta(t)). This conclusion follows from Eq. (2.11) with (x(t)=\\\\delta(t)) and (y(t)=h(t)). The coefficients of the impulse and all its derivatives must be matched on both sides of this equation. If (h(t)) contains (\\\\delta^{(1)}(t)), the first derivative of (\\\\delta(t)), the left-hand side of Eq. (2.11) will', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='contain a term (\\\\delta^{(N+1)}(t)). But the highest-order derivative term on the right-hand side is (\\\\delta^{(N)}(t)). Therefore, the two sides cannot match. Similar arguments can be made against the presence of the impulse’s higher-order derivatives in (h(t)).', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[h(t)=A_{0}\\\\delta(t)+\\\\mbox{characteristic mode terms}\\\\qquad t\\\\geq 0 \\\\tag{2.12}]\\n\\nbecause (h(t)) is the unit impulse response. Setting (x(t)=\\\\delta(t)) and (y(t)=h(t)) in Eq. (2.11) yields\\n\\n[(D^{N}+a_{1}D^{N-1}+\\\\cdot\\\\cdot\\\\cdot+a_{N-1}D+a_{N})h(t)=(b_{0}D^{N}+b_{1}D^{N- 1}+\\\\cdot\\\\cdot\\\\cdot+b_{N-1}D+b_{N})\\\\delta(t)]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='In this equation we substitute (h(t)) from Eq. (2.12) and compare the coefficients of similar impulsive terms on both sides. The highest order of the derivative of impulse on both sides is (N), with its coefficient value as (A_{0}) on the left-hand side and (b_{0}) on the right-hand side. The two values must be matched. Therefore, (A_{0}=b_{0}) and\\n\\n[h(t)=b_{0}\\\\delta(t)+\\\\mbox{characteristic modes} \\\\tag{2.13}]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='In Eq. (2.11), if (M<N), (b_{0}=0). Hence, the impulse term (b_{0}\\\\delta(t)) exists only if (M=N). The unknown coefficients of the (N) characteristic modes in (h(t)) in Eq. (2.13) can be determined by using the technique of impulse matching, as explained in the following example.\\n\\nEXAMPLE 2.5 Impulse Response via Impulse Matching\\n\\nFind the impulse response (h(t)) for a system specified by\\n\\n[(D^{2}+5D+6)y(t)=(D+1)x(t) \\\\tag{2.14}]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='In this case, (b_{0}=0). Hence, (h(t)) consists of only the characteristic modes. The characteristic polynomial is (\\\\lambda^{2}+5\\\\lambda+6=(\\\\lambda+2)(\\\\lambda+3)). The roots are (-2) and (-3). Hence, the impulse\\n\\n[MISSING_PAGE_FAIL:16]\\n\\nAs stated earlier, if the order of (P(D)) is less than the order of (Q(D)), that is, if (M<N), then (b_{0}=0), and the impulse term (b_{0}\\\\delta(t)) in (h(t)) is zero.\\n\\nExample 2.6: Impulse Response via Simplified Impulse Matching__', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Determine the unit impulse response (h(t)) for a system specified by the equation\\n\\n[\\\\left(D^{2}+3D+2\\\\right)y(t)=Dx(t) \\\\tag{2.19}]\\n\\nThis is a second-order system ((N=2)) having the characteristic polynomial\\n\\n[\\\\left(\\\\lambda^{2}+3\\\\lambda+2\\\\right)=(\\\\lambda+1)(\\\\lambda+2)]\\n\\nThe characteristic roots of this system are (\\\\lambda=-1) and (\\\\lambda=-2). Therefore,\\n\\n[y_{n}(t)=c_{1}e^{-t}+c_{2}e^{-2t} \\\\tag{2.20}]\\n\\nDifferentiation of this equation yields\\n\\n[\\\\dot{y}{n}(t)=-c{1}e^{-t}-2c_{2}e^{-2t} \\\\tag{2.21}]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='The initial conditions are [see Eq. (2.18)]\\n\\n[\\\\dot{y}{n}(0)=1\\\\qquad\\\\mbox{and}\\\\qquad y{n}(0)=0]\\n\\nSetting (t=0) in Eqs. (2.20) and (2.21), and substituting the initial conditions just given, we obtain\\n\\n[0 =c_{1}+c_{2}] [1 =-c_{1}-2c_{2}]\\n\\nSolution of these two simultaneous equations yields\\n\\n[c_{1}=1\\\\qquad\\\\mbox{and}\\\\qquad c_{2}=-1]\\n\\nTherefore,\\n\\n[y_{n}(t)=e^{-t}-e^{-2t}]\\n\\nMoreover, according to Eq. (2.19), (P(D)=D) so that\\n\\n[P(D)y_{n}(t)=Dy_{n}(t)=\\\\dot{y}_{n}(t)=-e^{-t}+2e^{-2t}]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Also in this case, (b_{0}=0) [the second-order term is absent in (P(D))]. Therefore,\\n\\n[h(t)=[P(D)y_{n}(t)]u(t)=(-e^{-t}+2e^{-2t})u(t)]\\n\\nComment. In the above discussion, we have assumed (M\\\\leq N), as specified by Eq. (2.11). Section 2.8 shows that the expression for (h(t)) applicable to all possible values of (M) and (N) is given by\\n\\n[h(t)=P(D)[y_{n}(t)u(t)]]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='where (y_{n}(t)) is a linear combination of the characteristic modes of the system subject to initial conditions [Eq. (2.18)]. This expression reduces to Eq. (2.17) when (M\\\\leq N).', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"Determination of the impulse response (h(t)) using the procedures in this section is relatively simple. However, in Ch. 4 we shall discuss another, even simpler method using the Laplace transform. As the next example demonstrates, it is also possible to find (h(t)) using functions from MATLAB's symbolic math toolbox.\\n\\nExample 2.7: Using MATLAB to Find the Impulse Response\\n\\nDetermine the impulse response (h(t)) for an LTIC system specified by the differential equation\\n\\n[(D^{2}+3D+2)y(t)=Dx(t)]\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"This is a second-order system with (b_{0}=0). First we find the zero-input component for initial conditions (y(0^{-})=0), and (\\\\dot{y}(0^{-})=1). Since (P(D)=D), the zero-input response is differentiated and the impulse response immediately follows as (h(t)=0\\\\delta(t)+[Dy_{n}(t)]u(t)).\\n\\ny_n = dsolve('D2y+3Dy+2y=0','y(0)=0','Dy(0)=1','t'); h = diff(y_n) h = 2/exp(2*t) - 1/exp(t) Therefore, (h(t)=(2e^{-2t}-e^{-t})u(t)).\\n\\nDrill 2.4: Finding the Impulse Response\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Determine the unit impulse response of LTIC systems described by the following equations:\\n\\n((D+2)y(t)=(3D+5)x(t))\\n\\n(D(D+2)y(t)=(D+4)x(t))\\n\\n((D^{2}+2D+1)y(t)=Dx(t))\\n\\nAnswers\\n\\n(3\\\\delta(t)-e^{-2t}u(t))\\n\\n((2-e^{-2t})u(t))\\n\\n((1-t)e^{-t}u(t))\\n\\n[MISSING_PAGE_EMPTY:19]\\n\\n2.4 System Response to External Input: The Zero-State Response\\n\\nFigure 2.3: Finding the system response to an arbitrary input (x(t)).\\n\\n[MISSING_PAGE_EMPTY:21]\\n\\nThe Distributive Property\\n\\nAccording to the distributive property,', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[x_{1}(t)[x_{2}(t)+x_{3}(t)]=x_{1}(t)x_{2}(t)+x_{1}(t)*x_{3}(t) \\\\tag{2.26}]\\n\\nThe Associative Property\\n\\nAccording to the associative property,\\n\\n[x_{1}(t)[x_{2}(t)x_{3}(t)]=[x_{1}(t)x_{2}(t)]x_{3}(t) \\\\tag{2.27}]\\n\\nThe proofs of Eqs. (2.26) and (2.27) follow directly from the definition of the convolution integral. They are left as an exercise for the reader.\\n\\nThe Shift Property\\n\\nIf\\n\\n[x_{1}(t)*x_{2}(t)=c(t)]\\n\\nthen\\n\\n[x_{1}(t)x_{2}(t-T)=x_{1}(t-T)x_{2}(t)=c(t-T)]\\n\\nMore generally, we see that', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[x_{1}(t-T_{1})*x_{2}(t-T_{2})=c(t-T_{1}-T_{2}) \\\\tag{2.28}]\\n\\nProof.: We are given\\n\\n[x_{1}(t)*x_{2}(t)=\\\\int_{-\\\\infty}^{\\\\infty}x_{1}(\\\\tau)x_{2}(t-\\\\tau)\\\\,d\\\\tau=c(t)]\\n\\nTherefore,\\n\\n[x_{1}(t)*x_{2}(t-T)= \\\\int_{-\\\\infty}^{\\\\infty}x_{1}(\\\\tau)x_{2}(t-T-\\\\tau)\\\\,d\\\\tau] [= c(t-T)]\\n\\nThe equally simple proof of Eq. (2.28) follows a similar approach.\\n\\nConvolution with an Impulse\\n\\nConvolution of a function (x(t)) with a unit impulse results in the function (x(t)) itself. By definition of convolution,', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[x(t)*\\\\delta(t)=\\\\int_{-\\\\infty}^{\\\\infty}x(\\\\tau)\\\\delta(t-\\\\tau)\\\\,d\\\\tau]\\n\\nBecause (\\\\delta(t-\\\\tau)) is an impulse located at (\\\\tau=t), according to the sampling property of the impulse [Eq. (1.11)], the integral here is just the value of (x(\\\\tau)) at (\\\\tau=t), that is, (x(t)). Therefore,\\n\\n[x(t)*\\\\delta(t)=x(t)]\\n\\nActually this result was derived earlier [Eq. (2.22)].\\n\\nChapter The Width Property', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='If the durations (widths) of (x_{1}(t)) and (x_{2}(t)) are finite, given by (T_{1}) and (T_{2}), respectively, then the duration (width) of (x_{1}(t)*x_{2}(t)) is (T_{1}+T_{2}) (Fig. 2.4). The proof of this property follows readily from the graphical considerations discussed later in Sec. 2.4-2.\\n\\n2.4 Zero-State Response and Causality\\n\\nThe (zero-state) response (y(t)) of an LTIC system is\\n\\n[y(t)=x(t)*h(t)=\\\\int_{-\\\\infty}^{\\\\infty}x(\\\\tau)h(t-\\\\tau)\\\\,d\\\\tau \\\\tag{2.29}]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='In deriving Eq. (2.29), we assumed the system to be linear and time-invariant. There were no other restrictions either on the system or on the input signal (x(t)). Since, in practice, most systems are causal, their response cannot begin before the input. Furthermore, most inputs are also causal, which means they start at (t=0).', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"Causality restriction on both signals and systems further simplifies the limits of integration in Eq. (2.29). By definition, the response of a causal system cannot begin before its input begins. Consequently, the causal system's response to a unit impulse (\\\\delta(t)) (which is located at (t=0)) cannot begin before (t=0). Therefore, a causal system's unit impulse response (h(t)) is a causal signal.\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='It is important to remember that the integration in Eq. (2.29) is performed with respect to (\\\\tau) (not (t)). If the input (x(t)) is causal, (x(\\\\tau)=0) for (\\\\tau<0). Therefore, (x(\\\\tau)=0) for (\\\\tau<0), as illustrated in Fig. 2.5a. Similarly, if (h(t)) is causal, (h(t-\\\\tau)=0) for (t-\\\\tau<0); that is, for (\\\\tau>t), as depicted in Fig. 2.5a. Therefore, the product (x(\\\\tau)h(t-\\\\tau)=0) everywhere except over the nonshaded interval (0\\\\leq\\\\tau\\\\leq t) shown in Fig. 2.5a (assuming (t\\\\geq 0)).', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Observe that if (t) is negative, (x(\\\\tau)h(t-\\\\tau)=0) for all (\\\\tau), as shown in Fig. 2.5b. Therefore, Eq. (2.29) reduces to', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[y(t)=x(t)*h(t)=\\\\begin{cases}\\\\int_{0^{-}}^{t}x(\\\\tau)h(t-\\\\tau)\\\\,d\\\\tau&t\\\\geq 0 \\\\ 0&t<0\\\\end{cases} \\\\tag{2.30}]\\n\\nThe lower limit of integration in Eq. (2.30) is taken as (0^{-}) to avoid the difficulty in integration that can arise if (x(t)) contains an impulse at the origin. This result shows that if (x(t)) and (h(t)) are both causal, the response (y(t)) is also causal.\\n\\nFigure 2.4: Width property of convolution.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"Because of the convolution's commutative property [Eq. (2.25)], we can also express Eq. (2.30) as [assuming causal (x(t)) and (h(t))]\\n\\n[y(t)=\\\\begin{cases}\\\\int_{0^{-}}^{t}h(\\\\tau)x(t-\\\\tau)\\\\,d\\\\tau&t\\\\geq 0\\\\ 0&t<0\\\\end{cases}]\\n\\nHereafter, the lower limit of (0^{-}) will be implied even when we write it as (0). As in Eq. (2.30), this result assumes that both the input and the system are causal.\\n\\nExample 2.8: Computing the Zero-State Response\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='For an LTIC system with the unit impulse response (h(t)=e^{-2t}u(t)), determine the response (y(t)) for the input\\n\\n[x(t)=e^{-t}u(t)]\\n\\nHere both (x(t)) and (h(t)) are causal (Fig. 2.6). Hence, from Eq. (2.30), we obtain\\n\\n[y(t)=\\\\int_{0}^{t}x(\\\\tau)h(t-\\\\tau)\\\\,d\\\\tau\\\\qquad t\\\\geq 0]\\n\\nBecause (x(t)=e^{-t}u(t)) and (h(t)=e^{-2t}u(t)),\\n\\n[x(\\\\tau)=e^{-\\\\tau}u(\\\\tau)\\\\qquad\\\\text{and}\\\\qquad h(t-\\\\tau)=e^{-2(t-\\\\tau)}u(t-\\\\tau)]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Remember that the integration is performed with respect to (\\\\tau) (not (t)), and the region of integration is (0\\\\leq\\\\tau\\\\leq t). Hence, (\\\\tau\\\\geq 0) and (t-\\\\tau\\\\geq 0). Therefore, (u(\\\\tau)=1) and (u(t-\\\\tau)=1); consequently,\\n\\n[y(t)=\\\\int_{0}^{t}e^{-\\\\tau}e^{-2(t-\\\\tau)}\\\\,d\\\\tau\\\\qquad t\\\\geq 0]Because this integration is with respect to (\\\\tau), we can pull (e^{-2t}) outside the integral, giving us\\n\\n[y(t)=e^{-2t}\\\\int_{0}^{t}e^{\\\\tau}\\\\,d\\\\tau=e^{-2t}(e^{t}-1)=e^{-t}-e^{-2t}\\\\qquad t\\\\geq 0]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Moreover, (y(t)=0) when (t<0) [see Eq. (2.30)]. Therefore,\\n\\n[y(t)=(e^{-t}-e^{-2t})u(t)]\\n\\nThe response is depicted in Fig. 2.6c.\\n\\n2.5 Computing the Zero-State Response\\n\\nFor an LTIC system with the impulse response (h(t)=6e^{-t}u(t)), determine the system response to the input: (a)(2u(t)) and (b)(3e^{-3t}u(t)).\\n\\nAnswers\\n\\n(12(1-e^{-t})u(t))\\n\\n(9(e^{-t}-e^{-3t})u(t))\\n\\nFigure 2.6: Convolution of (x(t)) and (h(t)).\\n\\n2.6 Zero-State Response with Resonance', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Repeat Drill 2.5 for the input (x(t)=e^{-t}u(t)).\\n\\nAnswer\\n\\n(6t^{-t}u(t))\\n\\nThe Convolution Table', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='The task of convolution is considerably simplified by a ready-made convolution table (Table 2.1). This table, which lists several pairs of signals and their convolution, can conveniently determine (y(t)), a system response to an input (x(t)), without performing the tedious job of integration. For instance, we could have readily found the convolution in Ex. 2.8 by using pair 4 (with (\\\\lambda_{1}=-1) and (\\\\lambda_{2}=-2)) to be ((e^{-t}-e^{-2t})u(t)). The following example demonstrates the', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='utility of this table.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Example 2.9: Convolution by Tables\\n\\nUse Table 2.1 to compute the loop current (y(t)) of the (RLC) circuit in Ex. 2.4 for the input (x(t)=10e^{-3t}u(t)) when all the initial conditions are zero.\\n\\nThe loop equation for this circuit [see Ex. 1.16 or Eq. (1.29)] is\\n\\n[(D^{2}+3D+2)y(t)=Dx(t)]\\n\\nThe impulse response (h(t)) for this system, as obtained in Ex. 2.6, is\\n\\n[h(t)=(2e^{-2t}-e^{-t})u(t)]\\n\\nThe input is (x(t)=10e^{-3t}u(t)), and the response (y(t)) is', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[y(t)=x(t)h(t)=10e^{-3t}u(t)[2e^{-2t}-e^{-t}]u(t)]\\n\\nUsing the distributive property of the convolution [Eq. (2.26)], we obtain\\n\\n[y(t) =10e^{-3t}u(t)2e^{-2t}u(t)-10e^{-3t}u(t)e^{-t}u(t)] [=20[e^{-3t}u(t)e^{-2t}u(t)]-10[e^{-3t}u(t)e^{-t}u(t)]]\\n\\nNow the use of pair 4 in Table 2.1 yields\\n\\n[y(t) =\\\\frac{20}{-3-(-2)}[e^{-3t}-e^{-2t}]u(t)-\\\\frac{10}{-3-(-1)}[e^{-3t }-e^{-t}]u(t)] [=-20(e^{-3t}-e^{-2t})u(t)+5(e^{-3t}-e^{-t})u(t)] [=(-5e^{-t}+20e^{-2t}-15e^{-3t})u(t)]\\n\\n[MISSING_PAGE_EMPTY:27]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='2.7 Convolution by Tables\\n\\nUse Table 2.1 to show (e^{-2t}u(t)*(1-e^{-t})u(t)=\\\\left(\\\\frac{1}{2}-e^{-t}+\\\\frac{1}{2}e^{-2t}\\\\right)u(t)).\\n\\n2.8 Zero-State Response by Convolution Table\\n\\n2.9 Another Zero-State Response by Convolution Table\\n\\nFor an LTIC system with the unit impulse response (h(t)=e^{-2t}u(t)), determine the zero-state response (y(t)) if the input (x(t)=\\\\sin 3t\\\\,u(t)). [Hint: Use pair 12 from Table 2.1.]\\n\\n2.9 Another Zero-State Response by Convolution Table', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='For an LTIC system with the unit impulse response (h(t)=e^{-2t}u(t)), determine the zero-state response (y(t)) if the input (x(t)=\\\\sin 3t\\\\,u(t)). [Hint: Use pair 12 from Table 2.1.]\\n\\n2.10 Answer\\n\\nThe LTIC system response discussed so far applies to general input signals, real or complex. However, if the system is real, that is, if (h(t)) is real, then we shall show that the real part of the input generates the real part of the output, and a similar conclusion applies to the imaginary part.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='If the input is (x(t)=x_{r}(t)+jx_{i}(t)), where (x_{r}(t)) and (x_{i}(t)) are the real and imaginary parts of (x(t)), then for real (h(t))\\n\\n[y(t)=h(t)[x_{r}(t)+jx_{i}(t)]=h(t)x_{r}(t)+jh(t)*x_{i}(t)=y_{r}(t)+jy_{i}(t)]\\n\\nwhere (y_{r}(t)) and (y_{i}(t)) are the real and the imaginary parts of (y(t)). Using the right-directed-arrow notation to indicate a pair of the input and the corresponding output, the foregoing result can be expressed as follows. If', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[x(t)=x_{r}(t)+jx_{i}(t)\\\\ \\\\ \\\\ \\\\Longrightarrow\\\\ \\\\ \\\\ y(t)=y_{r}(t)+jy_{i}(t)]\\n\\nthen\\n\\n[x_{r}(t)\\\\ \\\\ \\\\ \\\\Longrightarrow\\\\ \\\\ y_{r}(t)\\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\text{and}\\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ x_{i}(t)\\\\ \\\\ \\\\ \\\\Longrightarrow\\\\ \\\\ \\\\ y_{i}(t) \\\\tag{2.31}]\\n\\nChapter 2 Time-domain analysis of continuous-time systems\\n\\n2.4 Graphical Understanding of Convolution Operation', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"The convolution operation can be grasped readily through a graphical interpretation of the convolution integral. Such an understanding is helpful in evaluating the convolution integral of more complex signals. In addition, graphical convolution allows us to grasp visually or mentally the convolution integral's result, which can be of great help in sampling, filtering, and many other problems. Finally, many signals have no exact mathematical description, so they can be described only\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='graphically. If two such signals are to be convolved, we have no choice but to perform their convolution graphically.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='We shall now explain the convolution operation by convolving the signals (x(t)) and (g(t)), illustrated in Figs. 2.7a and 2.7b, respectively. If (c(t)) is the convolution of (x(t)) with (g(t)), then\\n\\n[c(t)=\\\\int_{-\\\\infty}^{\\\\infty}x(\\\\tau)g(t-\\\\tau)\\\\,d\\\\tau]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='One of the crucial points to remember here is that this integration is performed with respect to (\\\\tau) so that (t) is just a parameter (like a constant). This consideration is especially important when we sketch the graphical representations of the functions (x(\\\\tau)) and (g(t-\\\\tau)). Both these functions should be sketched as functions of (\\\\tau), not of (t).', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='The function (x(\\\\tau)) is identical to (x(t)), with (\\\\tau) replacing (t) (Fig. 2.7c). Therefore, (x(t)) and (x(\\\\tau)) will have the same graphical representations. Similar remarks apply to (g(t)) and (g(\\\\tau)) (Fig. 2.7d).\\n\\nTo appreciate what (g(t-\\\\tau)) looks like, let us start with the function (g(\\\\tau)) (Fig. 2.7d). Time reversal of this function (reflection about the vertical axis (\\\\tau=0)) yields (g(-\\\\tau)) (Fig. 2.7e). Let us denote this function by (\\\\phi(\\\\tau)):\\n\\n[\\\\phi(\\\\tau)=g(-\\\\tau)]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Now (\\\\phi(\\\\tau)) shifted by (t) seconds is (\\\\phi(\\\\tau-t)), given by\\n\\n[\\\\phi(\\\\tau-t)=g[-(\\\\tau-t)]=g(t-\\\\tau)]\\n\\nTherefore, we first time-reverse (g(\\\\tau)) to obtain (g(-\\\\tau)) and then time-shift (g(-\\\\tau)) by (t) to obtain (g(t-\\\\tau)). For positive (t), the shift is to the right (Fig. 2.7f); for negative (t), the shift is to the left (Figs. 2.7g, 2.7h).', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='The preceding discussion gives us a graphical interpretation of the functions (x(\\\\tau)) and (g(t-\\\\tau)). The convolution (c(t)) is the area under the product of these two functions. Thus, to compute (c(t)) at some positive instant (t=t_{1}), we first obtain (g(-\\\\tau)) by inverting (g(\\\\tau)) about the vertical axis. Next, we right-shift or delay (g(-\\\\tau)) by (t_{1}) to obtain (g(t_{1}-\\\\tau)) (Fig. 2.7f), and then we multiply this function by (x(\\\\tau)), giving us the product', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='(x(\\\\tau)g(t_{1}-\\\\tau)) (shaded portion in Fig. 2.7f). The area (A_{1}) under this product is (c(t_{1})), the value of (c(t)) at (t=t_{1}). We can therefore plot (c(t_{1})=A_{1}) on a curve describing (c(t)), as shown in Fig. 2.7i. The area under the product (x(\\\\tau)g(-\\\\tau)) in Fig. 2.7e is (c(0)), the value of the convolution for (t=0) (at the origin).', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='System Response to External Input: The Zero-State Response\\n\\nFigure 2.7: Graphical explanation of the convolution operation.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='A similar procedure is followed in computing the value of (c(t)) at (t=t_{2}), where (t_{2}) is negative (Fig. 2.7g). In this case, the function (g(-\\\\tau)) is shifted by a negative amount (that is, left-shifted) to obtain (g(t_{2}-\\\\tau)). Multiplication of this function with (x(\\\\tau)) yields the product (x(\\\\tau)g(t_{2}-\\\\tau)). The area under this product is (c(t_{2})=A_{2}), giving us another point on the curve (c(t)) at (t=t_{2}) (Fig. 2.7i). This procedure can be repeated for all values of', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='(t), from (-\\\\infty) to (\\\\infty). The result will be a curve describing (c(t)) for all time (t). Note that when (t\\\\leq-3,x(\\\\tau)) and (g(t-\\\\tau)) do not overlap (see Fig. 2.7h); therefore, (c(t)=0) for (t\\\\leq-3).', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Summary of the Graphical Procedure\\n\\nThe procedure for graphical convolution can be summarized as follows:\\n\\nKeep the function (x(\\\\tau)) fixed.\\n\\nVisualize the function (g(\\\\tau)) as a rigid wire frame, and rotate (or invert) this frame about the vertical axis ((\\\\tau=0)) to obtain (g(-\\\\tau)).\\n\\nShift the inverted frame along the (\\\\tau) axis by (t_{0}) seconds. The shifted frame now represents (g(t_{0}-\\\\tau)).', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='The area under the product of (x(\\\\tau)) and (g(t_{0}-\\\\tau)) (the shifted frame) is (c(t_{0})), the value of the convolution at (t=t_{0}).\\n\\nRepeat this procedure, shifting the frame by different values (positive and negative) to obtain (c(t)) for all values of (t).', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='The graphical procedure discussed here appears very complicated and discouraging at first reading. Indeed, some people claim that convolution has driven many electrical engineering undergraduates to contemplate theology either for salvation or as an alternative career (IEEE Spectrum, March 1991, p. 60). Actually, the bark of convolution is worse than its bite. In graphical convolution, we need to determine the area under the product (x(\\\\tau)g(t-\\\\tau)) for all values of (t) from (-\\\\infty) to', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='(\\\\infty). However, a mathematical description of (x(\\\\tau)g(t-\\\\tau)) is generally valid over a range of (t). Therefore, repeating the procedure for every value of (t) amounts to repeating it only a few times for different ranges of (t).', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='We can also use the commutative property of convolution to our advantage by computing (x(t)g(t)) or (g(t)x(t)), whichever is simpler. As a rule of thumb, convolution computations are simplified if we choose to invert (time-reverse) the simpler of the two functions. For example, if the mathematical description of (g(t)) is simpler than that of (x(t)), then (x(t)g(t)) will be easier to compute than (g(t)x(t)). In contrast, if the mathematical description of (x(t)) is simpler, the reverse will be', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='true.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='We shall demonstrate graphical convolution with the following examples. Let us start by using this graphical method to rework Ex. 2.8.\\n\\nEXAMPLE 2.10 Graphical Convolution of Two Causal Functions\\n\\nDetermine graphically (y(t)=x(t)*h(t)) for (x(t)=e^{-t}u(t)) and (h(t)=e^{-2t}u(t)).', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='In Figs. 2.8a and 2.8b we have (x(t)) and (h(t)), respectively; and Fig. 2.8c shows (x(\\\\tau)) and (h(-\\\\tau)) as functions of (\\\\tau). The function (h(t-\\\\tau)) is now obtained by shifting (h(-\\\\tau)) by (t). If (t) is positive, the shift is to the right (delay); if (t) is negative, the shift is to the left (advance). Figure 2.8d shows that for negative (t), (h(t-\\\\tau)) [obtained by left-shifting (h(-\\\\tau))] does not overlap (x(\\\\tau)), and the product (x(\\\\tau)h(t-\\\\tau)=0), so that', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[y(t)=0\\\\qquad t<0]\\n\\nFigure 2.8e shows the situation for (t\\\\geq 0). Here (x(\\\\tau)) and (h(t-\\\\tau)) do overlap, but the product is nonzero only over the interval (0\\\\leq\\\\tau\\\\leq t) (shaded interval). Therefore,\\n\\n[y(t)=\\\\int_{0}^{t}x(\\\\tau)h(t-\\\\tau)\\\\,d\\\\tau\\\\qquad t\\\\geq 0]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='All we need to do now is substitute correct expressions for (x(\\\\tau)) and (h(t-\\\\tau)) in this integral. From Figs. 2.8a and 2.8b, it is clear that the segments of (x(t)) and (g(t)) to be used in this convolution (Fig. 2.8e) are described by\\n\\n[x(t)=e^{-t}\\\\qquad\\\\mbox{and}\\\\qquad h(t)=e^{-2t}]\\n\\nTherefore,\\n\\n[x(\\\\tau)=e^{-\\\\tau}\\\\qquad\\\\mbox{and}\\\\qquad h(t-\\\\tau)=e^{-2(t-\\\\tau)}]\\n\\nConsequently,', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[y(t)=\\\\int_{0}^{t}e^{-\\\\tau}e^{-2(t-\\\\tau)}\\\\,d\\\\tau=e^{-2t}\\\\int_{0}^{t}e^{\\\\tau}\\\\, d\\\\tau=e^{-t}-e^{-2t}\\\\qquad\\\\qquad t\\\\geq 0]\\n\\nMoreover, (y(t)=0) for (t<0) so that\\n\\n[y(t)=(e^{-t}-e^{-2t})u(t)]\\n\\n[MISSING_PAGE_FAIL:33]\\n\\n2.11 Graphical Convolution: Causal Function and Two-Sided Function\\n\\nFind (c(t)=x(t)*g(t)) for the signals depicted in Figs. 2.9a and 2.9b.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Since (x(t)) is simpler than (g(t)), it is easier to evaluate (g(t)x(t)) than (x(t)g(t)). However, we shall intentionally take the more difficult route and evaluate (x(t)*g(t)).\\n\\nFrom (x(t)) and (g(t)) (Figs. 2.9a and 2.9b, respectively), observe that (g(t)) is composed of two segments. As a result, it can be described as\\n\\n[g(t)=\\\\begin{cases}2e^{-t}&\\\\text{segment A}\\\\ -2e^{2t}&\\\\text{segment B}\\\\end{cases}]\\n\\nTherefore,', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[g(t-\\\\tau)=\\\\begin{cases}2e^{-(t-\\\\tau)}&\\\\text{segment A}\\\\ -2e^{2(t-\\\\tau)}&\\\\text{segment B}\\\\end{cases}]\\n\\nThe segment of (x(t)) that is used in convolution is (x(t)=1) so that (x(\\\\tau)=1). Figure 2.9c shows (x(\\\\tau)) and (g(-\\\\tau)).', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='To compute (c(t)) for (t\\\\geq 0), we right-shift (g(-\\\\tau)) to obtain (g(t-\\\\tau)), as illustrated in Fig. 2.9d. Clearly, (g(t-\\\\tau)) overlaps with (x(\\\\tau)) over the shaded interval, that is, over the range (\\\\tau\\\\geq 0); segment A overlaps with (x(\\\\tau)) over the interval ((0,t)), while segment B overlaps with (x(\\\\tau)) over ((t,\\\\infty)). Remembering that (x(\\\\tau)=1), we have', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[c(t) =\\\\int_{0}^{\\\\infty}x(\\\\tau)g(t-\\\\tau)\\\\,d\\\\tau] [=\\\\int_{0}^{t}2e^{-(t-\\\\tau)}\\\\,d\\\\tau+\\\\int_{t}^{\\\\infty}-2e^{2(t- \\\\tau)}\\\\,d\\\\tau] [=2(1-e^{-t})-1=1-2e^{-t}\\\\qquad\\\\qquad t\\\\geq 0]\\n\\nFigure 2.9e shows the situation for (t<0). Here the overlap is over the shaded interval, that is, over the range (\\\\tau\\\\geq 0), where only the segment B of (g(t)) is involved. Therefore,\\n\\n[c(t)=\\\\int_{0}^{\\\\infty}x(\\\\tau)g(t-\\\\tau)\\\\,d\\\\tau=\\\\int_{0}^{\\\\infty}-2e^{2(t-\\\\tau) }\\\\,d\\\\tau=-e^{2t}\\\\qquad\\\\qquad t\\\\leq 0]\\n\\nTherefore,', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[c(t)=\\\\begin{cases}1-2e^{-t}&\\\\quad t\\\\geq 0\\\\ -e^{2t}&\\\\quad t\\\\leq 0\\\\end{cases}]\\n\\nFigure 2.9f shows a plot of (c(t)).\\n\\nChapter 2 Time-domain analysis of continuous-time systems\\n\\nFigure 2.9: Convolution of (x(t)) and (g(t)).\\n\\n2.12 Graphical Convolution of Two Finite-Duration Functions\\n\\nFind (x(t)*g(t)) for the functions (x(t)) and (g(t)) shown in Figs. 2.10a and 2.10b.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Here, (x(t)) has a simpler mathematical description than that of (g(t)), so it is preferable to time-reverse (x(t)). Hence, we shall determine (g(t)x(t)) rather than (x(t)g(t)). Thus,\\n\\n[c(t)=g(t)*x(t)=\\\\int_{-\\\\infty}^{\\\\infty}g(\\\\tau)x(t-\\\\tau)\\\\,d\\\\tau]\\n\\nFirst, we determine the expressions for the segments of (x(t)) and (g(t)) used in finding (c(t)). According to Figs. 2.10a and 2.10b, these segments can be expressed as\\n\\n[x(t)=1\\\\qquad\\\\text{and}\\\\qquad g(t)=\\\\tfrac{1}{3}t]\\n\\nso that', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[x(t-\\\\tau)=1\\\\qquad\\\\text{and}\\\\qquad g(\\\\tau)=\\\\tfrac{1}{3}\\\\tau]\\n\\nFigure 2.10c shows (g(\\\\tau)) and (x(-\\\\tau)), whereas Fig. 2.10d shows (g(\\\\tau)) and (x(t-\\\\tau)), which is (x(-\\\\tau)) shifted by (t). Because the edges of (x(-\\\\tau)) are at (\\\\tau=-1) and (1), the edges of (x(t-\\\\tau)) are at (-1+t) and (1+t). The two functions overlap over the interval ((0,1+t)) (shaded interval) so that', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[c(t)=\\\\int_{0}^{1+t}g(\\\\tau)x(t-\\\\tau)\\\\,d\\\\tau=\\\\int_{0}^{1+t}\\\\tfrac{1}{3}\\\\tau\\\\,d \\\\tau=\\\\tfrac{1}{6}(t+1)^{2}\\\\qquad\\\\qquad-1\\\\leq t\\\\leq 1 \\\\tag{2.32}]\\n\\nThis situation, depicted in Fig. 2.10d, is valid only for (-1\\\\leq t\\\\leq 1). For (t\\\\geq 1) but (\\\\leq 2), the situation is as illustrated in Fig. 2.10e. The two functions overlap only over the range (-1+t) to (1+t) (shaded interval). Note that the expressions for (g(\\\\tau)) and (x(t-\\\\tau)) do not change; only the range of integration changes. Therefore,', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[c(t)=\\\\int_{-1+t}^{1+t}\\\\tfrac{1}{3}\\\\tau\\\\,d\\\\tau=\\\\tfrac{2}{3}t\\\\qquad\\\\qquad 1\\\\leq t\\\\leq 2 \\\\tag{2.33}]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Also note that the expressions in Eqs. (2.32) and (2.33) both apply at (t=1), the transition point between their respective ranges. We can readily verify that both expressions yield a value of (2/3) at (t=1) so that (c(1)=2/3). The continuity of (c(t)) at transition points indicates a high probability of a correct answer. Continuity of (c(t)) at transition points is assured as long as (x(t)) and (g(t)) contain no impulse functions.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='For (t\\\\geq 2) but (\\\\leq 4), the situation is as shown in Fig. 2.10f. The functions (g(\\\\tau)) and (x(t-\\\\tau)) overlap over the interval from (-1+t) to (3) (shaded interval) so that\\n\\n[c(t)=\\\\int_{-1+t}^{3}\\\\tfrac{1}{3}\\\\tau\\\\,d\\\\tau=-\\\\tfrac{1}{6}(t^{2}-2t-8)\\\\qquad \\\\qquad 2\\\\leq t\\\\leq 4 \\\\tag{2.34}]Figure 2.10: Convolution of (x(t)) and (g(t)).\\n\\nBoth Eqs. (2.33) and (2.34) apply at the transition point (t=2). We can readily verify that (c(2)=4/3) when either of these expressions is used.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='For (t\\\\geq 4), (x(t-\\\\tau)) has been shifted so far to the right that it no longer overlaps with (g(\\\\tau)) as depicted in Fig. 2.10g. Consequently,\\n\\n[c(t)=0\\\\qquad\\\\qquad t\\\\geq 4]\\n\\nWe now turn our attention to negative values of (t). We have already determined (c(t)) up to (t=-1). For (t<-1), there is no overlap between the two functions, as illustrated in Fig. 2.10h, so that\\n\\n[c(t)=0\\\\qquad\\\\qquad t\\\\leq-1]\\n\\nCombining our results, we see that', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[c(t)=\\\\begin{cases}\\\\frac{1}{6}(t+1)^{2}&-1\\\\leq t<1\\\\ \\\\frac{2}{3}t&1\\\\leq t<2\\\\ -\\\\frac{1}{6}(t^{2}-2t-8)&2\\\\leq t<4\\\\ 0&\\\\text{otherwise}\\\\end{cases}]\\n\\nFigure 2.10i plots (c(t)) according to this expression.\\n\\nThe Width of Convolved Functions', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='The widths (durations) of (x(t)), (g(t)), and (c(t)) in Ex. 2.12 (Fig. 2.10) are 2, 3, and 5, respectively. Note that the width of (c(t)) in this case is the sum of the widths of (x(t)) and (g(t)). This observation is not a coincidence. Using the concept of graphical convolution, we can readily see that if (x(t)) and (g(t)) have the finite widths of (T_{1}) and (T_{2}) respectively, then the width of (c(t)) is equal to (T_{1}+T_{2}). The reason is that the time it takes for a signal of width', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='(duration) (T_{1}) to completely pass another signal of width (duration) (T_{2}) so that they become non-overlapping is (T_{1}+T_{2}). When the two signals become non-overlapping, the convolution goes to zero.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Drill 2.10 Interchanging Convolution Order\\n\\nRework Ex. 2.11 by evaluating (g(t)*x(t)).\\n\\nDrill 2.11 Showing Commutability Using Two Causal Signals\\n\\nUse graphical convolution to show that (x(t)g(t)=g(t)x(t)=c(t)) in Fig. 2.11.\\n\\nChapter 2 Time-domain analysis of continuous-time systems\\n\\n2.12 Showing Commutability Using a Causal Signal and an Anticausal Signal\\n\\nRepeat Drill 2.11 for the functions in Fig. 2.12.\\n\\n2.13 Showing Commutability Using Shifted Signals', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Repeat Drill 2.11 for the functions in Fig. 2.13.\\n\\nFigure 2.13: Convolution of shifted signals (x(t)) and (g(t)).\\n\\nFigure 2.12: Convolution of causal (x(t)) and anticausal (g(t)).\\n\\nFigure 2.11: Convolution of causal signals (x(t)) and (g(t)).\\n\\nThe Phantom of the Signals and Systems Opera', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='In the study of signals and systems we often come across some signals such as an impulse, which cannot be generated in practice and have never been sighted by anyone.2 One wonders why we even consider such idealized signals. The answer should be clear from our discussion so far in this chapter. Even if the impulse function has no physical existence, we can compute the system response (h(t)) to this phantom input according to the procedure in Sec. 2.3, and knowing (h(t)), we can compute the', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='system response to any arbitrary input. The concept of impulse response, therefore, provides an effective intermediary for computing system response to an arbitrary input. In addition, the impulse response (h(t)) itself provides a great deal of information and insight about the system behavior. In Sec. 2.6 we show that the knowledge of impulse response provides much valuable information, such as the response time, pulse dispersion, and filtering properties of the system. Many other useful', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='insights about the system behavior can be obtained by inspection of (h(t)).', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Footnote 2: The late Prof. S. J. Mason, the inventor of signal flow graph techniques, used to tell a story of a student frustrated with the impulse function. The student said, “The unit impulse is a thing that is so small you can’t see it, except at one place (the origin), where it is so big you can’t see it. In other words, you can’t see it at all; at least I can’t!” [2].', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Similarly, in frequency-domain analysis (discussed in later chapters), we use an everlasting exponential (or sinusoid) to determine system response. An everlasting exponential (or sinusoid), too, is a phantom, which nobody has ever seen and which has no physical existence. But it provides another effective intermediary for computing the system response to an arbitrary input. Moreover, the system response to everlasting exponential (or sinusoid) provides valuable information and insight', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"regarding the system's behavior. Clearly, idealized impulses and everlasting sinusoids are friendly and helpful spirits.\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Interestingly, the unit impulse and the everlasting exponential (or sinusoid) are the dual of each other in the time-frequency duality, to be studied in Ch. 7. Actually, the time-domain and the frequency-domain methods of analysis are the dual of each other.\\n\\nWhy Convolution? An Intuitive Explanation of System Response', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='On the surface, it appears rather strange that the response of linear systems (those gentlest of the gentle systems) should be given by such a tortuous operation of convolution, where one signal is fixed and the other is inverted and shifted. To understand this odd behavior, consider a hypothetical impulse response (h(t)) that decays linearly with time (Fig. 2.14a). This response is strongest at (t=0), the moment the impulse is applied, and it decays linearly at future instants so that one', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='second later (at (t=1) and beyond), it ceases to exist. This means that the closer the impulse input is to an instant (t), the stronger is its response at (t).', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Now consider the input (x(t)) shown in Fig. 2.14b. To compute the system response, we break the input into rectangular pulses and approximate these pulses with impulses. Generally, the response of a causal system at some instant (t) will be determined by all the impulse components of the input before (t). Each of these impulse components will have different weight in determining the response at the instant (t), depending on its proximity to (t). As seen earlier, the closer the impulse is to', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='(t), the stronger is its influence at (t). The impulse at (t) has the greatest weight (unity) in determiningthe response at (t). The weight decreases linearly for all impulses before (t) until the instant (t-1). The input before (t-1) has no influence (zero weight). Thus, to determine the system response at (t), we must assign a linearly decreasing weight to impulses occurring before (t), as shown in Fig. 14b. This weighting function is precisely the function (h(t-\\\\tau)). The system response at', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='(t) is then determined not by the input (x(\\\\tau)) but by the weighted input (x(\\\\tau)h(t-\\\\tau)), and the summation of all these weighted inputs is the convolution integral.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='2.4-3 Interconnected Systems', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='A larger, more complex system can often be viewed as the interconnection of several smaller subsystems, each of which is easier to characterize. Knowing the characterizations of these subsystems, it becomes simpler to analyze such large systems. We shall consider here two basic interconnections, cascade and parallel. Figure 15a shows (\\\\mathcal{S}{1}) and (\\\\mathcal{S}{2}), two LTIC subsystems connected in parallel, and Fig. 15b shows the same two systems connected in cascade.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='In Fig. 15a, the device depicted by the symbol (\\\\Sigma) inside a circle represents an adder, which adds signals at its inputs. Also the junction from which two (or more) branches radiate out is called the pickoff node. Every branch that radiates out from the pickoff node carries the same signal (the signal at the junction). In Fig. 15a, for instance, the junction at which the input is applied is a pickoff node from which two branches radiate out, each of which carries the input signal at the', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='node.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Let the impulse response of (\\\\mathcal{S}{1}) and (\\\\mathcal{S}{2}) be (h_{1}(t)) and (h_{2}(t)), respectively. Further assume that interconnecting these systems, as shown in Fig. 15, does not load them. This means that the impulse response of either of these systems remains unchanged whether observed when these systems are unconnected or when they are interconnected.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='To find (h_{p}(t)), the impulse response of the parallel system (\\\\mathcal{S}{p}) in Fig. 15a, we apply an impulse at the input of (\\\\mathcal{S}{p}). This results in the signal (\\\\delta(t)) at the inputs of (\\\\mathcal{S}{1}) and (\\\\mathcal{S}{2}), leading to their outputs (h_{1}(t)) and (h_{2}(t)), respectively. These signals are added by the adder to yield (h_{1}(t)+h_{2}(t)) as the output of (\\\\mathcal{S}_{p}):\\n\\n[h_{p}(t)=h_{1}(t)+h_{2}(t)]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='To find (h_{c}(t)), the impulse response of the cascade system (\\\\mathcal{S}{c}) in Fig. 15b, we apply the input (\\\\delta(t)) at the input of (\\\\mathcal{S}{c}), which is also the input to (\\\\mathcal{S}{1}). Hence, the output of (\\\\mathcal{S}{1}) is (h_{1}(t)), which now acts\\n\\nFigure 14: Intuitive explanation of convolution.\\n\\nas the input to (\\\\mathcal{S}{2}). The response of (\\\\mathcal{S}{2}) to input (h_{1}(t)) is (h_{1}(t)*h_{2}(t)). Therefore,\\n\\n[h_{c}(t)=h_{1}(t)*h_{2}(t)]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Because of the commutative property of convolution, it follows that interchanging the systems (\\\\mathcal{S}{1}) and (\\\\mathcal{S}{2}), as shown in Fig. 2.15c, results in the same impulse response (h_{1}(t)*h_{2}(t)). This means that when several LTIC systems are cascaded, the order of systems does not affect the impulse response of the composite system. In other words, linear operations, performed in cascade, commute. The order in which they are performed is not important, at least', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='theoretically.1', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Footnote 1: Change of order, however, could affect performance because of physical limitations and sensitivities to changes in the subsystems involved.\\n\\nFigure 2.15: Interconnected systems.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='We shall give here another interesting application of the commutative property of LTIC systems. Figure 2.15d shows a cascade of two LTIC systems: a system (\\\\mathcal{S}) with impulse response (h(t)), followed by an ideal integrator. Figure 2.15e shows a cascade of the same two systems in reverse order; an ideal integrator followed by (\\\\mathcal{S}). In Fig. 2.15d, if the input (x(t)) to (\\\\mathcal{S}) yields the output (y(t)), then the output of the system of Fig. 2.15d is the integral of (y(t)).', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='In Fig. 2.15e, the output of the integrator is the integral of (x(t)). The output in Fig. 2.15e is identical to the output in Fig. 2.15d. Hence, it follows that if an LTIC system response to input (x(t)) is (y(t)), then the response of the same system to the integral of (x(t)) is the integral of (y(t)). In other words,', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[\\\\text{if }x(t)\\\\Longrightarrow y(t)\\\\qquad\\\\text{then }\\\\int_{-\\\\infty}^{t}x(\\\\tau) \\\\,d\\\\tau\\\\Longrightarrow\\\\int_{-\\\\infty}^{t}y(\\\\tau)\\\\,d\\\\tau]\\n\\nReplacing the ideal integrator with an ideal differentiator in Figs. 2.15d and 2.15e, and following a similar argument, we conclude that\\n\\n[\\\\text{if }x(t)\\\\Longrightarrow y(t)\\\\qquad\\\\text{then }\\\\frac{dx(t)}{dt} \\\\Longrightarrow\\\\frac{dy(t)}{dt}]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='If we let (x(t)=\\\\delta(t)) and (y(t)=h(t)) in Fig. 2.15e, we find that (g(t)), the unit step response of an LTIC system with impulse (h(t)), is given by\\n\\n[g(t)=\\\\int_{-\\\\infty}^{t}h(\\\\tau)\\\\,d\\\\tau \\\\tag{2.35}]\\n\\nWe can also show that the system response to (\\\\dot{\\\\delta}(t)) is (dh(t)/dt). These results can be extended to other singularity functions. For example, the unit ramp response of an LTIC system is the integral of its unit step response, and so on.\\n\\n2.2 Inverse Systems', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='In Fig. 2.15b, if (\\\\mathcal{S}{1}) and (\\\\mathcal{S}{2}) are inverse systems with impulse response (h(t)) and (h_{i}(t)), respectively, then the impulse response of the cascade of these systems is (h(t)*h_{i}(t)). But, the cascade of a system with its inverse is an identity system, whose output is the same as the input. In other words, the unit impulse response of the cascade of inverse systems is also an unit impulse (\\\\delta(t)). Hence,\\n\\n[h(t)*h_{i}(t)=\\\\delta(t) \\\\tag{2.36}]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='We shall give an interesting application of the commutative property. As seen from Eq. (2.36), a cascade of inverse systems is an identity system. Moreover, in a cascade of several LTIC subsystems, changing the order of the subsystems in any manner does not affect the impulse response of the cascade system. Using these facts, we observe that the two systems, shown in Fig. 2.15f, are equivalent. We can compute the response of the cascade system on the right-hand side, by computing the response', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='of the system inside the dotted box to the input (\\\\dot{x}(t)). The impulse response of the dotted box is (g(t)), the integral of (h(t)), as given in Eq. (2.35). Hence, it follows that', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[y(t)=x(t)h(t)=\\\\dot{x}(t)g(t) \\\\tag{2.37}]\\n\\nRecall that (g(t)) is the unit step response of the system. Hence, an LTIC response can also be obtained as a convolution of (\\\\dot{x}(t)) (the derivative of the input) with the unit step response of the system. This result can be readily extended to higher derivatives of the input. An LTIC system response is the convolution of the (n)th derivative of the input with the (n)th integral of the impulse response.\\n\\nA Very Special Function for LTIC Systems:', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='The Everlasting Exponential (e^{st})', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"There is a very special connection of LTIC systems with the everlasting exponential function (e^{st}), where (s) is a complex variable, in general. We now show that the LTIC system's (zero-state) response to everlasting exponential input (e^{st}) is also the same everlasting exponential (within a multiplicative constant). Moreover, no other function can make the same claim. Such an input for which the system response is also of the same form is called the characteristic function (also\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='eigenfunction) of the system. Because a sinusoid is a form of exponential ((s=\\\\pm jo)), everlasting sinusoid is also a characteristic function of an LTIC system. Note that we are talking here of an everlasting exponential (or sinusoid), which starts at (t=-\\\\infty).', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"If (h(t)) is the system's unit impulse response, then system response (y(t)) to an everlasting exponential (e^{st}) is given by\\n\\n[y(t)=h(t)*e^{st}=\\\\int_{-\\\\infty}^{\\\\infty}h(\\\\tau)e^{s(t-\\\\tau)}\\\\,d\\\\tau=e^{st}\\\\int _{-\\\\infty}^{\\\\infty}h(\\\\tau)e^{-s\\\\tau}\\\\,d\\\\tau]\\n\\nThe integral on the right-most side is a function of a complex variable (s) and a constant with respect to (t). Let us denote this term by (H(s)), which is also complex, in general. Thus,\\n\\n[y(t)=H(s)e^{st} \\\\tag{38}]\\n\\nwhere\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[H(s)=\\\\int_{-\\\\infty}^{\\\\infty}h(\\\\tau)e^{-s\\\\tau}\\\\,d\\\\tau \\\\tag{39}]\\n\\nEquation (38) is valid only for the values of (s) for which (H(s)) exists, that is, if (\\\\int_{-\\\\infty}^{\\\\infty}h(\\\\tau)e^{-s\\\\tau}\\\\,d\\\\tau) exists (or converges). The region in the (s) plane for which this integral converges is called the region of convergence for (H(s)). Further elaboration of the region of convergence is presented in Ch. 4.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='For a given (s), note that (H(s)) is a constant. Thus, the input and the output are the same (within a multiplicative constant) for the everlasting exponential signal.\\n\\n(H(s)), which is called the transfer function of the system, is a function of complex variable (s). An alternate definition of the transfer function (H(s)) of an LTIC system, as seen from Eq. (38), is', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[H(s)=\\\\frac{\\\\mbox{output signal}}{\\\\mbox{input signal}}\\\\bigg{|}_{\\\\mbox{ input}=\\\\mbox{\\\\scriptsize everlasting exponential }e^{st}} \\\\tag{40}]\\n\\nThe transfer function is defined for, and is meaningful to, LTIC systems only. It does not exist for nonlinear or time-varying systems, in general.\\n\\nWe repeat again that this discussion is about the everlasting exponential, which starts at (t=-\\\\infty), not the causal exponential (e^{st}u(t)), which starts at (t=0).', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='For a system specified by Eq. (2.2), the transfer function is given by\\n\\n[H(s)=\\\\frac{P(s)}{Q(s)} \\\\tag{2.41}]\\n\\nThis follows readily by considering an everlasting input (x(t)=e^{st}). According to Eq. (2.38), the output is (y(t)=H(s)e^{st}). Substitution of this (x(t)) and (y(t)) in Eq. (2.2) yields\\n\\n[H(s)[Q(D)e^{st}]=P(D)e^{st}]\\n\\nMoreover,\\n\\n[D^{\\\\prime}e^{st}=\\\\frac{d^{\\\\prime}e^{st}}{dt^{\\\\prime}}=s^{\\\\prime}e^{st}]\\n\\nHence,\\n\\n[P(D)e^{st}=P(s)e^{st}\\\\qquad\\\\text{and}\\\\qquad Q(D)e^{st}=Q(s)e^{st}]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Consequently,\\n\\n[H(s)=\\\\frac{P(s)}{Q(s)}]\\n\\n2.14 Ideal Integrator and Differentiator Transfer Functions\\n\\nShow that the transfer function of an ideal integrator is (H(s)=1/s) and that of an ideal differentiator is (H(s)=s). Find the answer in two ways: using Eq. (2.39) and using Eq. (2.41).\\n\\n[Hint: Find (h(t)) for the ideal integrator and differentiator. You also may need to use the result in Prob. 1.4-12.]\\n\\n2.15 A Fundamental Property of LTI Systems', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='We can show that Eq. (2.38) is a fundamental property of LTI systems and it follows directly as a consequence of linearity and time invariance. To show this let us assume that the response of an LTI system to an everlasting exponential (e^{st}) is (y(s,t)). If we define\\n\\n[H(s,t)=\\\\frac{y(s,t)}{e^{st}}]\\n\\nthen\\n\\n[y(s,t)=H(s,t)\\\\,e^{st}]\\n\\nBecause of the time-invariance property, the system response to input (e^{s(t-T)}) is (H(s,t-T)\\\\,e^{s(t-T)}), that is,\\n\\n[y(s,t-T)=H(s,t-T)\\\\,e^{s(t-T)} \\\\tag{2.42}]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='The delayed input (e^{st}) represents the input (e^{st}) multiplied by a constant (e^{-sT}). Hence, according to the linearity property, the system response to (e^{s(t-T)}) must be (y(s,t)\\\\,e^{-sT}). Hence,\\n\\n[y(s,t-T)=y(s,t)\\\\,e^{-sT}=H(s,t)\\\\,e^{s(t-T)}]Comparison of this result with Eq. (42) shows that\\n\\n[H(s,t)=H(s,t-T)\\\\qquad\\\\text{for all}\\\\,T]\\n\\nThis means (H(s,t)) is independent of (t), and we can express (H(s,t)=H(s)). Hence,\\n\\n[y(s,t)=H(s)\\\\,e^{st}]\\n\\nTotal Response', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Assuming distinct roots, the total response of a linear system can be expressed as the sum of its zero-input response (ZIR) and its zero-state response (ZSR):\\n\\n[\\\\text{total response}=\\\\underbrace{\\\\sum_{k=1}^{N}c_{k}e^{\\\\lambda_{k}t}}{\\\\text{ZIR }}+\\\\underbrace{x(t)*h(t)}{\\\\text{ZSR}}]\\n\\nFor repeated roots, the zero-input component should be appropriately modified.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='For the series (RLC) circuit in Ex. 4 with the input (x(t)=10e^{-3t}u(t)) and the initial conditions (y(0^{-})=0,v_{C}(0^{-})=5), we determined the zero-input response in Ex. 1a [Eq. (9)]. We found the zero-state response in Ex. 9. From the results in Exs. 1a and 9, we obtain\\n\\n[\\\\text{total current}=\\\\underbrace{(-5e^{-t}+5e^{-2t})}{\\\\text{zero-input current}}+\\\\underbrace{(-5e^{-t}+20e^{-2t}-15e^{-3t})}{\\\\text{zero-state current}}\\\\qquad t\\\\geq 0 \\\\tag{43}]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Figure 16a shows the zero-input, zero-state, and total responses.\\n\\nFigure 16: Total response and its components.\\n\\n[MISSING_PAGE_EMPTY:47]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='away from the original state. Thus it is said to be in a neutral equilibrium. Clearly, when a system is in stable equilibrium, application of a small disturbance (input) produces a small response. In contrast, when the system is in unstable equilibrium, even a minuscule disturbance (input) produces an unbounded response. The BIBO-stability definition can be understood in the light of this concept. If every bounded input produces bounded output, the system is (BIBO) stable.2 In contrast, if even', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='one bounded input results in unbounded response, the system is (BIBO) unstable.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Footnote 2: The system is assumed to be in zero state.\\n\\nFor an LTIC system,\\n\\n[y(t)=h(t)*x(t)=\\\\int_{-\\\\infty}^{\\\\infty}h(\\\\tau)x(t-\\\\tau)\\\\,d\\\\tau]\\n\\nTherefore,\\n\\n[|y(t)|\\\\leq\\\\int_{-\\\\infty}^{\\\\infty}|h(\\\\tau)||x(t-\\\\tau)|\\\\,d\\\\tau]\\n\\nMoreover, if (x(t)) is bounded, then (|x(t-\\\\tau)|<K_{1}<\\\\infty), and\\n\\n[|y(t)|\\\\leq K_{1}\\\\int_{-\\\\infty}^{\\\\infty}|h(\\\\tau)|\\\\,d\\\\tau]\\n\\nHence for BIBO stability,\\n\\n[\\\\int_{-\\\\infty}^{\\\\infty}|h(\\\\tau)|\\\\,d\\\\tau<\\\\infty \\\\tag{45}]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='This is a sufficient condition for BIBO stability. We can show that this is also a necessary condition (see Prob. 2.5-7). Therefore, for an LTIC system, if its impulse response (h(t)) is absolutely integrable, the system is (BIBO) stable. Otherwise it is (BIBO) unstable. In addition, we shall show in Ch. 4 that a necessary (but not sufficient) condition for an LTIC system described by Eq. (1) to be BIBO-stable is (M\\\\leq N). If (M>N), the system is unstable. This is one of the reasons to avoid', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='systems with (M>N).', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Because the BIBO stability of a system can be ascertained by measurements at the external terminals (input and output), this is an external stability criterion. It is no coincidence that the BIBO criterion in Eq. (45) is in terms of the impulse response, which is an external description of the system.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='As observed in Sec. 1.9, the internal behavior of a system is not always ascertainable from the external terminals. Therefore, external (BIBO) stability may not be a correct indication of internal stability. Indeed, some systems that appear stable by the BIBO criterion may be internally unstable. This is like a room on fire inside a house: no trace of fire is visible from outside, but the entire house will be burned to ashes.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='The BIBO stability is meaningful only for systems in which the internal and the external description are equivalent (controllable and observable systems). Fortunately, most practical systems fall into this category, and whenever we apply this criterion, we implicitly assume that the system, in fact, belongs to this category. Internal stability is all-inclusive, and external stability can always be determined from internal stability. For this reason, we now investigate the internal stability', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='criterion.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='2.5-2 Internal (Asymptotic) Stability\\n\\nBecause of the great variety of possible system behaviors, there are several definitions of internal stability in the literature. Here we shall consider a definition that is suitable for causal, linear, time-invariant (LTI) systems.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='If, in the absence of an external input, a system remains in a particular state (or condition) indefinitely, then that state is said to be an equilibrium state of the system. For an LTI system, zero state, in which all initial conditions are zero, is an equilibrium state. Now suppose an LTI system is in zero state and we change this state by creating small nonzero initial conditions (small disturbance). These initial conditions will generate signals consisting of characteristic modes in the', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='system. By analogy with the cone, if the system is stable, it should eventually return to zero state. In other words, when left to itself, every mode in a stable system arising as a result of nonzero initial conditions should approach (0) as (t\\\\to\\\\infty). However, if even one of the modes grows with time, the system will never return to zero state, and the system would be identified as unstable. In the borderline case, some modes neither decay to zero nor grow indefinitely, while all the', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='remaining modes decay to zero. This case is like the neutral equilibrium in the cone. Such a system is said to be marginally stable. Internal stability is also called asymptotic stability or stability in the sense of Lyapunov.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='For a system characterized by Eq. (2.1), we can restate the internal stability criterion in terms of the location of the (N) characteristic roots (\\\\lambda_{1}), (\\\\lambda_{2}), (\\\\ldots), (\\\\lambda_{N}) of the system in a complex plane. The characteristic modes are of the form (e^{\\\\lambda_{k}t}) or (t^{\\\\prime}e^{\\\\lambda_{k}t}). The locations of various roots in the complex plane and the corresponding modes are shown in Fig. 2.17. These modes (\\\\to 0) as (t\\\\to\\\\infty) if Re (\\\\lambda_{k}<0). In', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='contrast, the modes (\\\\to\\\\infty) as (t\\\\to\\\\infty) if Re (\\\\lambda_{k}>0).+', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Footnote †: This conclusion is also valid for the terms of the form (t^{\\\\prime}e^{\\\\lambda_{k}t}).', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='From Fig. 2.17, we see that a system is (asymptotically) stable if all its characteristic roots lie in the LHP, that is, if Re (\\\\lambda_{k}<0) for all (k). If even a single characteristic root lies in the RHP, the system is (asymptotically) unstable. Modes due to roots on the imaginary axis ((\\\\lambda=\\\\pm j\\\\omega_{0})) are of the form (e^{\\\\pm j\\\\omega_{0}t}). Hence, if some roots are on the imaginary axis, and all the remaining roots are in the LHP, the system is marginally stable (assuming that', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='the roots on the imaginary axis are not repeated). If the imaginary axis roots are repeated, the characteristic modes are of the form (t^{\\\\prime}e^{\\\\pm j\\\\omega_{0}t}), which do grow with time indefinitely. Hence, the system is unstable. Figure 2.18 shows stability regions in the complex plane.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='To summarize:\\n\\nAn LTIC system is asymptotically stable if, and only if, all the characteristic roots are in the LHP. The roots may be simple (unrepeated) or repeated.\\n\\nAn LTIC system is unstable if, and only if, one or both of the following conditions exist: (i) at least one root is in the RHP; (ii) there are repeated roots on the imaginary axis.\\n\\nAn LTIC system is marginally stable if, and only if, there are no roots in the RHP, and there are some unrepeated roots on the imaginary axis.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='2.5-3 Relationship Between BIBO and Asymptotic Stability\\n\\nExternal stability is determined by applying an external input with zero initial conditions, while internal stability is determined by applying the nonzero initial conditions and no external input. This is why these stabilities are also called the zero-state stability and the zero-input stability, respectively.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Recall that (h(t)), the impulse response of an LTIC system, is a linear combination of the system characteristic modes. For an LTIC system, specified by Eq. (2.1), we can readily show that when a characteristic root (\\\\lambda_{k}) is in the LHP, the corresponding mode (e^{\\\\lambda_{k}t}) is absolutely integrable. In\\n\\nFigure 2.17: Location of characteristic roots and the corresponding characteristic modes.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='contrast, if (\\\\lambda_{k}) is in the RHP or on the imaginary axis, (e^{\\\\lambda_{k}t}) is not absolutely integrable.2 This means that an asymptotically stable system is BIBO-stable. Moreover, a marginally stable or asymptotically unstable system is BIBO-unstable. The converse is not necessarily true; that is, BIBO stability does not necessarily inform us about the internal stability of the system. For instance, if a system is uncontrollable and/or unobservable, some modes of the system are', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='invisible and/or uncontrollable from the external terminals [3]. Hence, the stability picture portrayed by the external description is of questionable value. BIBO (external) stability cannot assure internal (asymptotic) stability, as the following example shows.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Footnote 2: Consider a mode of the form (e^{\\\\lambda_{t}}), where (\\\\lambda=\\\\alpha+j\\\\beta). Hence, (e^{\\\\lambda_{t}}=e^{\\\\alpha t}e^{j\\\\beta t}) and (|e^{\\\\lambda_{t}}|=e^{\\\\alpha t}). Therefore,\\n\\n[\\\\int_{-\\\\infty}^{\\\\infty}|e^{\\\\lambda_{t}}u(\\\\tau)|\\\\,d\\\\tau=\\\\int_{0}^{\\\\infty}e^{ \\\\alpha\\\\tau}\\\\,d\\\\tau=\\\\begin{cases}-1/\\\\alpha&\\\\alpha<0\\\\ \\\\infty&\\\\alpha\\\\geq 0\\\\end{cases}]\\n\\nThis conclusion is also valid when the integrand is of the form (|t^{k}e^{\\\\lambda_{t}}u(t)|).', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='The composite system impulse response (h(t)) is given by\\n\\n[h(t)=h_{1}(t)h_{2}(t)=h_{2}(t)h_{1}(t) =e^{t}u(t)*[\\\\delta(t)-2e^{-t}u(t)]] [=e^{t}u(t)-2\\\\Bigg{[}\\\\frac{e^{t}-e^{-t}}{2}\\\\Bigg{]}u(t)] [=e^{-t}u(t)]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='If the composite cascade system were to be enclosed in a black box with only the input and the output terminals accessible, any measurement from these external terminals would show that the impulse response of the system is (e^{-t}u(t)), without any hint of the dangerously unstable system the system is harboring within.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='The composite system is BIBO-stable because its impulse response, (e^{-t}u(t)), is absolutely integrable. Observe, however, the subsystem (\\\\mathcal{S}{2}) has a characteristic root 1, which lies in the RHP. Hence, (\\\\mathcal{S}{2}) is asymptotically unstable. Eventually, (\\\\mathcal{S}{2}) will burn out (or saturate) because of the unbounded characteristic response generated by intended or unintended initial conditions, no matter how small. We shall show in Ex. 10.12 that this composite system is', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='observable, but not controllable. If the positions of (\\\\mathcal{S}{1}) and (\\\\mathcal{S}{2}) were interchanged ((\\\\mathcal{S}{2}) followed by (\\\\mathcal{S}_{1})), the system is still BIBO-stable, but asymptotically unstable. In this case, the analysis in Ex. 10.12 shows that the composite system is controllable, but not observable.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='This example shows that BIBO stability does not always imply asymptotic stability. However, asymptotic stability always implies BIBO stability.\\n\\nFortunately, uncontrollable and/or unobservable systems are not commonly observed in practice. Henceforth, in determining system stability, we shall assume that unless otherwise mentioned, the internal and the external descriptions of a system are equivalent, implying that the system is controllable and observable.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Example 2.14 Investigating Asymptotic and BIBO Stability\\n\\nInvestigate the asymptotic and the BIBO stability of LTIC system described by the following equations, assuming that the equations are internal system descriptions:\\n\\n((D+1)(D^{2}+4D+8)y(t)=(D-3)x(t))\\n\\n((D-1)(D^{2}+4D+8)y(t)=(D+2)x(t))\\n\\n((D+2)(D^{2}+4)y(t)=(D^{2}+D+1)x(t))\\n\\n((D+1)(D^{2}+4)^{2}y(t)=(D^{2}+2D+8)x(t))\\n\\nFigure 2.19: Composite system for Ex. 2.13.\\n\\nThe characteristic polynomials of these systems are', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[\\\\begin{array}{ll}\\\\mbox{\\\\bf(a)}&\\\\mbox{\\\\bf($\\\\lambda+1$)($\\\\lambda^{2}+4\\\\lambda+8$) }=(\\\\lambda+1)(\\\\lambda+2-j2)(\\\\lambda+2+j2)\\\\ \\\\mbox{\\\\bf($\\\\lambda-1$)($\\\\lambda^{2}+4\\\\lambda+8$)}=(\\\\lambda-1)(\\\\lambda+2-j2)( \\\\lambda+2+j2)\\\\ \\\\mbox{\\\\bf(c)}&\\\\mbox{\\\\bf($\\\\lambda+2$)($\\\\lambda^{2}+4$)}=(\\\\lambda+2)(\\\\lambda-j2 )(\\\\lambda+j2)\\\\ \\\\mbox{\\\\bf(d)}&\\\\mbox{\\\\bf($\\\\lambda+1$)($\\\\lambda^{2}+4$)}^{2}=(\\\\lambda+2)(\\\\lambda -j2)^{2}(\\\\lambda+j2)^{2}\\\\end{array}]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Consequently, the characteristic roots of the systems are (see Fig. 2.20):\\n\\n[\\\\begin{array}{ll}\\\\mbox{\\\\bf(a)}&-1,\\\\,-2\\\\pm j2\\\\ \\\\mbox{\\\\bf(b)}&1,\\\\,-2\\\\pm j2\\\\ \\\\mbox{\\\\bf(c)}&-2,\\\\,\\\\pm j2\\\\ \\\\mbox{\\\\bf(d)}&-1,\\\\,\\\\pm j2,\\\\,\\\\pm j2\\\\end{array}]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='System (a) is asymptotically stable (all roots in LHP), system (b) is unstable (one root in RHP), system (c) is marginally stable (unrepeated roots on imaginary axis) and no roots in RHP, and system (d) is unstable (repeated roots on the imaginary axis). BIBO stability is readily determined from the asymptotic stability. System (a) is BIBO-stable, system (b) is BIBO-unstable, system (c) is BIBO-unstable, and system (d) is BIBO-unstable. We have assumed that these systems are controllable and', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='observable.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='2.15 Assessing Stability by Characteristic Roots\\n\\nFor each case, plot the characteristic roots and determine asymptotic and BIBO stabilities. Assume the equations reflect internal descriptions.\\n\\n(a): (D(D+2)y(t)=3x(t))\\n(b): (D^{2}(D+3)y(t)=(D+5)x(t))\\n(c): ((D+1)(D+2)y(t)=(2D+3)x(t))\\n(d): ((D^{2}+1)(D^{2}+9)y(t)=(D^{2}+2D+4)x(t))\\n(e): ((D+1)(D^{2}-4D+9)y(t)=(D+7)x(t))\\n\\nFigure 2.20: Characteristic root locations for the systems of Ex. 2.14.\\n\\nImplications of Stability', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='All practical signal-processing systems must be asymptotically stable. Unstable systems are useless from the viewpoint of signal processing because any set of intended or unintended initial conditions leads to an unbounded response that either destroys the system or (more likely) leads it to some saturation conditions that change the nature of the system. Even if the discernible initial conditions are zero, stray voltages or thermal noise signals generated within the system will act as initial', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='conditions. Because of exponential growth of a mode or modes in unstable systems, a stray signal, no matter how small, will eventually cause an unbounded output.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Marginally stable systems, though BIBO unstable, do have one important application in the oscillator, which is a system that generates a signal on its own without the application of an external input. Consequently, the oscillator output is a zero-input response. If such a response is to be a sinusoid of frequency (\\\\omega_{0}), the system should be marginally stable with characteristic roots at (\\\\pm\\\\dot{y}\\\\omega_{0}). Thus, to design an oscillator of frequency (\\\\omega_{0}), we should pick a', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='system with the characteristic polynomial ((\\\\lambda-j\\\\omega_{0})(\\\\lambda+j\\\\omega_{0})=\\\\lambda^{2}+{\\\\omega_{0}}^{2}). A system described by the differential equation', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[\\\\big{(}D^{2}+{\\\\omega_{0}}^{2}\\\\big{)}y(t)=x(t)]\\n\\nwill do the job. However, practical oscillators are invariably realized using nonlinear systems.\\n\\n2.6 Intuitive Insights into System Behavior', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='This section attempts to provide an understanding of what determines system behavior. Because of its intuitive nature, the discussion is more or less qualitative. We shall now show that the most important attributes of a system are its characteristic roots or characteristic modes because they determine not only the zero-input response but also the entire behavior of the system.\\n\\nDependence of System Behavior on Characteristic Modes', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"Recall that the zero-input response of a system consists of the system's characteristic modes. For a stable system, these characteristic modes decay exponentially and eventually vanish. This behavior may give the impression that these modes do not substantially affect system behavior in general and system response in particular. This impression is totally wrong! We shall now see that the system's characteristic modes leave their imprint on every aspect of the system behavior. We may compare the\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"system's characteristic modes (or roots) to a seed that eventually dissolves in theground; however, the plant that springs from it is totally determined by the seed. The imprint of the seed exists on every cell of the plant.\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='To understand this interesting phenomenon, recall that the characteristic modes of a system are very special to that system because it can sustain these signals without the application of an external input. In other words, the system offers a free ride and ready access to these signals. Now imagine what would happen if we actually drove the system with an input having the form of a characteristic mode! We would expect the system to respond strongly (this is, in fact, the resonance phenomenon', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='discussed later in this section). If the input is not exactly a characteristic mode but is close to such a mode, we would still expect the system response to be strong. However, if the input is very different from any of the characteristic modes, we would expect the system to respond poorly. We shall now show that these intuitive deductions are indeed true.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"We have devised a measure of similarity of signals later (see in Ch. 6). Here we shall take a simpler approach. Let us restrict the system's inputs to exponentials of the form (e^{\\\\zeta t}), where (\\\\zeta) is generally a complex number. The similarity of two exponential signals (e^{\\\\zeta t}) and (e^{\\\\lambda t}) will then be measured by the closeness of (\\\\zeta) and (\\\\lambda). If the difference (\\\\zeta-\\\\lambda) is small, the signals are similar; if (\\\\zeta-\\\\lambda) is large, the signals are\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='dissimilar.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Now consider a first-order system with a single characteristic mode (e^{\\\\lambda t}) and the input (e^{\\\\zeta t}). The impulse response of this system is then given by (Ae^{\\\\lambda t}), where the exact value of (A) is not important for this qualitative discussion. The system response (y(t)) is given by\\n\\n[y(t)=h(t)x(t)=Ae^{\\\\lambda t}u(t)e^{\\\\zeta t}u(t)]\\n\\nFrom the convolution table (Table 2.1), we obtain', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[y(t)=\\\\frac{A}{\\\\zeta-\\\\lambda}[e^{\\\\zeta t}-e^{\\\\lambda t}]u(t) \\\\tag{2.46}]Clearly, if the input (e^{\\\\xi t}) is similar to (e^{\\\\lambda t}), (\\\\zeta-\\\\lambda) is small and the system response is large. The closer the input (x(t)) to the characteristic mode, the stronger the system response. In contrast, if the input is very different from the natural mode, (\\\\zeta-\\\\lambda) is large and the system responds poorly. This is precisely what we set out to prove.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='We have proved the foregoing assertion for a single-mode (first-order) system. It can be generalized to an (N)th-order system, which has (N) characteristic modes. The impulse response (h(t)) of such a system is a linear combination of its (N) modes. Therefore, if (x(t)) is similar to any one of the modes, the corresponding response will be high; if it is similar to none of the modes, the response will be small. Clearly, the characteristic modes are very influential in determining system', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='response to a given input.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='It would be tempting to conclude on the basis of Eq. (2.46) that if the input is identical to the characteristic mode, so that (\\\\zeta=\\\\lambda), then the response goes to infinity. Remember, however, that if (\\\\zeta=\\\\lambda), the numerator on the right-hand side of Eq. (2.46) also goes to zero. We shall study this interesting behavior (resonance phenomenon) later in this section.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='We now show that mere inspection of the impulse response (h(t)) (which is composed of characteristic modes) reveals a great deal about the system behavior.\\n\\nResponse Time of a System: The System Time Constant', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"Like human beings, systems have a certain response time. In other words, when an input (stimulus) is applied to a system, a certain amount of time elapses before the system fully responds to that input. This time lag or response time is called the system time constant. As we shall see, a system's time constant is equal to the width of its impulse response (h(t)).\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"An input (\\\\delta(t)) to a system is instantaneous (zero duration), but its response (h(t)) has a duration (T_{h}). Therefore, the system requires a time (T_{h}) to respond fully to this input, and we are justified in viewing (T_{h}) as the system's response time or time constant. We arrive at the same conclusion via another argument. The output is a convolution of the input with (h(t)). If an input is a pulse of width (T_{x}), then the output pulse width is (T_{x}+T_{h}) according to the width\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='property of convolution. This conclusion shows that the system requires (T_{h}) seconds to respond fully to any input. The system time constant indicates how fast the system is. A system with a smaller time constant is a faster system that responds quickly to an input. A system with a relatively large time constant is a sluggish system that cannot respond well to rapidly varying signals.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"Strictly speaking, the duration of the impulse response (h(t)) is (\\\\infty) because the characteristic modes approach zero asymptotically as (t\\\\to\\\\infty). However, beyond some value of (t), (h(t)) becomes negligible. It is therefore necessary to use some suitable measure of the impulse response's effective width.\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='There is no single satisfactory definition of effective signal duration (or width) applicable to every situation. For the situation depicted in Fig. 2.21, a reasonable definition of the duration (h(t)) would be (T_{h}), the width of the rectangular pulse (\\\\hat{h}(t)). This rectangular pulse (\\\\hat{h}(t)) has an area identical to that of (h(t)) and a height identical to that of (h(t)) at some suitable instant (t=t_{0}). In Fig. 2.21, (t_{0}) is chosen as the instant at which (h(t)) is maximum.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='According to this definition,1', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Footnote 1: This definition is satisfactory when (h(t)) is a single, mostly positive (or mostly negative) pulse. Such systems are lowpass systems. This definition should not be applied indiscriminately to all systems.\\n\\n[T_{h}h(t_{0})=\\\\int_{-\\\\infty}^{\\\\infty}h(t)\\\\,dt]or\\n\\n[T_{h}=\\\\frac{\\\\int_{-\\\\infty}^{\\\\infty}h(t)\\\\,dt}{h(t_{0})} \\\\tag{2.47}]\\n\\nNow if a system has a single mode\\n\\n[h(t)=Ae^{\\\\lambda t}u(t)]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='with (\\\\lambda) negative and real, then (h(t)) is maximum at (t=0) with value (h(0)=A). Therefore, according to Eq. (2.47),\\n\\n[T_{h}=\\\\frac{1}{A}\\\\int_{0}^{\\\\infty}Ae^{\\\\lambda t}\\\\,dt=-\\\\frac{1}{\\\\lambda}]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"Thus, the time constant in this case is simply the (negative of the) reciprocal of the system's characteristic root. For the multimode case, (h(t)) is a weighted sum of the system's characteristic modes, and (T_{h}) is a weighted average of the time constants associated with the (N) modes of the system.\\n\\n2.6-3 Time Constant and Rise Time of a System\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Rise time of a system, defined as the time required for the unit step response to rise from 10% to 90% of its steady-state value, is an indication of the speed of response.2 The system time constant may also be viewed from a perspective of rise time. The unit step response (y(t)) of a system is the convolution of (u(t)) with (h(t)). Let the impulse response (h(t)) be a rectangular pulse of width (T_{h}), as shown in Fig. 2.22. This assumption simplifies the discussion, yet gives satisfactory', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='results for qualitative discussion. The result of this convolution is illustrated in Fig. 2.22. Note that the output does not rise from zero to a final value instantaneously as the input rises; instead, the output takes (T_{h}) seconds to accomplish this. Hence, the rise time (T_{r}) of the system is equal to the system time constant', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Footnote 2: Because of varying definitions of rise time, the reader may find different results in the literature. The qualitative and intuitive nature of this discussion should always be kept in mind.\\n\\n[T_{r}=T_{h}]\\n\\nThis result and Fig. 2.22 show clearly that a system generally does not respond to an input instantaneously. Instead, it takes time (T_{h}) for the system to respond fully.\\n\\nFigure 2.21: Effective duration of an impulse response.\\n\\n2.6.4 Time Constant and Filtering', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"A larger time constant implies a sluggish system because the system takes longer to respond fully to an input. Such a system cannot respond effectively to rapid variations in the input. In contrast, a smaller time constant indicates that a system is capable of responding to rapid variations in the input. Thus, there is a direct connection between a system's time constant and its filtering properties.\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='A high-frequency sinusoid varies rapidly with time. A system with a large time constant will not be able to respond well to this input. Therefore, such a system will suppress rapidly varying (high-frequency) sinusoids and other high-frequency signals, thereby acting as a lowpass filter (a filter allowing the transmission of low-frequency signals only). We shall now show that a system\\n\\nFigure 2.23. Time constant and filtering.\\n\\nFigure 2.22. Rise time of a system.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='with a time constant (T_{h}) acts as a lowpass filter having a cutoff frequency of (f_{c}=1/T_{h}) hertz, so that sinusoids with frequencies below (f_{c}) Hz are transmitted reasonably well, while those with frequencies above (f_{c}) Hz are suppressed.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='To demonstrate this fact, let us determine the system response to a sinusoidal input (x(t)) by convolving this input with the effective impulse response (h(t)) in Fig. 23a. From Figs. 23b and 23c we see the process of convolution of (h(t)) with the sinusoidal inputs of two different frequencies. The sinusoid in Fig. 23b has a relatively high frequency, while the frequency of the sinusoid in Fig. 23c is low. Recall that the convolution of (x(t)) and (h(t)) is equal to the area under the product', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='(x(\\\\tau)h(t-\\\\tau)). This area is shown shaded in Figs. 23b and 23c for the two cases. For the high-frequency sinusoid, it is clear from Fig. 23b that the area under (x(\\\\tau)h(t-\\\\tau)) is very small because its positive and negative areas nearly cancel each other out. In this case the output (y(t)) remains periodic but has a rather small amplitude. This happens when the period of the sinusoid is much smaller than the system time constant (T_{h}). In contrast, for the low-frequency sinusoid, the', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='period of the sinusoid is larger than (T_{h}), rendering the partial cancellation of area under (x(\\\\tau)h(t-\\\\tau)) less effective. Consequently, the output (y(t)) is much larger, as depicted in Fig. 23c.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Between these two possible extremes in system behavior, a transition point occurs when the period of the sinusoid is equal to the system time constant (T_{h}). The frequency at which this transition occurs is known as the cutoff frequency(f_{c}) of the system. Because (T_{h}) is the period of cutoff frequency (f_{c}),\\n\\n[f_{c}=\\\\frac{1}{T_{h}}]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='The frequency (f_{c}) is also known as the bandwidth of the system because the system transmits or passes sinusoidal components with frequencies below (f_{c}) while attenuating components with frequencies above (f_{c}). Of course, the transition in system behavior is gradual. There is no dramatic change in system behavior at (f_{c}=1/T_{h}). Moreover, these results are based on an idealized (rectangular pulse) impulse response; in practice these results will vary somewhat, depending on the', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='exact shape of (h(t)). Remember that the \"feel\" of general system behavior is more important than exact system response for this qualitative discussion.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"Since the system time constant is equal to its rise time, we have\\n\\n[T_{r}=\\\\frac{1}{f_{c}}\\\\qquad\\\\mbox{or}\\\\qquad f_{c}=\\\\frac{1}{T_{r}} \\\\tag{48}]\\n\\nThus, a system's bandwidth is inversely proportional to its rise time. Although Eq. (48) was derived for an idealized (rectangular) impulse response, its implications are valid for lowpass LTIC systems, in general. For a general case, we can show that [1]\\n\\n[f_{c}=\\\\frac{k}{T_{r}}]\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='where the exact value of (k) depends on the nature of (h(t)). An experienced engineer often can estimate quickly the bandwidth of an unknown system by simply observing the system response to a step input on an oscilloscope.\\n\\nTime Constant and Pulse Dispersion (Spreading)', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='In general, the transmission of a pulse through a system causes pulse dispersion (or spreading). Therefore, the output pulse is generally wider than the input pulse. This system behavior can have serious consequences in communication systems in which information is transmitted by pulse amplitudes. Dispersion (or spreading) causes interference or overlap with neighboring pulses, thereby distorting pulse amplitudes and introducing errors in the received information.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"Earlier we saw that if an input (x(t)) is a pulse of width (T_{x}), then (T_{y}), the width of the output (y(t)), is\\n\\n[T_{y}=T_{x}+T_{h}]\\n\\nThis result shows that an input pulse spreads out (disperses) as it passes through a system. Since (T_{h}) is also the system's time constant or rise time, the amount of spread in the pulse is equal to the time constant (or rise time) of the system.\\n\\nTime Constant and Rate of Information Transmission\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='In pulse communications systems, which convey information through pulse amplitudes, the rate of information transmission is proportional to the rate of pulse transmission. We shall demonstrate that to avoid the destruction of information caused by dispersion of pulses during their transmission through the channel (transmission medium), the rate of information transmission should not exceed the bandwidth of the communications channel.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"Since an input pulse spreads out by (T_{h}) seconds, the consecutive pulses should be spaced (T_{h}) seconds apart to avoid interference between pulses. Thus, the rate of pulse transmission should not exceed (1/T_{h}) pulses/second. But (1/T_{h}=f_{c}), the channel's bandwidth, so that we can transmit pulses through a communications channel at a rate of (f_{c}) pulses per second and still avoid significant interference between the pulses. The rate of information transmission is therefore\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"proportional to the channel's bandwidth (or to the reciprocal of its time constant).1\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Footnote 1: Theoretically, a channel of bandwidth (f_{c}) can transmit correctly up to (2f_{c}) pulse amplitudes per second [4]. Our derivation here, being very simple and qualitative, yields only half the theoretical limit. In practice it is not easy to attain the upper theoretical limit.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"The discussion of Secs. 2.6-2, 2.6-3, 2.6-4, 2.6-5, and 2.6-6) shows that the system time constant determines much of a system's behavior--its filtering characteristics, rise time, pulse dispersion, and so on. In turn, the time constant is determined by the system's characteristic roots. Clearly the characteristic roots and their relative amounts in the impulse response (h(t)) determine the behavior of a system.\\n\\nExample 2.15 Intuitive Insights into Lowpass System Behavior\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Find the time constant (T_{h}), rise time (T_{r}), and cutoff frequency (f_{c}) for a lowpass system that has impulse response (h(t)=te^{-t}u(t)). Determine the maximum rate that pulses of 1 secondduration can be transmitted through the system so that interference is essentially avoided between adjacent pulses at the system output.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='The system impulse response (h(t)=te^{-t}u(t)), which looks similar to the impulse response of Fig. 2.21, has a peak value of (e^{-1}=0.3679) at a time (t_{0}=1). According to Eq. (2.47) and using integration by parts, the system time constant is therefore\\n\\n[T_{h}=\\\\frac{\\\\int_{0}^{\\\\infty}te^{-t}dt}{e^{-1}}=e^{1}\\\\left(-te^{-t}\\\\big{|}{0} ^{\\\\infty}+\\\\int{0}^{\\\\infty}e^{-t}dt\\\\right)=e^{1}\\\\left(0-e^{-t}\\\\big{|}_{0}^{ \\\\infty}\\\\right)=e^{1}(1)=2.7183]\\n\\nThus,', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[T_{h}=2.7183\\\\ \\\\text{s},\\\\qquad T_{r}=T_{h}=2.7183\\\\ \\\\text{s},\\\\qquad\\\\text{and} \\\\qquad f_{c}=\\\\frac{1}{T_{h}}=0.3679\\\\ \\\\text{Hz}]\\n\\nDue to its lowpass nature, this system will spread an input pulse of 1 second to an output with width\\n\\n[T_{y}=T_{x}+T_{h}=1+2.7183=3.7183\\\\ \\\\text{s}]\\n\\nTo avoid interference between pulses at the output, the pulse transmission rate should be no more than the reciprocal of the output pulse width. That is,', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[\\\\text{maximum pulse transmission rate}=\\\\frac{1}{3.7183}=0.2689\\\\ \\\\text{pulse/s}]\\n\\nBy narrowing the input pulses, the pulse transmission rate could increase up to (f_{c}=0.3679) pulse/s.\\n\\n2.6-7 The Resonance Phenomenon', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Finally, we come to the fascinating phenomenon of resonance. As we have already mentioned several times, this phenomenon is observed when the input signal is identical or is very close to a characteristic mode of the system. For the sake of simplicity and clarity, we consider a first-order system having only a single mode, (e^{\\\\lambda t}). Let the impulse response of this system be1', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Footnote 1: For convenience, we omit multiplying (x(t)) and (h(t)) by (u(t)). Throughout this discussion, we assume that they are causal.\\n\\n[h(t)=Ae^{\\\\lambda t}]\\n\\nand let the input be\\n\\n[x(t)=e^{(\\\\lambda-\\\\epsilon)t}]\\n\\nThe system response (y(t)) is then given by\\n\\n[y(t)=Ae^{\\\\lambda t}*e^{(\\\\lambda-\\\\epsilon)t}]From the convolution table we obtain\\n\\n[y(t)=\\\\frac{A}{\\\\epsilon}\\\\Big{[}e^{\\\\lambda t}-e^{(\\\\lambda-\\\\epsilon)t}\\\\Big{]}=Ae^{ \\\\lambda t}\\\\left(\\\\frac{1-e^{-\\\\epsilon t}}{\\\\epsilon}\\\\right) \\\\tag{2.49}]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"Now, as (\\\\epsilon\\\\to 0), both the numerator and the denominator of the term in the parentheses approach zero. Applying L'Hopital's rule to this term yields\\n\\n[\\\\lim_{\\\\epsilon\\\\to 0}y(t)=Ate^{\\\\lambda t}]\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"Clearly, the response does not go to infinity as (\\\\epsilon\\\\to 0), but it acquires a factor (t), which approaches (\\\\infty) as (t\\\\to\\\\infty). If (\\\\lambda) has a negative real part (so that it lies in the LHP), (e^{\\\\lambda t}) decays faster than (t) and (y(t)\\\\to 0) as (t\\\\to\\\\infty). The resonance phenomenon in this case is present, but its manifestation is aborted by the signal's own exponential decay.\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='This discussion shows that resonance is a cumulative phenomenon, not instantaneous. It builds up linearly with (t).+ When the mode decays exponentially, the signal decays too fast for resonance to counteract the decay; as a result, the signal vanishes before resonance has a chance to build it up. However, if the mode were to decay at a rate less than (1/t), we should see the resonance phenomenon clearly. This specific condition would be possible if Re (\\\\lambda\\\\geq 0). For instance, when Re', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='(\\\\lambda=0) so that (\\\\lambda) lies on the imaginary axis of the complex plane ((\\\\lambda=jo)), the output becomes', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Footnote †: ({}^{\\\\dagger}) If the characteristic root in question repeats (r) times, resonance effect increases as (t^{r-1}). However, (t^{r-1}e^{\\\\lambda t}\\\\to 0) as (t\\\\to\\\\infty) for any value of (r), provided Re (\\\\lambda<0) ((\\\\lambda) in the LHP).\\n\\n[y(t)=Ate^{i\\\\omega t}]\\n\\nHere, the response does go to infinity linearly with (t).', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='For a real system, if (\\\\lambda=jo) is a root, (\\\\lambda^{}=-jo) must also be a root; the impulse response is of the form (Ae^{i\\\\omega t}+Ae^{-i\\\\omega t}=2A\\\\cos\\\\omega t). The response of this system to input (A\\\\cos\\\\omega t) is (2A\\\\cos\\\\omega t\\\\cos\\\\omega t). The reader can show that this convolution contains a term of the form (At\\\\cos\\\\omega t). The resonance phenomenon is clearly visible. The system response to its characteristic mode increases linearly with time, eventually reaching (\\\\infty), as', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='indicated in Fig. 2.24.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"Recall that when (\\\\lambda=jo), the system is marginally stable. As we have indicated, the full effect of resonance cannot be seen for an asymptotically stable system; only in a marginally stable system does the resonance phenomenon boost the system's response to infinity when the system's input\\n\\nFigure 2.24: Buildup of system response in resonance.\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='is a characteristic mode. But even in an asymptotically stable system, we see a manifestation of resonance if its characteristic roots are close to the imaginary axis so that Re (\\\\lambda) is a small, negative value. We can show that when the characteristic roots of a system are (\\\\sigma\\\\pm j\\\\omega_{0}), then the system response to the input (e^{i\\\\omega_{0}t}) or the sinusoid (\\\\cos\\\\omega_{0}t) is very large for small (\\\\sigma).1 The system response drops off rapidly as the input signal frequency', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='moves away from (\\\\omega_{0}). This frequency-selective behavior can be studied more profitably after an understanding of frequency-domain analysis has been acquired. For this reason we postpone full discussion of this subject until Ch. 4.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Footnote 1: This follows directly from Eq. (2.49) with (\\\\lambda=\\\\sigma+j\\\\omega_{0}) and (\\\\epsilon=\\\\sigma).\\n\\nImportance of the Resonance Phenomenon', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='The resonance phenomenon is very important because it allows us to design frequency-selective systems by choosing their characteristic roots properly. Lowpass, bandpass, highpass, and bandstop filters are all examples of frequency-selective networks. In mechanical systems, the inadvertent presence of resonance can cause signals of such tremendous magnitude that the system may fall apart. A musical note (periodic vibrations) of proper frequency can shatter glass if the frequency is matched to', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='the characteristic root of the glass, which acts as a mechanical system. Similarly, a company of soldiers marching in step across a bridge amounts to applying a periodic force to the bridge. If the frequency of this input force happens to be nearer to a characteristic root of the bridge, the bridge may respond (vibrate) violently and collapse, even though it would have been strong enough to carry many soldiers marching out of step. A case in point is the Tacoma Narrows Bridge failure of 1940.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"This bridge was opened to traffic in July 1940. Within four months of opening (on November 7, 1940), it collapsed in a mild gale, not because of the wind's brute force but because the frequencies of wind-generated vortices, which matched the natural frequencies (characteristic roots) of the bridge, caused resonance.\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"Because of the great damage that may occur, mechanical resonance is generally to be avoided, especially in structures or vibrating mechanisms. If an engine with periodic force (such as piston motion) is mounted on a platform, the platform with its mass and springs should be designed so that their characteristic roots are not close to the engine's frequency of vibration. Proper design of this platform can not only avoid resonance, but also attenuate vibrations if the system roots are placed far\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='away from the frequency of vibration.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='2.7 MATLAB: M-Files\\n\\nM-files are stored sequences of MATLAB commands and help simplify complicated tasks. There are two types of M-file: script and function. Both types are simple text files and require a.m filename extension.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"Although M-files can be created by using any text editor, MATLAB's built-in editor is the preferable choice because of its special features. As with any program, comments improve the readability of an M-file. Comments begin with the % character and continue through the end of the line.\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='An M-file is executed by simply typing the filename (without the.m extension). To execute, M-files need to be located in the current directory or any other directory in the MATLAB path. New directories are easily added to the MATLAB path by using the addpath command.\\n\\n2.7-1 Script M-Files', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Script files, the simplest type of M-file, consist of a series of MATLAB commands. Script files record and automate a series of steps, and they are easy to modify. To demonstrate the utility of a script file, consider the operational amplifier circuit shown in Fig. 2.25.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"The system's characteristic modes define the circuit's behavior and provide insight regarding system behavior. Using ideal, infinite gain difference amplifier characteristics, we first derive the differential equation that relates output (y(t)) to input (x(t)). Kirchhoff's current law (KCL) at the node shared by (R_{1}) and (R_{3}) provides\\n\\n[\\\\frac{x(t)-v(t)}{R_{3}}+\\\\frac{y(t)-v(t)}{R_{2}}+\\\\frac{0-v(t)}{R_{1}}-C_{2} \\\\dot{v}(t)=0]\\n\\nKCL at the inverting input of the op amp gives\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[\\\\frac{v(t)}{R_{1}}+C_{1}\\\\dot{y}(t)=0]\\n\\nCombining and simplifying the KCL equations yield\\n\\n[\\\\ddot{y}(t)+\\\\frac{1}{C_{2}}\\\\left{\\\\frac{1}{R_{1}}+\\\\frac{1}{R_{2}}+\\\\frac{1}{R_{ 3}}\\\\right}\\\\dot{y}(t)+\\\\frac{1}{R_{1}R_{2}C_{1}C_{2}}y(t)=-\\\\frac{1}{R_{1}R_{3}C {1}C{2}}x(t)]\\n\\nwhich is the desired constant coefficient differential equation. Thus, the characteristic equation is given by', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[\\\\dot{\\\\lambda}^{2}+\\\\frac{1}{C_{2}}\\\\left{\\\\frac{1}{R_{1}}+\\\\frac{1}{R_{2}}+\\\\frac {1}{R_{3}}\\\\right}\\\\dot{\\\\lambda}+\\\\frac{1}{R_{1}R_{2}C_{1}C_{2}}=(a_{0}\\\\dot{ \\\\lambda}^{2}+a_{1}\\\\dot{\\\\lambda}+a_{2})=0 \\\\tag{2.50}]\\n\\nThe roots (\\\\lambda_{1}) and (\\\\lambda_{2}) of Eq. (2.50) establish the nature of the characteristic modes (e^{\\\\lambda_{1}t}) and (e^{\\\\lambda_{2}t}).', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"As a first case, assign nominal component values of (R_{1}=R_{2}=R_{3}=10) k(\\\\Omega) and (C_{1}=C_{2}=1) uF. A series of MATLAB commands allows convenient computation of the roots (\\\\lambda=[\\\\lambda_{1};\\\\lambda_{2}]). Although (\\\\lambda) can be determined using the quadratic equation, MATLAB's roots command is more convenient. The roots command requires an input vector that contains the polynomial coefficients in descending order. Even if a coefficient is zero, it must still be included in the\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='vector.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Figure 2.25: Operation-amplifier circuit.\\n\\n% CH2MP1.m:Chapter2,MATLABProgram1  %ScriptM-filedeterminescharacteristicrootsofop-ampcircuit.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='%Setcomponentvalues:  R = [1e4, 1e4, 1e4]; C = [1e-6, 1e-6];  %Determinecoefficientsforcharacteristicequation:  A = [1, (1/R(1)+1/R(2)+1/R(3))/C(2), 1/(R(1)R(2)C(1)*C(2))];  %Determinecharacteristicroots:  lambda=roots(A); A scriptfileiscreatedbyplacingthesecommandsinastfile,whichinthiscaseisnamedCH2MP1.m.Whilecommentlinesimproprogramclarity,theirremovaldoesnotaffectprogramfunctionality.Theprogramisexecutedbytyping\\n\\nCH2MP1', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Afterexecution,alltheresultingvariablesareavailableintheworkspace.Forexample,toviewthecharacteristicroots,type\\n\\nlambda  lambda = -261.8034  -38.1966 Thus,thecharacteristicmodesaresimpledecayingexponentials:(e^{-261.8034t})and(e^{-38.1966t}).\\n\\nScriptfilespermitsimpleorincrementalchanges,therebysavingsignificanteffort.Considerwhathappenswhencapacitor(C_{1})ischangedfrom1.0(\\\\mu)Fto1.0nF.ChangingCH2MP1.msothatC = [1e-9, 1e-6]allowscomputationofthenewcharacteristicroots:', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='CH2MP1 >>lambda  lambda = 1.0e+003*  -0.1500+3.1587i  -0.1500-3.1587i Perhapssurprisingly,thecharacteristicmodesarenowcomplexexponentialscapableofsupportingoscillations.Theimaginaryportionof(\\\\lambda)dictatesanoscillationrateof3158.7rad/sorabout503Hz.Therealportiondictatestherateofdecay.Thetimeexpectedtoreducetheamplitudeto25%isapproximately(t=\\\\ln 0.25/\\\\text{Re}(\\\\lambda)\\\\approx 0.01)second.\\n\\nFunction M-Files', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Itis inconvenienttomodifyandsaveacscriptfileeachtimeachangeofparametersisdesired.FunctionM-filesprovideasensiblealternative.UnlikescriptM-files,functionM-filescanacceptinputargumentsaswellasreturnoutputs.FunctionstrulyextendtheMATLABlanguageinwaysthatscriptfilescannot.\\n\\nSyntactically, a function M-file is identical to a script M-file except for the first line. The general form of the first line is\\n\\nfunction [output1,..., outputM] = filename(input1,..., inputM)', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='For example, consider modification of CH2MP1.m to make function CH2MP2.m. Component values are passed to the function as two separate inputs: a length-3 vector of resistor values and a length-2 vector of capacitor values. The characteristic roots are returned as a (2\\\\times 1) complex vector.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='function [lambda] = CH2MP2(R,C) % CH2MP2.m : Chapter 2, MATLAB Program 2 % Function M-file finds characteristic roots of op-amp circuit. % INPUTS: R = length-3 vector of resistances % C = length-2 vector of capacitances % OUTPUS: lambda = characteristic roots\\n\\n% Determine coefficients for characteristic equation: A = [1, (1/R(1)+1/R(2)+1/R(3))/C(2), 1/(R(1)R(2)C(1)*C(2))]; % Determine characteristic roots: lambda = roots(A);', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='As with script M-files, function M-files execute by typing the name at the command prompt. However, inputs must also be included. For example, CH2MP2 easily confirms the oscillatory modes of the preceding example.\\n\\nlambda = CH2MP2([1e4, 1e4, 1e4],[1e-9, 1e-6])  lambda = 1.0e+003 *  -0.1500 + 3.1587i  -0.1500 - 3.1587i', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Although scripts and functions have similarities, they also have distinct differences that are worth pointing out. Scripts operate on workspace data; either functions must be supplied data through inputs or they must create their own data. Unless passed as an output, variables and data created by functions remain local to the function; variables or data generated by scripts are global and are added to the workspace. To emphasize this point, consider polynomial coefficient vector A, which is', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='created and used in both CH2MP1.m and CH2MP2.m. Following execution of function CH2MP2, the variable A is not added to the workspace. Following execution of script CH2MP1, however, A is available in the workspace. Recall, the workspace is easily viewed by typing either who or whos.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='For-Loops\\n\\nReal resistors and capacitors never exactly equal their nominal values. Suppose that the circuit components are measured as (R_{1}=10.322) k(\\\\Omega), (R_{2}=9.952) k(\\\\Omega), (R_{3}=10.115) k(\\\\Omega), (C_{1}=1.120) nF, and (C_{2}=1.320) nF. These values are consistent with the 10 and 25% tolerance resistor and capacitor values commonly and readily available. CH2MP2.m uses these component values to calculate the new values of (\\\\lambda).', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='lambda = CH2MP2([10322,9592,10115],[1.12e-9, 1.32e-6]) lambda = 1.0e+003\\n\\n-0.1136 + 2.6113i  -0.1136 - 2.6113i Now the natural modes oscillate at 2611.3 rad/s or about 416 Hz. Decay to 25% amplitude is expected in (t=\\\\ln 0.25/(-113.6)\\\\approx 0.012) second. These values, which differ significantly from the nominal values of 503 Hz and (t\\\\approx 0.01) second, warrant a more formal investigation of the effect of component variations on the locations of the characteristic roots.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='It is sensible to look at three values for each component: the nominal value, a low value, and a high value. Low and high values are based on component tolerances. For example, a 10% 1 k(\\\\Omega) resistor could have an expected low value of (1000(1-0.1)=900)(\\\\Omega) and an expected high value of (1000(1+0.1)=1100)(\\\\Omega). For the five passive components in the design, (3^{5}=243) permutations are possible.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Using either CH2MP1.m or CH2MP2.m to solve each of the 243 cases would be very tedious and boring. For-loops help automate repetitive tasks such as this. In MATLAB, the general structure of a for statement is\\n\\nfor variable = expression, statement,..., statement, end Five nested for-loops, one for each passive component, are required for the present example.\\n\\n% CH2MP3.m : Chapter 2, MATLAB Program 3  % Script M-file determines characteristic roots over a range of component values.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='% Pre-allocate memory for all computed roots:  lambda = zeros(2,243);  % Initialize index to identify each permutation:  p=0;  for R1 = 1e4[0.9,1.0,1.1],  for R2 = 1e4[0.9,1.0,1.1],  for R3 = 1e4[0.9,1.0,1.1],  for C1 = 1e-9[0.75,1.0,1.25],  for C2 = 1e-6*[0.75,1.0,1.25],  p = p+1;  lambda(:,p) = CH2MP2([R1 R2 R3],[C1 C2]);  end  end  end end', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"plot(real(lambda(:))),imag(lambda(:)),'kx',...  real(lambda(:,1)),imag(lambda(:,1)),'kv',...  real(lambda(:,end)),imag(lambda(:,end)),'k-')  xlabel('Real'),ylabel('Imaginary')  legend('Char. Roots','Min. Val. Roots','Max. Val. Roots','Location','West');The command lambda = zeros(2,243) preallocates a (2\\\\times 243) array to store the computed roots. When necessary, MATLAB performs dynamic memory allocation, so this command is not strictly necessary. However, preallocation significantly improves\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='script execution speed. Notice also that it would be nearly useless to call script CH2MP1 from within the nested loop; script file parameters cannot be changed during execution.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"The plot instruction is quite long. Long commands can be broken across several lines by terminating intermediate lines with three dots (...). The three dots tell MATLAB to continue the present command to the next line. Black x's locate roots of each permutation. The command lambda(:) vectorizes the (2\\\\times 243) matrix lambda into a (486\\\\times 1) vector. This is necessary in this case to ensure that a proper legend is generated. Because of loop order, permutation (p=1) corresponds to the case\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='of all components at the smallest values and permutation (p=243) corresponds to the case of all components at the largest values. This information is used to separately highlight the minimum and maximum cases using down-triangles ((\\\\bigtriangledown)) and up-triangles ((\\\\bigtriangleup)), respectively. In addition to terminating each for loop, end is used to indicate the final index along a particular dimension, which eliminates the need to remember the particular size of a variable. An', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='overloaded function, such as end, serves multiple uses and is typically interpreted based on context.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"The graphical results provided by CH2MP3 are shown in Fig. 26. Between extremes, root oscillations vary from 365 to 745 Hz and decay times to 25% amplitude vary from 6.2 to 12.7 ms. Clearly, this circuit's behavior is quite sensitive to ordinary component variations.\\n\\nGraphical Understanding of Convolution\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='MATLAB graphics effectively illustrate the convolution process. Consider the case of (y(t)=x(t)*h(t)), where (x(t)=1.5\\\\sin\\\\left(\\\\pi t\\\\right)(u(t)-u(t-1))) and (h(t)=1.5(u(t)-u(t-1.5))-u(t-2)+u(t-2.5)). Program CH2MP4 steps through the convolution over the time interval ((-0.25\\\\leq t\\\\leq 3.75)).\\n\\nFigure 26: Effect of component values on characteristic root locations.\\n\\n% CH2MP4.m: Chapter 2, MATLAB Program 4  % Script M-file graphically demonstrates the convolution process.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"figure(1) % Create figure window and make visible on screen  u = @(t) 1.0(t=0);  x = @(t) 1.5sin(pit).(u(t)-u(t-1));  h = @(t) 1.5(u(t)-u(t-1.5))-u(t-2)+u(t-2.5);  dtau = 0.005; tau = -1:dtau:4;  ti = 0; tvec = -.25::1:3.75;  y = NaNzeros(1,length(tvec)); % Pre-allocate memory  for t = tvec,  ti = ti+1; % Time index  xh = x(t-tau).h(tau); lxh = length(xh);  y(ti) = sum(xh.dtau); % Trapezoidal approximation of convolution integral  subplot(2,1,1),plot(tau,h(tau),'k-',tau,x(t-tau),'k-',t,0,'ok');\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"axis([tau(1) tau(end) -2.0 2.5]);  patch([tau(1:end-1);tau(1:end-1);tau(2:end);tau(2:end)],...  [zeros(1,lxh-1);xh(1:end-1);xh(2:end);zeros(1,lxh-1)],...  [.8.8.8],'edgecolor','none');  xlabel('\\\\tau'); title('h(\\\\tau)[solid], x(t-\\\\tau)[dashed], h(\\\\tau)x(t-\\\\tau)[gray]');  c = get(gca,'children'); set(gca,'children',[c(2);c(3);c(4);c(1)]);  subplot(2,1,2),plot(tvec,y,'k',tvec(ti),y(ti),'ok');  xlabel('t'); ylabel('y(t) = (\\\\backslash)int h(\\\\tau)x(t-\\\\tau)d\\\\tau');  axis([tau(1) tau(end) -1.0 2.0]);\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='grid;  drawnow;  end At each step, the program plots (h(\\\\tau)), (x(t-\\\\tau)), and shades the area (h(\\\\tau)x(t-\\\\tau)) gray. This gray area, which reflects the integral of (h(\\\\tau)x(t-\\\\tau)), is also the desired result, (y(t)). Figures 2.27, 2.28, and 2.29 display the convolution process at times (t) of 0.75, 2.25, and 2.85 seconds, respectively. These figures help illustrate how the regions of integration change with time. Figure 2.27 has limits of integration from 0 to ((t=0.75)). Figure 2.28', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='has two regions of integration, with limits ((t-1=1.25)) to 1.5 and 2.0 to ((t=2.25)). The last plot, Fig. 2.29, has limits from 2.0 to 2.5.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Several comments regarding CH2MP4 are in order. The command figure(1) opens the first figure window and, more important, makes sure it is visible. Anonymous functions are used to represent the functions (u(t)), (x(t)), and (h(t)). NaN, standing for not-a-number, usually results from operations such as (0/0) or (\\\\infty-\\\\infty). MATLAB refuses to plot NaN values, so preallocating (y(t)) with NaNs ensures that MATLAB displays only values of (y(t)) that have been computed. As its name suggests,', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='length returns the length of the input vector. The subplot(a,b,c) command partitions the current figure window into an a-by-b matrix of axes and selects axes c for use. Subplots facilitate graphical comparison by allowing multiple axes in a single figure window. The patch command is used to create the gray-shaded area for (h(\\\\tau)x(t-\\\\tau)). In CH2MP4, the get and set commands are used to reorder plot objects so that the gray area does not obscure other lines. Details of the patch, get, and set', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"commands, as used in CH2MP4, are somewhat advanced and are not pursued here.2 MATLAB also prints most Greek letters if the Greek name is preceded by a backslash ((\\\\backslash)) character. For example, \\\\tautau in the xlabel command produces the symbol (\\\\tau) in the plot's axis label. Similarly, an integral sign is produced by (\\\\backslash)int. Finally, the drawn command forces MATLAB to update the graphics window for each loop iteration. Although slow, this creates an animation-like effect.\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Replacing drawnow with the pause command allows users to manually step through the convolution process. The pause command still forces the graphics window to update, but the program will not continue until a key is pressed.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Figure 28: Graphical convolution at step (t=2.25) seconds.\\n\\nFigure 27: Graphical convolution at step (t=0.75) second.\\n\\nChapter 2 Time-domain analysis of continuous-time systems\\n\\n2.8 Appendix: Determining the Impulse Response\\n\\nIn Eq. (2.13), we showed that for an LTIC system (S) specified by Eq. (2.11), the unit impulse response (h(t)) can be expressed as\\n\\n[h(t)=b_{0}\\\\delta(t)+\\\\text{characteristic modes} \\\\tag{2.51}]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='To determine the characteristic mode terms in Eq. (2.51), let us consider a system (S_{0}) whose input (x(t)) and the corresponding output (w(t)) are related by\\n\\n[Q(D)w(t)=x(t) \\\\tag{2.52}]', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Observe that both the systems (S) and (S_{0}) have the same characteristic polynomial; namely, (Q(\\\\lambda)), and, consequently, the same characteristic modes. Moreover, (S_{0}) is the same as (S) with (P(D)=1), that is, (b_{0}=0). Therefore, according to Eq. (2.51), the impulse response of (S_{0}) consists of characteristic mode terms only without an impulse at (t=0). Let us denote this impulse response of (S_{0}) by (y_{n}(t)). Observe that (y_{n}(t)) consists of characteristic modes of (S)', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='and therefore may be viewed as a zero-input response of (S). Now (y_{n}(t)) is the response of (S_{0}) to input (\\\\delta(t)). Therefore, according to Eq. (2.52),', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[Q(D)y_{n}(t)=\\\\delta(t)]\\n\\nor\\n\\n[(D^{N}+a_{1}D^{N-1}+\\\\cdot\\\\cdot\\\\cdot+a_{N1}D+a_{N})y_{n}(t)=\\\\delta(t)]\\n\\nor\\n\\n[y_{n}^{(N)}(t)+a_{1}y_{n}^{(N-1)}(t)+\\\\cdot\\\\cdot\\\\cdot+a_{N-1}y_{n}^{(1)}(t)+a_{ N}y_{n}(t)=\\\\delta(t)]\\n\\nFigure 2.29: Graphical convolution at step (t=2.85) seconds.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='where (y_{n}^{(k)}(t)) represents the (k)th derivative of (y_{n}(t)). The right-hand side contains a single impulse term, (\\\\delta(t)). This is possible only if (y_{n}^{(N-1)}(t)) has a unit jump discontinuity at (t=0), so that (y_{n}^{(N)}(t)=\\\\delta(t)). Moreover, the lower-order terms cannot have any jump discontinuity because this would mean the presence of the derivatives of (\\\\delta(t)). Therefore (y_{n}(0)=y_{n}^{(1)}(0)=\\\\cdot\\\\cdot\\\\cdot=y_{n}^{(N-2)}(0)=0) (no discontinuity at (t=0)), and', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='the (N) initial conditions on (y_{n}(t)) are', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[y_{n}(0)=y_{n}^{(1)}(0)=\\\\cdot\\\\cdot\\\\cdot=y_{n}^{(N-2)}(0)=0\\\\qquad\\\\text{and} \\\\qquad y_{n}^{(N-1)}(0)=1 \\\\tag{53}]\\n\\nThis discussion means that (y_{n}(t)) is the zero-input response of the system (S) subject to initial conditions [Eq. (53)].\\n\\nWe now show that for the same input (x(t)) to both systems, (S) and (S_{0}), their respective outputs (y(t)) and (w(t)) are related by\\n\\n[y(t)=P(D)w(t) \\\\tag{54}]\\n\\nTo prove this result, we operate on both sides of Eq. (52) by (P(D)) to obtain', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[Q(D)P(D)w(t)=P(D)x(t)]\\n\\nComparison of this equation with Eq. (2) leads immediately to Eq. (54).', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Now if the input (x(t)=\\\\delta(t)), the output of (S_{0}) is (y_{n}(t)), and the output of (S), according to Eq. (54), is (P(D)y_{n}(t)). This output is (h(t)), the unit impulse response of (S). Note, however, that because it is an impulse response of a causal system (S_{0}), the function (y_{n}(t)) is causal. To incorporate this fact we must represent this function as (y_{n}(t)u(t)). Now it follows that (h(t)), the unit impulse response of the system (S), is given by', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[h(t)=P(D)[y_{n}(t)u(t)] \\\\tag{55}]\\n\\nwhere (y_{n}(t)) is a linear combination of the characteristic modes of the system subject to initial conditions (53).', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='The right-hand side of Eq. (55) is a linear combination of the derivatives of (y_{n}(t)u(t)). Evaluating these derivatives is clumsy and inconvenient because of the presence of (u(t)). The derivatives will generate an impulse and its derivatives at the origin. Fortunately when (M\\\\leq N) [Eq. (11)], we can avoid this difficulty by using the observation in Eq. (51), which asserts that at (t=0) (the origin), (h(t)=b_{0}\\\\delta(t)). Therefore, we need not bother to find (h(t)) at the origin. This', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='simplification means that instead of deriving (P(D)[y_{n}(t)u(t)]), we can derive (P(D)y_{n}(t)) and add to it the term (b_{0}\\\\delta(t)) so that', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[h(t) =b_{0}\\\\delta(t)+P(D)y_{n}(t)\\\\qquad t\\\\geq 0] [=b_{0}\\\\delta(t)+[P(D)y_{n}(t)]u(t)]\\n\\nThis expression is valid when (M\\\\leq N) [the form given in Eq. (11)]. When (M>N), Eq. (55) should be used.\\n\\n9 Summary', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='This chapter discusses time-domain analysis of LTIC systems. The total response of a linear system is a sum of the zero-input response and zero-state response. The zero-input response is the system response generated only by the internal conditions (initial conditions) of the system, assuming that the external input is zero; hence the adjective \"zero-input.\" The zero-state response is the system response generated by the external input, assuming that all initial conditions are zero, that is,', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='when the system is in zero state.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Every system can sustain certain forms of response on its own with no external input (zero input). These forms are intrinsic characteristics of the system; that is, they do not depend on any external input. For this reason they are called characteristic modes of the system. Needless to say, the zero-input response is made up of characteristic modes chosen in a combination required to satisfy the initial conditions of the system. For an (N)th-order system, there are (N) distinct modes.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='The unit impulse function is an idealized mathematical model of a signal that cannot be generated in practice.1 Nevertheless, introduction of such a signal as an intermediary is very helpful in analysis of signals and systems. The unit impulse response of a system is a combination of the characteristic modes of the system2 because the impulse (\\\\delta(t)=0) for (t>0). Therefore, the system response for (t>0) must necessarily be a zero-input response, which, as seen earlier, is a combination of', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='characteristic modes.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Footnote 1: However, it can be closely approximated by a narrow pulse of unit area and having a width that is much smaller than the time constant of an LTIC system in which it is used.\\n\\nFootnote 2: There is the possibility of an impulse in addition to the characteristic modes.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='The zero-state response (response due to external input) of a linear system can be obtained by breaking the input into simpler components and then adding the responses to all the components. In this chapter we represent an arbitrary input (x(t)) as a sum of narrow rectangular pulses [staircase approximation of (x(t))]. In the limit as the pulse width (\\\\to 0), the rectangular pulse components approach impulses. Knowing the impulse response of the system, we can find the system response to all', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"the impulse components and add them to yield the system response to the input (x(t)). The sum of the responses to the impulse components is in the form of an integral, known as the convolution integral. The system response is obtained as the convolution of the input (x(t)) with the system's impulse response (h(t)). Therefore, the knowledge of the system's impulse response allows us to determine the system response to any arbitrary input.\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='LTIC systems have a very special relationship to the everlasting exponential signal (e^{st}) because the response of an LTIC system to such an input signal is the same signal within a multiplicative constant. The response of an LTIC system to the everlasting exponential input (e^{st}) is (H(s)e^{st}), where (H(s)) is the transfer function of the system.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='If every bounded input results in a bounded output, the system is stable in the bounded-input/bounded-output (BIBO) sense. An LTIC system is BIBO-stable if and only if its impulse response is absolutely integrable. Otherwise, it is BIBO-unstable. BIBO stability is a stability seen from external terminals of the system. Hence, it is also called external stability or zero-state stability.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"In contrast, internal stability (or the zero-input stability) examines the system stability from inside. When some initial conditions are applied to a system in zero state, then, if the system eventually returns to zero state, the system is said to be stable in the asymptotic or Lyapunov sense. If the system's response increases without bound, it is unstable. If the system does not go to zero state and the response does not increase indefinitely, the system is marginally stable. The internal\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content=\"stability criterion, in terms of the location of a system's characteristic roots, can be summarized as follows:1. An LTIC system is asymptotically stable if, and only if, all the characteristic roots are in the LHP. The roots may be repeated or unrepeated.\", metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='2. An LTIC system is unstable if, and only if, either one or both of the following conditions exist: (i) at least one root is in the RHP; (ii) there are repeated roots on the imaginary axis.\\n3. An LTIC system is marginally stable if, and only if, there are no roots in the RHP, and there are some unrepeated roots on the imaginary axis.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='It is possible for a system to be externally (BIBO) stable but internally unstable. When a system is controllable and observable, its external and internal descriptions are equivalent. Hence, external (BIBO) and internal (asymptotic) stabilities are equivalent and provide the same information. Such a BIBO-stable system is also asymptotically stable, and vice versa. Similarly, a BIBO-unstable system is either marginally stable or asymptotically unstable system.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='The characteristic behavior of a system is extremely important because it determines not only the system response to internal conditions (zero-input behavior), but also the system response to external inputs (zero-state behavior) and the system stability. The system response to external inputs is determined by the impulse response, which itself is made up of characteristic modes. The width of the impulse response is called the time constant of the system, which indicates how fast the system can', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='respond to an input. The time constant plays an important role in determining such diverse system behaviors as the response time and filtering properties of the system, dispersion of pulses, and the rate of pulse transmission through the system.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='References\\n\\n[1] Lathi, B. P., Signals and Systems. Berkeley-Cambridge Press, Carmichael, CA, 1987.\\n\\n[2] Mason, S. J., Electronic Circuits, Signals, and Systems. Wiley, New York, 1960.\\n\\n[3] Kailath, T., Linear System. Prentice-Hall, Englewood Cliffs, NJ, 1980.\\n\\n[4] Lathi, B. P., Modern Digital and Analog Communication Systems, 3rd ed. Oxford University Press, New York, 1998.\\n\\nReferences\\n\\n[1] Lathi, B. P., Signals and Systems. Berkeley-Cambridge Press, Carmichael, CA, 1987.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[2] Mason, S. J., Electronic Circuits, Signals, and Systems. Wiley, New York, 1960.\\n\\n[3] Kailath, T., Linear System. Prentice-Hall, Englewood Cliffs, NJ, 1980.\\n\\n[4] Lathi, B. P., Modern Digital and Analog Communication Systems, 3rd ed. Oxford University Press, New York, 1998.\\n\\nReferences\\n\\n[1] Lathi, B. P., Signals and Systems. Berkeley-Cambridge Press, Carmichael, CA, 1987.\\n\\n[2] Mason, S. J., Electronic Circuits, Signals, and Systems. Wiley, New York, 1960.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[3] Kailath, T., Linear System. Prentice-Hall, Englewood Cliffs, NJ, 1980.\\n\\n[4] Lathi, B. P., Modern Digital and Analog Communication Systems, 3rd ed. Oxford University Press, New York, 1998.\\n\\nReferences\\n\\n[1] Lathi, B. P., Signals and Systems. Berkeley-Cambridge Press, Carmichael, CA, 1987.\\n\\n[2] Mason, S. J., Electronic Circuits, Signals, and Systems. Wiley, New York, 1960.\\n\\n[3] Kailath, T., Linear System. Prentice-Hall, Englewood Cliffs, NJ, 1980.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[4] Lathi, B. P., Modern Digital and Analog Communication Systems, 3rd ed. Oxford University Press, New York, 1998.\\n\\nReferences\\n\\n[1] Lathi, B. P., Signals and Systems. Berkeley-Cambridge Press, Carmichael, CA, 1987.\\n\\n[2] Mason, S. J., Electronic Circuits, Signals, and Systems. Wiley, New York, 1960.\\n\\n[3] Kailath, T., Linear System. Prentice-Hall, Englewood Cliffs, NJ, 1980.\\n\\n[4] Lathi, B. P., Modern Digital and Analog Communication Systems, 3rd ed. Oxford University Press, New York, 1998.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='References\\n\\n[1] Lathi, B. P., Signals and Systems. Berkeley-Cambridge Press, Carmichael, CA, 1987.\\n\\n[2] Mason, S. J., Electronic Circuits, Signals, and Systems. Wiley, New York, 1960.\\n\\n[3] Kailath, T., Linear System. Prentice-Hall, Englewood Cliffs, NJ, 1980.\\n\\n[4] Lathi, B. P., Modern Digital and Analog Communication Systems, 3rd ed. Oxford University Press, New York, 1998.\\n\\nReferences\\n\\n[1] Lathi, B. P., Signals and Systems. Berkeley-Cambridge Press, Carmichael, CA, 1987.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[2] Mason, S. J., Electronic Circuits, Signals, and Systems. Wiley, New York, 1960.\\n\\n[3] Kailath, T., Linear System. Prentice-Hall, Englewood Cliffs, NJ, 1980.\\n\\n[4] Lathi, B. P., Modern Digital and Analog Communication Systems, 3rd ed. Oxford University Press, New York, 1998.\\n\\nReferences\\n\\n[1] Lathi, B. P., Signals and Systems. Berkeley-Cambridge Press, Carmichael, CA, 1987.\\n\\n[2] Mason, S. J., Electronic Circuits, Signals, and Systems. Wiley, New York, 1960.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[3] Kailath, T., Linear System. Prentice-Hall, Englewood Cliffs, NJ, 1980.\\n\\n[4] Lathi, B. P., Modern Digital and Analog Communication Systems, 3rd ed. Oxford University Press, New York, 1998.\\n\\nReferences\\n\\n[1] Lathi, B. P., Signals and Systems. Berkeley-Cambridge Press, Carmichael, CA, 1987.\\n\\n[2] Mason, S. J., Electronic Circuits, Signals, and Systems. Wiley, New York, 1960.\\n\\n[3] Kailath, T., Linear System. Prentice-Hall, Englewood Cliffs, NJ, 1980.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[4] Lathi, B. P., Modern Digital and Analog Communication Systems, 3rd ed. Oxford University Press, New York, 1998.\\n\\nReferences\\n\\n[1] Lathi, B. P., Signals and Systems. Berkeley-Cambridge Press, Carmichael, CA, 1987.\\n\\n[2] Mason, S. J., Electronic Circuits, Signals, and Systems. Wiley, New York, 1960.\\n\\n[3] Kailath, T., Linear System. Prentice-Hall, Englewood Cliffs, NJ, 1980.\\n\\n[4] Lathi, B. P., Modern Digital and Analog Communication Systems, 3rd ed. Oxford University Press, New York, 1998.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='References\\n\\n[1] Lathi, B. P., Signals and Systems. Berkeley-Cambridge Press, Carmichael, CA, 1987.\\n\\n[2] Mason, S. J., Electronic Circuits, Signals, and Systems. Wiley, New York, 1960.\\n\\n[3] Kailath, T., Linear System. Prentice-Hall, Englewood Cliffs, NJ, 1980.\\n\\n[4] Lathi, B. P., Modern Digital and Analog Communication Systems, 3rd ed. Oxford University Press, New York, 1998.\\n\\nReferences\\n\\n[1] Lathi, B. P., Signals and Systems. Berkeley-Cambridge Press, Carmichael, CA, 1987.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[2] Mason, S. J., Electronic Circuits, Signals, and Systems. Wiley, New York, 1960.\\n\\n[3] Kailath, T., Linear System. Prentice-Hall, Englewood Cliffs, NJ, 1980.\\n\\n[4] Lathi, B. P., Modern Digital and Analog Communication Systems, 3rd ed. Oxford University Press, New York, 1998.\\n\\nReferences\\n\\n[1] Lathi, B. P., Signals and Systems. Berkeley-Cambridge Press, Carmichael, CA, 1987.\\n\\n[2] Mason, S. J., Electronic Circuits, Signals, and Systems. Wiley, New York, 1960.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[3] Kailath, T., Linear System. Prentice-Hall, Englewood Cliffs, NJ, 1980.\\n\\n[4] Lathi, B. P., Modern Digital and Analog Communication Systems, 3rd ed. Oxford University Press, New York, 1998.\\n\\nReferences\\n\\n[1] Lathi, B. P., Signals and Systems. Berkeley-Cambridge Press, Carmichael, CA, 1987.\\n\\n[2] Mason, S. J., Electronic Circuits, Signals, and Systems. Wiley, New York, 1960.\\n\\n[3] Kailath, T., Linear System. Prentice-Hall, Englewood Cliffs, NJ, 1980.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='[4] Lathi, B. P., Modern Digital and Analog Communication Systems, 3rd ed. Oxford University Press, New York, 1998.\\n\\nReferences\\n\\n[1] Lathi, B. P., Signals and Systems. Berkeley-Cambridge Press, Carmichael, CA, 1987.\\n\\n[2] Mason, S. J., Electronic Circuits, Signals, and Systems. Wiley, New York, 1960.\\n\\n[3] Kailath, T., Linear System. Prentice-Hall, Englewood Cliffs, NJ, 1980.\\n\\n[4] Lathi, B. P., Modern Digital and Analog Communication Systems, 3rd ed. Oxford University Press, New York, 1998.', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='References\\n\\n[2] Lathi, B. P., Signals and Systems. Berkeley-Cambridge Press, Carmichael, CA, 1987.\\n\\n[3] Mason, S. J., Electronic Circuits, Signals, and Systems. Wiley, New York, 1960.\\n\\n[4] Kailath, T., Linear System. Prentice-Hall, Englewood Cliffs, NJ, 1980.\\n\\n[5] Lathi, B. P., Modern Digital and Analog Communication Systems, 3rd ed. Oxford University Press, New York, 1998.\\n\\nReferences\\n\\n[', metadata={'source': '..\\\\mmd\\\\Chapter_02.mmd'}),\n",
       " Document(page_content='Signals and Systems\\n\\nIn this chapter we shall discuss basic aspects of signals and systems. We shall also introduce fundamental concepts and qualitative explanations of the hows and whys of systems theory, thus building a solid foundation for understanding the quantitative analysis in the remainder of the book. For simplicity, the focus of this chapter is on continuous-time signals and systems. Chapter 3 presents the same ideas for discrete-time signals and systems.\\n\\nSignals', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='A signal is a set of data or information. Examples include a telephone or a television signal, monthly sales of a corporation, or daily closing prices of a stock market (e.g., the Dow Jones averages). In all these examples, the signals are functions of the independent variable time. This is not always the case, however. When an electrical charge is distributed over a body, for instance, the signal is the charge density, a function of space rather than time. In this book we deal almost', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='exclusively with signals that are functions of time. The discussion, however, applies equally well to other independent variables.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Systems', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Signals may be processed further by systems, which may modify them or extract additional information from them. For example, an anti-aircraft gun operator may want to know the future location of a hostile moving target that is being tracked by his radar. Knowing the radar signal, he knows the past location and velocity of the target. By properly processing the radar signal (the input), he can approximately estimate the future location of the target. Thus, a system is an entity that processes a', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='set of signals (inputs) to yield another set of signals (outputs). A system may be made up of physical components, as in electrical, mechanical, or hydraulic systems (hardware realization), or it may be an algorithm that computes an output from an input signal (software realization).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Size of a Signal', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='The size of any entity is a number that indicates the largeness or strength of that entity. Generally speaking, the signal amplitude varies with time. How can a signal that exists over a certain timeinterval with varying amplitude be measured by one number that will indicate the signal size or signal strength? Such a measure must consider not only the signal amplitude, but also its duration. For instance, if we are to devise a single number (V) as a measure of the size of a human being, we must', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content=\"consider not only his or her width (girth), but also the height. If we make a simplifying assumption that the shape of a person is a cylinder of variable radius (r) (which varies with the height (h)), then one possible measure of the size of a person of height (H) is the person's volume (V), given by\", metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[V=\\\\pi\\\\int_{0}^{H}r^{2}(h)\\\\,dh]\\n\\nSignal Energy', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Arguing in this manner, we may consider the area under a signal (x(t)) as a possible measure of its size, because it takes account not only of the amplitude but also of the duration. However, this will be a defective measure because even for a large signal (x(t)), its positive and negative areas could cancel each other, indicating a signal of small size. This difficulty can be corrected by defining the signal size as the area under (|x(t)|^{2}), which is always positive. We call this measure', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='the signal energy(E_{x}), defined as', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[E_{x}=\\\\int_{-\\\\infty}^{\\\\infty}|x(t)|^{2}\\\\,dt \\\\tag{1}]\\n\\nThis definition simplifies for a real-valued signal (x(t)) to (E_{x}=\\\\int_{-\\\\infty}^{\\\\infty}x^{2}(t)\\\\,dt). There are also other possible measures of signal size, such as the area under (|x(t)|). The energy measure, however, is not only more tractable mathematically but is also more meaningful (as shown later) in the sense that it is indicative of the energy that can be extracted from the signal.\\n\\nSignal Power', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Signal energy must be finite for it to be a meaningful measure of signal size. A necessary condition for the energy to be finite is that the signal amplitude (\\\\to 0) as (|t|\\\\to\\\\infty) (Fig. 1a). Otherwise the integral in Eq. (1) will not converge.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='When the amplitude of (x(t)) does not (\\\\to 0) as (|t|\\\\to\\\\infty) (Fig. 1b), the signal energy is infinite. A more meaningful measure of the signal size in such a case would be the time average of the energy, if it exists. This measure is called the power of the signal. For a signal (x(t)), we define its power (P_{x}) as\\n\\n[P_{x}=\\\\lim_{T\\\\to\\\\infty}\\\\frac{1}{T}\\\\int_{-T/2}^{T/2}|x(t)|^{2}\\\\,dt \\\\tag{2}]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='This definition simplifies for a real-valued signal (x(t)) to (P_{x}=\\\\lim_{T\\\\to\\\\infty}\\\\frac{1}{T}\\\\int_{-T/2}^{T/2}x^{2}(t)\\\\,dt). Observe that the signal power (P_{x}) is the time average (mean) of the signal magnitude squared, that is, the mean-square value of (|x(t)|). Indeed, the square root of (P_{x}) is the familiar rms (root-mean-square) value of (x(t)).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Generally, the mean of an entity averaged over a large time interval approaching infinity exists if the entity either is periodic or has a statistical regularity. If such a condition is not satisfied, the average may not exist. For instance, a ramp signal (x(t)=t) increases indefinitely as (|t|\\\\to\\\\infty), and neither the energy nor the power exists for this signal. However, the unit step function, which is not periodic nor has statistical regularity, does have a finite power.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='When (x(t)) is periodic, (|x(t)|^{2}) is also periodic. Hence, the power of (x(t)) can be computed from Eq. (1.2) by averaging (|x(t)|^{2}) over one period.\\n\\n1.3.3 Comments.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='The signal energy as defined in Eq. (1.1) does not indicate the actual energy (in the conventional sense) of the signal because the signal energy depends not only on the signal, but also on the load. It can, however, be interpreted as the energy dissipated in a normalized load of a 1 ohm resistor if a voltage (x(t)) were to be applied across the 1 ohm resistor [or if a current (x(t)) were to be passed through the 1 ohm resistor]. The measure of \"energy\" is therefore indicative of the energy', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='capability of the signal, not the actual energy. For this reason the concepts of conservation of energy should not be applied to this \"signal energy.\" Parallel observation applies to \"signal power\" defined in Eq. (1.2). These measures are but convenient indicators of the signal size, which prove useful in many applications. For instance, if we approximate a signal (x(t)) by another signal (g(t)), the error in the approximation is (e(t)=x(t)-g(t)). The energy (or power) of (e(t)) is a convenient', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='indicator of the goodness of the approximation. It provides us with a quantitative measure of determining the closeness of the approximation. In communication systems, during transmission over a channel, message signals are corrupted by unwanted signals (noise). The quality of the received signal is judged by the relative sizes of the desired signal and the unwanted signal (noise). In this case the ratio of the message signal and noise signal powers (signal-to-noise power ratio) is a good', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='indication of the received signal quality.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='1.3.4 Units of Energy and Power.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Equation (1.1) is not correct dimensionally. This is because here we are using the term energy not in its conventional sense, but to indicate the signal size. The same observation applies to Eq. (1.2) for power. The units of energy and power, as defined here, depend on the nature of the signal (x(t)). If (x(t)) is a voltage signal, its energy (E_{x}) has units of volts squared-seconds (V({}^{2}) s), and its power (P_{x}) has units of volts squared. If (x(t)) is a current signal, these units', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='will be amperes squared-seconds (A({}^{2}) s) and amperes squared, respectively.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Figure 1.1: Examples of signals: (a) a signal with finite energy and (b) a signal with finite power.\\n\\nIn Fig. 1.2a, the signal amplitude (\\\\to 0) as (|t|\\\\to\\\\infty). Therefore the suitable measure for this signal is its energy (E_{x}) given by\\n\\n[E_{x}=\\\\int_{-\\\\infty}^{\\\\infty}|x(t)|^{2}\\\\,dt=\\\\int_{-1}^{0}(2)^{2}\\\\,dt+\\\\int_{0}^{ \\\\infty}4e^{-t}\\\\,dt=4+4=8]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='In Fig. 1.2b, the signal magnitude does not (\\\\to 0) as (|t|\\\\to\\\\infty). However, it is periodic, and therefore its power exists. We can use Eq. (1.2) to determine its power. We can simplify the procedure for periodic signals by observing that a periodic signal repeats regularly each period (2 seconds in this case). Therefore, averaging (|x(t)|^{2}) over an infinitely large interval is identical to averaging this quantity over one period (2 seconds in this case). Thus', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[P_{x}=\\\\tfrac{1}{2}\\\\int_{-1}^{1}|x(t)|^{2}\\\\,dt=\\\\tfrac{1}{2}\\\\int_{-1}^{1}t^{2}\\\\, dt=\\\\tfrac{1}{3}]\\n\\nRecall that the signal power is the square of its rms value. Therefore, the rms value of this signal is (1/\\\\sqrt{3}).\\n\\nFigure 1.2: Signals for Ex. 1.1\\n\\nChapter 1 Signals and Systems\\n\\n1.1 Determining Power and RMS Value\\n\\nThe power and the rms value of', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[\\\\begin{split}&\\\\text{(a) This is a periodic signal with period $T_{0}=2\\\\pi/\\\\omega_{0}$. The suitable measure of this signal is its power. Because it is a periodic signal, we may compute its power by averaging its energy over one period (T_{0}=2\\\\pi/\\\\omega_{0}). However, for the sake of demonstration, we shall use Eq. (1.2) to solve this problem by averaging over an infinitely large time interval.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[\\\\begin{split} P_{x}&=\\\\lim_{T\\\\to\\\\infty}\\\\frac{1}{T} \\\\int_{-T/2}^{T/2}C^{2}\\\\cos^{2}\\\\left(\\\\omega_{0}t+\\\\theta\\\\right)dt=\\\\lim_{T\\\\to \\\\infty}\\\\frac{C^{2}}{2T}\\\\int_{-T/2}^{T/2}\\\\left[1+\\\\cos\\\\left(2\\\\omega_{0}t+2\\\\theta \\\\right)\\\\right]dt\\\\ &=\\\\lim_{T\\\\to\\\\infty}\\\\frac{C^{2}}{2T}\\\\int_{-T/2}^{T/2}dt+\\\\lim_{T \\\\to\\\\infty}\\\\frac{C^{2}}{2T}\\\\int_{-T/2}^{T/2}\\\\cos\\\\left(2\\\\omega_{0}t+2\\\\theta \\\\right)dt\\\\end{split}]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='The first term on the right-hand side is equal to (C^{2}/2). The second term, however, is zero because the integral appearing in this term represents the area under a sinusoid over a very large time interval (T) with (T\\\\to\\\\infty). This area is at most equal to the area of half the cycle because of cancellations of the positive and negative areas of a sinusoid. The second term is this area multiplied by (C^{2}/2T) with (T\\\\to\\\\infty). Clearly this term is zero, and\\n\\n[P_{x}=\\\\frac{C^{2}}{2}]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='This shows that a sinusoid of amplitude (C) has a power (C^{2}/2) regardless of the value of its frequency (\\\\omega_{0}\\\\left(\\\\omega_{0}\\\\neq 0\\\\right)) and phase (\\\\theta). The rms value is (C/\\\\sqrt{2}). If the signal frequency is zero (dc or a constant signal of amplitude (C)), the reader can show that the power is (C^{2}).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='(b) In Ch. 6, we shall show that a sum of two sinusoids may or may not be periodic, depending on whether the ratio (\\\\omega_{1}/\\\\omega_{2}) is a rational number. Therefore, the period of this signal is not known. Hence, its power will be determined by averaging its energy over (T) seconds with (T\\\\to\\\\infty). Thus,', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[\\\\begin{split} P_{x}&=\\\\lim_{T\\\\to\\\\infty}\\\\frac{1}{T} \\\\int_{-T/2}^{T/2}[C_{1}\\\\cos\\\\left(\\\\omega_{1}t+\\\\theta_{1}\\\\right)+C_{2}\\\\cos\\\\left( \\\\omega_{2}t+\\\\theta_{2}\\\\right)]^{2}dt\\\\ &=\\\\lim_{T\\\\to\\\\infty}\\\\frac{1}{T}\\\\int_{-T/2}^{T/2}{C_{1}}^{2}\\\\cos^{ 2}\\\\left(\\\\omega_{1}t+\\\\theta_{1}\\\\right)dt+\\\\lim_{T\\\\to\\\\infty}\\\\frac{1}{T}\\\\int_{-T/ 2}^{T/2}{C_{2}}^{2}\\\\cos^{2}\\\\left(\\\\omega_{2}t+\\\\theta_{2}\\\\right)dt\\\\ &\\\\quad+\\\\lim_{T\\\\to\\\\infty}\\\\frac{2C_{1}C_{2}}{T}\\\\int_{-T/2}^{T/2}', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='\\\\cos\\\\left(\\\\omega_{1}t+\\\\theta_{1}\\\\right)\\\\cos\\\\left(\\\\omega_{2}t+\\\\theta_{2}\\\\right) dt\\\\end{split}]The first and second integrals on the right-hand side are the powers of the two sinusoids, which are ({C_{1}}^{2}/2) and ({C_{2}}^{2}/2), as found in part (a). The third term, the product of two sinusoids, can be expressed as a sum of two sinusoids (\\\\cos\\\\left[(\\\\omega_{1}+\\\\omega_{2})t+(\\\\theta_{1}+\\\\theta_{2})\\\\right]) and (\\\\cos\\\\left[(\\\\omega_{1}-\\\\omega_{2})t+(\\\\theta_{1}-\\\\theta_{2})\\\\right]), respectively.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Now, arguing as in part (a), we see that the third term is zero. Hence, we have+', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Footnote †: This is true only if (\\\\omega_{1}\\\\neq\\\\omega_{2}). If (\\\\omega_{1}=\\\\omega_{2}), the integrand of the third term contains a constant (\\\\cos\\\\left(\\\\theta_{1}-\\\\theta_{2}\\\\right)), and the third term (\\\\to 2C_{1}C_{2}\\\\cos\\\\left(\\\\theta_{1}-\\\\theta_{2}\\\\right)) as (T\\\\to\\\\infty).\\n\\n[P_{x}=\\\\frac{{C_{1}}^{2}}{2}+\\\\frac{{C_{2}}^{2}}{2}]\\n\\nand the rms value is (\\\\sqrt{({C_{1}}^{2}+{C_{2}}^{2})/2}).\\n\\nWe can readily extend this result to a sum of any number of sinusoids with distinct frequencies. Thus, if', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[x(t)=\\\\sum_{n=1}^{\\\\infty}C_{n}\\\\cos\\\\left(\\\\omega_{n}t+\\\\theta_{n}\\\\right)]\\n\\nassuming that none of the two sinusoids have identical frequencies and (\\\\omega_{n}\\\\neq 0), then\\n\\n[P_{x}=\\\\tfrac{1}{2}\\\\sum_{n=1}^{\\\\infty}{C_{n}}^{2}]\\n\\nIf (x(t)) also has a dc term, as\\n\\n[x(t)=C_{0}+\\\\sum_{n=1}^{\\\\infty}C_{n}\\\\cos\\\\left(\\\\omega_{n}t+\\\\theta_{n}\\\\right)]\\n\\nthen\\n\\n[P_{x}=C_{0}^{2}+\\\\tfrac{1}{2}\\\\sum_{n=1}^{\\\\infty}{C_{n}}^{2} \\\\tag{1.3}]\\n\\n(c) In this case the signal is complex, and we use Eq. (1.2) to compute the power.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[P_{x}=\\\\lim_{T\\\\to\\\\infty}\\\\frac{1}{T}\\\\int_{-T/2}^{T/2}|De^{i\\\\omega_{0}t}|^{2}\\\\,dt]\\n\\nRecall that (|e^{i\\\\omega_{0}t}|=1) so that (|De^{i\\\\omega_{0}t}|^{2}=|D|^{2}), and\\n\\n[P_{x}=|D|^{2} \\\\tag{1.4}]\\n\\nThe rms value is (|D|).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Comment. In part (b) of Ex. 1.2, we have shown that the power of the sum of two sinusoids is equal to the sum of the powers of the sinusoids. It may appear that the power of (x_{1}(t)+x_{2}(t))is (P_{x_{1}}+P_{x_{2}}). Unfortunately, this conclusion is not true in general. It is true only under a certain condition (orthogonality), discussed later (Sec. 6.5-3).\\n\\n1.1 Computing Energy, Power, and RMS Value', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Show that the energies of the signals in Figs. 1.3a, 1.3b, 1.3c, and 1.3d are 4, 1, 4/3, and 4/3, respectively. Observe that doubling a signal quadruples the energy, and time-shifting a signal has no effect on the energy. Show also that the power of the signal in Fig. 1.3e is 0.4323. What is the rms value of signal in Fig. 1.3e?\\n\\n1.2 Computing Power over a Period', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Redo Ex. 1.1a to find the power of a sinusoid (C\\\\cos\\\\left(\\\\omega_{0}t+\\\\theta\\\\right)) by averaging the signal energy over one period (T_{0}=2\\\\pi/\\\\omega_{0}) (rather than averaging over the infinitely large interval). Show also that the power of a dc signal (x(t)=C_{0}) is (C_{0}^{2}), and its rms value is (C_{0}).\\n\\n1.3 Power of a Sum of Two Equal-Frequency Sinusoids', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Show that if (\\\\omega_{1}=\\\\omega_{2}), the power of (x(t)=C_{1}\\\\,\\\\cos\\\\left(\\\\omega_{1}t+\\\\theta_{1}\\\\right)+C_{2}\\\\,\\\\cos\\\\left(\\\\omega_{ 2}t+\\\\theta_{2}\\\\right)) is ([{C_{1}}^{2}+{C_{2}}^{2}+2C_{1}C_{2}\\\\,\\\\cos\\\\left(\\\\theta_{1}-\\\\theta_{2}\\\\right) ]/2), which is not equal to the Ex. 1.2b result of (({C_{1}}^{2}+{C_{2}}^{2})/2).\\n\\nFigure 1.3: Signals for Drill 1.1\\n\\nChapter 1 Introduction\\n\\n1.1 Some Useful Signal Operations\\n\\nThe use of\\n\\nChapter 1 Signals and Systems\\n\\n1.3 Time Shifting', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='An exponential function (x(t)=e^{-2t}) shown in Fig. 1.5a is delayed by 1 second. Sketch and mathematically describe the delayed function. Repeat the problem with (x(t)) advanced by 1 second.\\n\\nThe function (x(t)) can be described mathematically as\\n\\n[x(t)=\\\\begin{cases}e^{-2t}&t\\\\geq 0\\\\ 0&t<0\\\\end{cases} \\\\tag{1.5}]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Let (x_{d}(t)) represent the function (x(t)) delayed (right-shifted) by 1 second, as illustrated in Fig. 1.5b. This function is (x(t-1)); its mathematical description can be obtained from (x(t))\\n\\nFigure 1.5: (a) Signal (x(t)). (b) Signal (x(t)) delayed by 1 second. (c) Signal (x(t)) advanced by 1 second.\\n\\nby replacing (t) with (t-1) in Eq. (1.5). Thus,\\n\\n[x_{d}(t)=x(t-1)=\\\\begin{cases}e^{-2(t-1)}&t-1\\\\geq 0\\\\quad\\\\text{or}\\\\quad t\\\\geq 1 \\\\ 0&t-1<0\\\\quad\\\\text{or}\\\\quad t<1\\\\end{cases}]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Let (x_{a}(t)) represent the function (x(t)) advanced (left-shifted) by (1) second, as depicted in Fig. 1.5c. This function is (x(t+1)); its mathematical description can be obtained from (x(t)) by replacing (t) with (t+1) in Eq. (1.5). Thus,\\n\\n[x_{a}(t)=x(t+1)=\\\\begin{cases}e^{-2(t+1)}&t+1\\\\geq 0\\\\quad\\\\text{or}\\\\quad t\\\\geq-1 \\\\ 0&t+1<0\\\\quad\\\\text{or}\\\\quad t<-1\\\\end{cases}]\\n\\n1.4 Working with Time Delay and Time Advance', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Write a mathematical description of the signal (x_{3}(t)) in Fig. 1.3c. Next, delay this signal by (2) seconds. Sketch the delayed signal. Show that this delayed signal (x_{d}(t)) can be described mathematically as (x_{d}(t)=2(t-2)) for (2\\\\leq t\\\\leq 3), and equal to (0) otherwise. Now repeat the procedure with the signal advanced (left-shifted) by (1) second. Show that this advanced signal (x_{a}(t)) can be described as (x_{a}(t)=2(t+1)) for (-1\\\\leq t\\\\leq 0), and (0) otherwise.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='1.2-2 Time Scaling\\n\\nThe compression or expansion of a signal in time is known as time scaling. Consider the signal (x(t)) of Fig. 1.6a. The signal (\\\\phi(t)) in Fig. 1.6b is (x(t)) compressed in time by a factor of (2). Therefore, whatever happens in (x(t)) at some instant (t) also happens to (\\\\phi(t)) at the instant (t/2) so that\\n\\n[\\\\phi\\\\Big{(}\\\\frac{t}{2}\\\\Big{)}=x(t)\\\\qquad\\\\text{and}\\\\qquad\\\\phi(t)=x(2t)]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Observe that because (x(t)=0) at (t=T_{1}) and (T_{2}), we must have (\\\\phi(t)=0) at (t=T_{1}/2) and (T_{2}/2), as shown in Fig. 1.6b. If (x(t)) were recorded on a tape and played back at twice the normal recording speed, we would obtain (x(2t)). In general, if (x(t)) is compressed in time by a factor (a) ((a>1)), the resulting signal (\\\\phi(t)) is given by\\n\\n[\\\\phi(t)=x(at)]\\n\\nUsing a similar argument, we can show that (x(t)) expanded (slowed down) in time by a factor (a) ((a>1)) is given by', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[\\\\phi(t)=x\\\\bigg{(}\\\\frac{t}{a}\\\\bigg{)}]\\n\\nFigure 1.6c shows (x(t/2)), which is (x(t)) expanded in time by a factor of (2). Observe that in a time-scaling operation, the origin (t=0) is the anchor point, which remains unchanged under the scaling operation because at (t=0), (x(t)=x(at)=x(0)).\\n\\nIn summary, to time-scale a signal by a factor (a), we replace (t) with (at). If (a>1), the scaling results in compression, and if (a<1), the scaling results in expansion.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Example 1.4: Continuous Time-Scaling Operation__\\n\\nFigure 1.7a shows a signal (x(t)). Sketch and describe mathematically this signal time-compressed by factor 3. Repeat the problem for the same signal time-expanded by factor 2.\\n\\n[x(t)=\\\\begin{cases}2&-1.5\\\\leq t<0\\\\ 2\\\\,e^{-t/2}&0\\\\leq t<3\\\\ 0&\\\\text{otherwise}\\\\end{cases} \\\\tag{1.6}]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Figure 1.7b shows (x_{c}(t)), which is (x(t)) time-compressed by factor 3; consequently, it can be described mathematically as (x(3t)), which is obtained by replacing (t) with (3t) in the right-hand side of Eq. (1.6). Thus,', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[x_{c}(t)=x(3t)=\\\\begin{cases}2&-1.5\\\\leq 3t<0\\\\quad\\\\text{or}\\\\quad-0.5\\\\leq t<0\\\\ 2\\\\,e^{-3t/2}&0\\\\leq 3t<3\\\\quad\\\\text{or}\\\\quad 0\\\\leq t<1\\\\ 0&\\\\text{otherwise}\\\\end{cases}]Observe that the instants (t=-1.5) and (3) in (x(t)) correspond to the instants (t=-0.5), and (1) in the compressed signal (x(3t)).\\n\\nFigure 7c shows (x_{e}(t)), which is (x(t)) time-expanded by factor (2); consequently, it can be described mathematically as (x(t/2)), which is obtained by replacing (t) with (t/2) in (x(t)). Thus,', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[x_{e}(t)=x\\\\left(\\\\frac{t}{2}\\\\right)=\\\\begin{cases}2&-1.5\\\\leq\\\\frac{t}{2}<0\\\\quad \\\\text{or}\\\\quad-3\\\\leq t<0\\\\ 2e^{-t/4}&0\\\\leq\\\\frac{t}{2}<3\\\\quad\\\\text{or}\\\\quad 0\\\\leq t<6\\\\ 0&\\\\text{otherwise}\\\\end{cases}]\\n\\nObserve that the instants (t=-1.5) and (3) in (x(t)) correspond to the instants (t=-3) and (6) in the expanded signal (x(t/2)).\\n\\nChapter 1 Signals and Systems\\n\\n1.2 Time Reversal', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Consider the signal (x(t)) in Fig. 1.8a. We can view (x(t)) as a rigid wire frame hinged at the vertical axis. To time-reverse (x(t)), we rotate this frame (180^{\\\\circ}) about the vertical axis. This time reversal [the reflection of (x(t)) about the vertical axis] gives us the signal (\\\\phi(t)) (Fig. 1.8b). Observe that whatever happens in Fig. 1.8a at some instant (t) also happens in Fig. 1.8b at the instant (-t), and vice versa. Therefore,\\n\\n[\\\\phi(t)=x(-t)]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Thus, to time-reverse a signal we replace (t) with (-t), and the time reversal of signal (x(t)) results in a signal (x(-t)). We must remember that the reversal is performed about the vertical axis, which acts as an anchor or a hinge. Recall also that the reversal of (x(t)) about the horizontal axis results in (-x(t)).\\n\\nFigure 1.8: Time reversal of a signal.\\n\\n1.2 Some Useful Signal Operations\\n\\nExample 1.5: Time Reversal of a Signal', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='For the signal (x(t)) illustrated in Fig. 1.9a, sketch (x(-t)), which is time-reversed (x(t)).\\n\\nThe instants (-1) and (-5) in (x(t)) are mapped into instants (1) and (5) in (x(-t)). Because (x(t)=e^{t/2}), we have (x(-t)=e^{-t/2}). The signal (x(-t)) is depicted in Fig. 1.9b. We can describe (x(t)) and (x(-t)) as\\n\\n[x(t)=\\\\begin{cases}e^{t/2}&-1\\\\geq t>-5\\\\ 0&\\\\text{otherwise}\\\\end{cases}]\\n\\nand its time-reversed version (x(-t)) is obtained by replacing (t) with (-t) in (x(t)) as', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[x(-t)=\\\\begin{cases}e^{-t/2}&-1\\\\geq-t>-5\\\\quad\\\\text{or}\\\\quad 1\\\\leq t<5\\\\ 0&\\\\text{otherwise}\\\\end{cases}]\\n\\n1.2-4 Combined Operations\\n\\nCertain complex operations require simultaneous use of more than one of the operations just described. The most general operation involving all the three operations is (x(at-b)), which is realized in two possible sequences of operation:', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Time-shift (x(t)) by (b) to obtain (x(t-b)). Now time-scale the shifted signal (x(t-b)) by (a) [i.e., replace (t) with (at)] to obtain (x(at-b)).\\n\\nTime-scale (x(t)) by (a) to obtain (x(at)). Now time-shift (x(at)) by (b/a) [i.e., replace (t) with (t-(b/a))] to obtain (x[a(t-b/a))] = (x(at-b)). In either case, if (a) is negative, time scaling involves time reversal.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='For example, the signal (x(2t-6)) can be obtained in two ways. We can delay (x(t)) by (6) to obtain (x(t-6)), and then time-compress this signal by factor (2) (replace (t) with (2t)) to obtain (x(2t-6))\\n\\nFigure 1.9: Example of time reversal.\\n\\nAlternatively, we can first time-compress (x(t)) by factor 2 to obtain (x(2t)), then delay this signal by 3 (replace (t) with (t-3)) to obtain (x(2t-6)).\\n\\n1.3 Classification of Signals', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Classification helps us better understand and utilize the items around us. Cars, for example, are classified as sports, offroad, family, and so forth. Knowing you have a sports car is useful in deciding whether to drive on a highway or on a dirt road. Knowing you want to drive up a mountain, you would probably choose an offroad vehicle over a family sedan. Similarly, there are several classes of signals. Some signal classes are more suitable for certain applications than others. Further,', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='different signal classes often require different mathematical tools. Here we shall consider only the following classes of signals, which are suitable for the scope of this book:', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Continuous-time and discrete-time signals\\n\\nAnalog and digital signals\\n\\nPeriodic and aperiodic signals\\n\\nEnergy and power signals\\n\\nDeterministic and probabilistic signals\\n\\n1.3-1 Continuous-Time and Discrete-Time Signals', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='A signal that is specified for a continuum of values of time (t) (Fig. 1.10a) is a continuous-time signal, and a signal that is specified only at discrete values of (t) (Fig. 1.10b) is a discrete-time signal. Telephone and video camera outputs are continuous-time signals, whereas the quarterly gross national product (GNP), monthly sales of a corporation, and stock market daily averages are discrete-time signals.\\n\\n1.3-2 Analog and Digital Signals', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='The concept of continuous time is often confused with that of analog. The two are not the same. The same is true of the concepts of discrete time and digital. A signal whose amplitude can take on any value in a continuous range is an analog signal. This means that an analog signal amplitude can take on an infinite number of values. A digital signal, on the other hand, is one whose amplitude can take on only a finite number of values. Signals associated with a digital computer are digital', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='because they take on only two values (binary signals). A digital signal whose amplitudes can take on (M) values is an (M)-ary signal of which binary ((M=2)) is a special case. The terms continuous time and discrete time qualify the nature of a signal along the time (horizontal) axis. The terms analog and digital, on the other hand, qualify the nature of the signal amplitude (vertical axis). Figure 1.11 shows examples of signals of various types. It is clear that analog is not necessarily', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='continuous-time and digital need not be discrete-time. Figure 1.11c shows an example of an analog discrete-time signal. An analog signal can be converted into a digital signal [analog-to-digital (A/D) conversion] through quantization (rounding off ), as explained in Sec. 8.3.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='1.3.1 Periodic and Aperiodic Signals\\n\\nA signal (x(t)) is said to be periodic if for some positive constant (T_{0})\\n\\n[x(t)=x(t+T_{0})\\\\qquad\\\\mbox{for all $t$} \\\\tag{1.7}]\\n\\nThe smallest value of (T_{0}) that satisfies the periodicity condition of Eq. (1.7) is the fundamental period of (x(t)). The signals in Figs. 1.2b and 1.3e are periodic signals with periods 2 and 1, respectively. A signal is aperiodic if it is not periodic. Signals in Figs. 1.2a, 1.3a, 1.3b, 1.3c, and 1.3d are all aperiodic.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='By definition, a periodic signal (x(t)) remains unchanged when time-shifted by one period. For this reason, a periodic signal must start at (t=-\\\\infty): if it started at some finite instant, say, (t=0), the time-shifted signal (x(t+T_{0})) would start at (t=-T_{0}) and (x(t+T_{0})) would not be the same as\\n\\nFigure 1.10: (a) Continuous-time and (b) discrete-time signals.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='(x(t)). Therefore, a periodic signal, by definition, must start at (t=-\\\\infty) and continue forever, as illustrated in Fig. 12.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Another important property of a periodic signal (x(t)) is that (x(t)) can be generated by periodic extension of any segment of (x(t)) of duration (T_{0}) (the period). As a result, we can generate (x(t)) from any segment of (x(t)) having a duration of one period by placing this segment and the reproduction thereof end to end ad infinitum on either side. Figure 13 shows a periodic signal (x(t)) of period (T_{0}=6). The shaded portion of Fig. 13a shows a segment of (x(t)) starting at (t=-1) and', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='having a duration of one period (6 seconds). This segment, when repeated forever in either direction, results in the periodic signal (x(t)). Figure 13b shows another shaded segment of (x(t)) of duration (T_{0}) starting at (t=0). Again, we see that this segment, when repeated forever on either side, results in (x(t)). The reader can verify that this construction is possible with any segment of (x(t)) starting at any instant as long as the segment duration is one period.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Figure 11: Examples of signals: (a) analog, continuous time; (b) digital, continuous time; (c) analog, discrete time; and (d) digital, discrete time.\\n\\nFigure 12: A periodic signal of period (T_{0}).\\n\\nAn additional useful property of a periodic signal (x(t)) of period (T_{0}) is that the area under (x(t)) over any interval of duration (T_{0}) is the same; that is, for any real numbers (a) and (b),\\n\\n[\\\\int_{a}^{a+T_{0}}x(t)\\\\,dt=\\\\int_{b}^{b+T_{0}}x(t)\\\\,dt]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='This result follows from the fact that a periodic signal takes the same values at the intervals of (T_{0}). Hence, the values over any segment of duration (T_{0}) are repeated in any other interval of the same duration. For convenience, the area under (x(t)) over any interval of duration (T_{0}) will be denoted by\\n\\n[\\\\int_{T_{0}}x(t)\\\\,dt]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='It is helpful to label signals that start at (t=-\\\\infty) and continue forever as everlasting signals. Thus, an everlasting signal exists over the entire interval (-\\\\infty<t<\\\\infty). The signals in Figs. (b)b and (b)b are examples of everlasting signals. Clearly, a periodic signal, by definition, is an everlasting signal.\\n\\nA signal that does not start before (t=0) is a causal signal. In other words, (x(t)) is a causal signal if\\n\\n[x(t)=0\\\\qquad t<0]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='The signals in Figs. (a)a-(c)c are causal signals. A signal that starts before (t=0) is a noncausal signal. All the signals in Figs. (d)d and (e)e are noncausal. Observe that an everlasting signal is always noncausal but a noncausal signal is not necessarily everlasting. The everlasting signal in Fig. (b)b is noncausal; however, the noncausal signal in Fig. (a)a is not everlasting. A signal that is zero for all (t\\\\geq 0) is called an anti-causal signal.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Comment. A true everlasting signal cannot be generated in practice for obvious reasons. Why should we bother to postulate such a signal? In later chapters we shall see that certain signals\\n\\nFigure 13: Generation of a periodic signal by periodic extension of its segment of one-period duration.\\n\\n(e.g., an impulse and an everlasting sinusoid) that cannot be generated in practice do serve a very useful purpose in the study of signals and systems.\\n\\n1.3-4 Energy and Power Signals', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='A signal with finite energy is an energy signal, and a signal with finite and nonzero power is a power signal. The signals in Figs. 1.2a and 1.2b are examples of energy and power signals, respectively. Observe that power is the time average of energy. Since the averaging is over an infinitely large interval, a signal with finite energy has zero power, and a signal with finite power has infinite energy. Therefore, a signal cannot be both an energy signal and a power signal. If it is one, it', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='cannot be the other. On the other hand, there are signals that are neither energy nor power signals. The ramp signal is one such case.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Comments. All practical signals have finite energies and are therefore energy signals. A power signal must necessarily have infinite duration; otherwise, its power, which is its energy averaged over an infinitely large interval, will not approach a (nonzero) limit. Clearly, it is impossible to generate a true power signal in practice because such a signal has infinite duration and infinite energy.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Also, because of periodic repetition, periodic signals for which the area under (|x(t)|^{2}) over one period is finite are power signals; however, not all power signals are periodic.\\n\\nDrill 1.6: Neither Energy nor Power\\n\\nShow that an everlasting exponential (e^{-at}) is neither an energy nor a power signal for any real value of (a). However, if (a) is imaginary, it is a power signal with power (P_{x}=1) regardless of the value of (a).\\n\\n1.3-5 Deterministic and Random Signals', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='A signal whose physical description is known completely, in either a mathematical form or a graphical form, is a deterministic signal. A signal whose values cannot be predicted precisely but are known only in terms of probabilistic description, such as mean value or mean-squared value, is a random signal. In this book we shall exclusively deal with deterministic signals. Random signals are beyond the scope of this study.\\n\\n1.4 Some Useful Signal Models', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='In the area of signals and systems, the step, the impulse, and the exponential functions play very important roles. Not only do they serve as a basis for representing other signals, but their use can simplify many aspects of the signals and systems.\\n\\n1.4 Some Useful Signal Models\\n\\nThe Unit Step Function (u(t))', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='In much of our discussion, the signals begin at (t=0) (causal signals). Such signals can be conveniently described in terms of unit step function (u(t)) shown in Fig. 1.14a. This function is defined by\\n\\n[u(t)=\\\\left{\\\\begin{array}{ll}1&t\\\\geq 0\\\\ 0&t<0\\\\end{array}\\\\right. \\\\tag{1.8}]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='If we want a signal to start at (t=0) (so that it has a value of zero for (t<0)), we need only multiply the signal by (u(t)). For instance, the signal (e^{-at}) represents an everlasting exponential that starts at (t=-\\\\infty). The causal form of this exponential (Fig. 1.14b) can be described as (e^{-at}u(t)).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='The unit step function also proves very useful in specifying a function with different mathematical descriptions over different intervals. Examples of such functions appear in Fig. 1.7. These functions have different mathematical descriptions over different segments of time, as seen from Eqs. (1.5) and (1.6). Such a description often proves clumsy and inconvenient in mathematical treatment. We can use the unit step function to describe such functions by a single expression that is valid for all', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='(t).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Consider, for example, the rectangular pulse depicted in Fig. 1.15a. We can express such a pulse in terms of familiar step functions by observing that the pulse (x(t)) can be expressed as the sum of the two delayed unit step functions, as shown in Fig. 1.15b. The unit step function (u(t)) delayed by (T) seconds is (u(t-T)). From Fig. 1.15b, it is clear that\\n\\n[x(t)=u(t-2)-u(t-4)]\\n\\nFigure 1.14: (a) Unit step function (u(t)). (b) Exponential (e^{-at}u(t)).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Figure 1.15: Representation of a rectangular pulse by step functions.\\n\\nChapter 1 Signals and Systems\\n\\n1.6 Describing a Triangle Function with the Unit Step\\n\\nUse the unit step function to describe the signal in Fig. 1.16a.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='The signal illustrated in Fig. 1.16a can be conveniently handled by breaking it up into the two components (x_{1}(t)) and (x_{2}(t)), depicted in Figs. 1.16b and 1.16c, respectively. Here, (x_{1}(t)) can be obtained by multiplying the ramp (t) by the gate pulse (u(t)-u(t-2)), as shown in Fig. 1.16b. Therefore,\\n\\n[x_{1}(t)=t[u(t)-u(t-2)]]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='The signal (x_{2}(t)) can be obtained by multiplying another ramp by the gate pulse illustrated in Fig. 1.16c. This ramp has a slope (-2); hence it can be described by (-2t+c). Now, because the ramp has a zero value at (t=3), the constant (c=6), and the ramp can be described by (-2(t-3)). Also, the gate pulse in Fig. 1.16c is (u(t-2)-u(t-3)). Therefore,\\n\\n[x_{2}(t)=-2(t-3)[u(t-2)-u(t-3)]]\\n\\nFigure 1.16: Representation of a signal defined interval by interval.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[x(t) =x_{1}(t)+x_{2}(t)] [=t[u(t)-u(t-2)]-2(t-3)\\\\left[u(t-2)-u(t-3)\\\\right]] [=tu(t)-3(t-2)u(t-2)+2(t-3)u(t-3)]\\n\\nExample 1.7: Describing a Piecewise Function with the Unit Step\\n\\nDescribe the signal in Fig. 1.7a by a single expression valid for all (t).\\n\\nOver the interval from (-1.5) to (0), the signal can be described by a constant (2), and over the interval from (0) to (3), it can be described by (2\\\\,e^{-t/2}). Therefore,', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[x(t) =\\\\underbrace{2[u(t+1.5)-u(t)]}{\\\\text{constant part}}+ \\\\underbrace{2e^{-t/2}[u(t)-u(t-3)]}{\\\\text{exponential part}}] [=2u(t+1.5)-2(1-e^{-t/2})u(t)-2e^{-t/2}u(t-3)]\\n\\nCompare this expression with the expression for the same function found in Eq. (1.6).\\n\\nDrill 1.7: Using Reflected Unit Step Functions\\n\\nShow that the signals depicted in Figs. 1.17a and 1.17b can be described as (u(-t)) and (e^{-at}u(-t)), respectively.\\n\\nFigure 1.17: Signals for Drill 1.7.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='1.8 Describing a Piecewise Function with the Unit Step\\n\\nShow that the signal shown in Fig. 1.18 can be described as\\n\\n[x(t)=(t-1)u(t-1)-(t-2)u(t-2)-u(t-4)]\\n\\n1.4 The Unit Impulse Function (\\\\delta(t))\\n\\nThe unit impulse function (\\\\delta(t)) is one of the most important functions in the study of signals and systems. This function was first defined in two parts by P. A. M. Dirac as\\n\\n[\\\\delta(t)=0\\\\quad t\\\\neq 0\\\\qquad\\\\text{and}\\\\qquad\\\\int_{-\\\\infty}^{\\\\infty}\\\\delta(t) \\\\,dt=1 \\\\tag{1.9}]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='We can visualize an impulse as a tall, narrow, rectangular pulse of unit area, as illustrated in Fig. 1.19b. The width of this rectangular pulse is a very small value (\\\\epsilon\\\\to 0). Consequently, its height is a very large value (1/\\\\epsilon\\\\to\\\\infty). The unit impulse therefore can be regarded as a rectangular pulse with a width that has become infinitesimally small, a height that has become infinitely large, and an overall area that has been maintained at unity. Thus (\\\\delta(t)=0) everywhere', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='except at (t=0), where it is undefined. For this reason, a unit impulse is represented by the separlike symbol in Fig. 1.19a.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Other pulses, such as the exponential, triangular, or Gaussian types, may also be used in impulse approximation. The important feature of the unit impulse function is not its shape but the fact that its effective duration (pulse width) approaches zero while its area remains at unity. For example, the exponential pulse (\\\\alpha e^{-\\\\alpha t}u(t)) in Fig. 1.20a becomes taller and narrower as (\\\\alpha) increases.\\n\\nFigure 1.19: A unit impulse and its approximation.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Figure 1.18: Signal for Drill 1.8.\\n\\nIn the limit as (\\\\alpha\\\\rightarrow\\\\infty), the pulse height (\\\\rightarrow\\\\infty), and its width or duration (\\\\to 0). Yet, the area under the pulse is unity regardless of the value of (\\\\alpha) because\\n\\n[\\\\int_{0}^{\\\\infty}\\\\alpha e^{-\\\\alpha t}dt=1]\\n\\nThe pulses in Figs. 1.20b and 1.20c behave in a similar fashion. Clearly, the exact impulse function cannot be generated in practice; it can only be approached.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='From Eq. (9), it follows that the function (k\\\\delta(t)=0) for all (t\\\\neq 0), and its area is (k). Thus, (k\\\\delta(t)) is an impulse function whose area is (k) (in contrast to the unit impulse function, whose area is (1)).\\n\\nMultiplication of a Function by an Impulse', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Let us now consider what happens when we multiply the unit impulse (\\\\delta(t)) by a function (\\\\phi(t)) that is known to be continuous at (t=0). Since the impulse has nonzero value only at (t=0), and the value of (\\\\phi(t)) at (t=0) is (\\\\phi(0)), we obtain\\n\\n[\\\\phi(t)\\\\delta(t)=\\\\phi(0)\\\\delta(t)]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Thus, multiplication of a continuous-time function (\\\\phi(t)) with an unit impulse located at (t=0) results in an impulse, which is located at (t=0) and has strength (\\\\phi(0)) [the value of (\\\\phi(t)) at the location of the impulse]. Use of exactly the same argument leads to the generalization of this result, stating that provided (\\\\phi(t)) is continuous at (t=T,\\\\phi(t)) multiplied by an impulse (\\\\delta(t-T)) (impulse located at (t=T)) results in an impulse located at (t=T) and having strength', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='(\\\\phi(T)) [the value of (\\\\phi(t)) at the location of the impulse].', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[\\\\phi(t)\\\\delta(t-T)=\\\\phi(T)\\\\delta(t-T) \\\\tag{10}]\\n\\nSampling Property of the Unit Impulse Function\\n\\nFrom Eq. (10) it follows that\\n\\n[\\\\int_{-\\\\infty}^{\\\\infty}\\\\phi(t)\\\\delta(t-T)\\\\,dt=\\\\phi(T)\\\\int_{-\\\\infty}^{\\\\infty} \\\\delta(t)\\\\,dt=\\\\phi(T) \\\\tag{11}]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='provided (\\\\phi(t)) is continuous at (t=T). This result means that the area under the product of a function with an impulse (\\\\delta(t-T)) is equal to the value of that function at the instant at which the unit impulse is located. This property is very important and useful and is known as the sampling or sifting property of the unit impulse.\\n\\nFigure 10: Other possible approximations to a unit impulse.\\n\\nChapter 1 Signals and Systems\\n\\n1.1 Unit Impulse as a Generalized Function', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='The definition of the unit impulse function given in Eq. (1.9) is not mathematically rigorous, which leads to serious difficulties. First, the impulse function does not define a unique function: for example, it can be shown that (\\\\delta(t)+\\\\dot{\\\\delta}(t)) also satisfies Eq. (1.9) [1]. Moreover, (\\\\delta(t)) is not even a true function in the ordinary sense. An ordinary function is specified by its values for all time (t). The impulse function is zero everywhere except at (t=0), and at this, the', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='only interesting part of its range, it is undefined. These difficulties are resolved by defining the impulse as a generalized function rather than an ordinary function. A generalized function is defined by its effect on other functions instead of by its value at every instant of time.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='In this approach the impulse function is defined by the sampling property [Eq. (1.11)]. We say nothing about what the impulse function is or what it looks like. Instead, the impulse function is defined in terms of its effect on a test function (\\\\phi(t)). We define a unit impulse as a function for which the area under its product with a function (\\\\phi(t)) is equal to the value of the function (\\\\phi(t)) at the instant at which the impulse is located. It is assumed that (\\\\phi(t)) is continuous at', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='the location of the impulse. Recall that the sampling property [Eq. (1.11)] is the consequence of the classical (Dirac) definition of the unit impulse in Eq. (1.9). In contrast, the sampling property [Eq. (1.11)] defines the impulse function in the generalized function approach.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='We now present an interesting application of the generalized function definition of an impulse. Because the unit step function (u(t)) is discontinuous at (t=0), its derivative (du/dt) does not exist at (t=0) in the ordinary sense. We now show that this derivative does exist in the generalized sense, and it is, in fact, (\\\\delta(t)). As a proof, let us evaluate the integral of ((du/dt)\\\\phi(t)), using integration by parts:', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[\\\\int_{-\\\\infty}^{\\\\infty}\\\\frac{du(t)}{dt}\\\\phi(t)\\\\,dt =u(t)\\\\phi(t)\\\\bigg{|}{-\\\\infty}^{\\\\infty}-\\\\int{-\\\\infty}^{\\\\infty}u(t )\\\\dot{\\\\phi}(t)\\\\,dt] [=\\\\phi(\\\\infty)-0-\\\\int_{0}^{\\\\infty}\\\\dot{\\\\phi}(t)\\\\,dt] [=\\\\phi(\\\\infty)-\\\\phi(t)|_{0}^{\\\\infty}=\\\\phi(0)]\\n\\nThis result shows that (du/dt) satisfies the sampling property of (\\\\delta(t)). Therefore it is an impulse (\\\\delta(t)) in the generalized sense--that is,\\n\\n[\\\\frac{du(t)}{dt}=\\\\delta(t) \\\\tag{1.12}]\\n\\nConsequently,\\n\\n[\\\\int_{-\\\\infty}^{t}\\\\delta(\\\\tau)\\\\,d\\\\tau=u(t)]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='These results can also be obtained graphically from Fig. 1.19b. We observe that the area from (-\\\\infty) to (t) under the limiting form of (\\\\delta(t)) in Fig. 1.19b is zero if (t<-\\\\epsilon/2) and unity if (t\\\\geq\\\\epsilon/2) with (\\\\epsilon\\\\to 0). Consequently,', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[\\\\int_{-\\\\infty}^{t}\\\\delta(\\\\tau)\\\\,d\\\\tau =\\\\begin{cases}0&t<0\\\\ 1&t\\\\geq 0\\\\end{cases}] [=u(t)]This result shows that the unit step function can be obtained by integrating the unit impulse function. Similarly the unit ramp function (x(t)=tu(t)) can be obtained by integrating the unit step function. We may continue with unit parabolic function (t^{2}/2) obtained by integrating the unit ramp, and so on. On the other side, we have derivatives of impulse function, which can be defined as generalized', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='functions (see Prob. 1.4-12). All these functions, derived from the unit impulse function (successive derivatives and integrals), are called singularity functions.+', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Footnote †: Singularity functions were defined by late Prof. S. J. Mason as follows. A singularity is a point at which a function does not possess a derivative. Each of the singularity functions (or if not the function itself, then the function differentiated a finite number of times) has a singular point at the origin and is zero elsewhere [2].\\n\\nTherefore,\\n\\n[e^{st}=e^{(\\\\sigma+j\\\\omega)t}=e^{\\\\sigma t}e^{j\\\\omega t}=e^{\\\\sigma t}(\\\\cos\\\\omega t +j\\\\sin\\\\omega t) \\\\tag{1.13}]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Since (s^{*}=\\\\sigma-j\\\\omega) (the conjugate of (s)), then\\n\\n[e^{s^{*}t}=e^{(\\\\sigma-j\\\\omega)t}=e^{\\\\sigma t}e^{-j\\\\omega t}=e^{\\\\sigma t}(\\\\cos \\\\omega t-j\\\\sin\\\\omega t)]\\n\\nand\\n\\n[e^{\\\\sigma t}\\\\cos\\\\omega t=\\\\tfrac{1}{2}(e^{st}+e^{s^{*}t}) \\\\tag{1.14}]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content=\"A comparison of Eq. (1.13) with Euler's formula shows that (e^{st}) is a generalization of the function (e^{j\\\\omega t}), where the frequency variable (j\\\\omega) is generalized to a complex variable (s=\\\\sigma+j\\\\omega). For this reason, we designate the variable (s) as the complex frequency. In fact, function (e^{st}) encompasses a large class of functions. The following functions are either special cases of or can be expressed in terms of (e^{st}):\\n\\nA constant (k=ke^{\\\\sigma t}\\\\qquad(s=0))\", metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='A monotonic exponential (e^{\\\\sigma t}\\\\qquad(\\\\omega=0,\\\\,s=\\\\sigma))\\n\\nA sinusoid (\\\\cos\\\\omega t\\\\qquad(\\\\sigma=0,\\\\,s=\\\\pm j\\\\omega))\\n\\nAn exponentially varying sinusoid (e^{\\\\sigma t}\\\\cos\\\\omega t\\\\qquad(s=\\\\sigma\\\\pm j\\\\omega))\\n\\nThese functions are illustrated in Fig. 1.21.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='The complex frequency (s) can be conveniently represented on a complex frequency plane ((s) plane), as depicted in Fig. 1.22. The horizontal axis is the real axis ((\\\\sigma) axis), and the vertical axis is the imaginary axis ((\\\\omega) axis). The absolute value of the imaginary part of (s) is (|\\\\omega|) (the\\n\\nFigure 1.21: Sinusoids of complex frequency (\\\\sigma+j\\\\omega).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='radian frequency), which indicates the frequency of oscillation of (e^{st}); the real part (\\\\sigma) (the neper frequency) gives information about the rate of increase or decrease of the amplitude of (e^{st}). For signals whose complex frequencies lie on the real axis ((\\\\sigma) axis, where (\\\\omega=0)), the frequency of oscillation is zero. Consequently these signals are monotonically increasing or decreasing exponentials (Fig. 21a). For signals whose frequencies lie on the imaginary axis', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='((\\\\omega) axis, where (\\\\sigma=0)), (e^{\\\\sigma t}=1). Therefore, these signals are conventional sinusoids with constant amplitude (Fig. 21b). The case (s=0) ((\\\\sigma=\\\\omega=0)) corresponds to a constant (dc) signal because (e^{0t}=1). For the signals illustrated in Figs. 21c and 21d, both (\\\\sigma) and (\\\\omega) are nonzero; the frequency (s) is complex and does not lie on either axis. The signal in Fig. 21c decays exponentially. Therefore, (\\\\sigma) is negative, and (s) lies to the left of the', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='imaginary axis. In contrast, the signal in Fig. 21d grows exponentially. Therefore, (\\\\sigma) is positive, and (s) lies to the right of the imaginary axis. Thus the (s) plane (Fig. 21) can be separated into two parts: the left half-plane (LHP) corresponding to exponentially decaying signals and the right half-plane (RHP) corresponding to exponentially growing signals. The imaginary axis separates the two regions and corresponds to signals of constant amplitude.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='An exponentially growing sinusoid (e^{2t}\\\\cos 5t), for example, can be expressed as a linear combination of exponentials (e^{(2+j5)t}) and (e^{(2-j5)t}) with complex frequencies (2+j5) and (2-j5), respectively, which lie in the RHP. An exponentially decaying sinusoid (e^{-2t}\\\\cos 5t) can be expressed as a linear combination of exponentials (e^{(-2+j5)t}) and (e^{(-2-j5)t}) with complex frequencies (-2+j5) and (-2-j5), respectively, which lie in the LHP. A constant-amplitude sinusoid (\\\\cos 5t)', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='can be expressed as a linear combination of exponentials (e^{j5t}) and (e^{-j5t}) with complex frequencies (\\\\pm j5), which lie on the imaginary axis. Observe that the monotonic exponentials (e^{\\\\pm 2t}) are also generalized sinusoids with complex frequencies (\\\\pm 2).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Figure 22: Complex frequency plane.\\n\\nChapter 1 Signals and Systems\\n\\n1.5 Even and Odd Functions\\n\\nA function (x_{e}(t)) is said to be an even function of (t) if it is symmetrical about the vertical axis. A function (x_{o}(t)) is said to be an odd function of (t) if it is antisymmetrical about the vertical axis. Mathematically expressed, these symmetry conditions require\\n\\n[x_{e}(t)=x_{e}(-t)\\\\qquad\\\\text{and}\\\\qquad x_{o}(t)=-x_{o}(-t) \\\\tag{1.15}]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='An even function has the same value at the instants (t) and (-t) for all values of (t). On the other hand, the value of an odd function at the instant (t) is the negative of its value at the instant (-t). An example even signal and an example odd signal are shown in Figs. 1.23a and 1.23b, respectively.\\n\\n1.5-1 Some Properties of Even and Odd Functions\\n\\nEven and odd functions have the following properties:', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[\\\\text{even function}\\\\times\\\\text{odd function}=\\\\text{odd function}] [\\\\text{odd function}\\\\times\\\\text{odd function}=\\\\text{even function}] [\\\\text{even function}\\\\times\\\\text{even function}=\\\\text{even function}]\\n\\nThe proofs are trivial and follow directly from the definition of odd and even functions [Eq. (1.15)].\\n\\nFigure 1.23: Functions of (t): (a) even and (b) odd.\\n\\nArea\\n\\nBecause of the symmetries of even and odd functions about the vertical axis, it follows from Eq. (1.15) [or Fig. 1.23] that', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[\\\\int_{-a}^{a}x_{e}(t)\\\\,dt=2\\\\int_{0}^{a}x_{e}(t)\\\\,dt\\\\qquad\\\\text{and}\\\\qquad\\\\int_{ -a}^{a}x_{o}(t)\\\\,dt=0 \\\\tag{1.16}]\\n\\nThese results are valid under the assumption that there is no impulse (or its derivatives) at the origin. The proof of these statements is obvious from the plots of even and odd functions. Formal proofs, left as an exercise for the reader, can be accomplished by using the definitions in Eq. (1.15).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Because of their properties, study of odd and even functions proves useful in many applications, as will become evident in later chapters.\\n\\nEven and Odd Components of a Signal\\n\\nEvery signal (x(t)) can be expressed as a sum of even and odd components because\\n\\n[x(t)=\\\\underbrace{\\\\tfrac{1}{2}[x(t)+x(-t)]}{\\\\text{even}}+\\\\underbrace{\\\\tfrac{1 }{2}[x(t)-x(-t)]}{\\\\text{odd}} \\\\tag{1.17}]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='From the definitions in Eq. (1.15), we can clearly see that the first component on the right-hand side is an even function, while the second component is odd. This is apparent from the fact that replacing (t) by (-t) in the first component yields the same function. The same maneuver in the second component yields the negative of that component.\\n\\nExample 1.8: Finding the Even and Odd Components of a Signal\\n\\nFind and sketch the even and odd components of (x(t)=e^{-at}u(t)).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Based on Eq. (1.17), we can express (x(t)) as a sum of the even component (x_{e}(t)) and the odd component (x_{o}(t)) as\\n\\n[x(t)=x_{e}(t)+x_{o}(t)]\\n\\nwhere\\n\\n[x_{e}(t)=\\\\tfrac{1}{2}[e^{-at}u(t)+e^{at}u(-t)]\\\\qquad\\\\text{and}\\\\qquad x_{o}(t)= \\\\tfrac{1}{2}[e^{-at}u(t)-e^{at}u(-t)]]\\n\\nThe function (e^{-at}u(t)) and its even and odd components are illustrated in Fig. 1.24.\\n\\nExample 1.9: Finding the Even and Odd Components of a Complex Signal\\n\\nFind the even and odd components of (e^{it}).\\n\\nFrom Eq. (1.17),', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[e^{it}=x_{e}(t)+x_{o}(t)]\\n\\nwhere\\n\\n[x_{e}(t)=\\\\tfrac{1}{2}[e^{it}+e^{-jt}]=\\\\cos t\\\\qquad\\\\text{and}\\\\qquad x_{o}(t)= \\\\tfrac{1}{2}[e^{it}-e^{-jt}]=j\\\\sin t]\\n\\nFigure 1.24: Finding even and odd components of a signal.\\n\\nChapter A Modification for Complex Signals', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='While a complex signal can be decomposed into even and odd components, it is more common to decompose complex signals using conjugate symmetries. A complex signal (x(t)) is said to be conjugate-symmetric if (x(t)=x^{}(-t)). A conjugate-symmetric signal is even in the real part and odd in the imaginary part. Thus, a real conjugate-symmetric signal is an even signal. A signal is conjugate-antisymmetric if (x(t)=-x^{}(-t)). A conjugate-antisymmetric signal is odd in the real part and even in the', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='imaginary part. A real conjugate-antisymmetric signal is an odd signal. Any signal (x(t)) can be decomposed into a conjugate-symmetric portion (x_{cs}(t)) plus a conjugate-antisymmetric portion (x_{ca}(t)). That is,', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[x(t)=x_{cs}(t)+x_{ca}(t)]\\n\\nwhere\\n\\n[x_{cs}(t)=\\\\frac{x(t)+x^{}(-t)}{2}\\\\qquad\\\\text{and}\\\\qquad x_{ca}(t)=\\\\frac{x(t)-x ^{}(-t)}{2}]\\n\\nThe proof is similar to the one for decomposing a signal into even and odd components. As we shall see in later chapters, conjugate symmetries commonly occur in real-world signals and their transforms.\\n\\n1.6 Systems', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='As mentioned in Sec. 1.1, systems are used to process signals to allow modification or extraction of additional information from the signals. A system may consist of physical components (hardware realization) or of an algorithm that computes the output signal from the input signal (software realization).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content=\"Roughly speaking, a physical system consists of interconnected components, which are characterized by their terminal (input-output) relationships. In addition, a system is governed by laws of interconnection. For example, in electrical systems, the terminal relationships are the familiar voltage-current relationships for the resistors, capacitors, inductors, transformers, transistors, and so on, as well as the laws of interconnection (i.e., Kirchhoff's laws). We use these laws to derive\", metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='mathematical equations relating the outputs to the inputs. These equations then represent a mathematical model of the system.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='A system can be conveniently illustrated by a \"black box\" with one set of accessible terminals where the input variables (x_{1}(t)), (x_{2}(t)), (\\\\ldots),(x_{j}(t)) are applied and another set of accessible terminals where the output variables (y_{1}(t)), (y_{2}(t)),(\\\\ldots),(y_{k}(t)) are observed (Fig. 1.25).\\n\\nThe study of systems consists of three major areas: mathematical modeling, analysis, and design. Although we shall be dealing with mathematical modeling, our main concern is with', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Figure 1.25: Representation of a system.\\n\\nanalysis and design. The major portion of this book is devoted to the analysis problem--how to determine the system outputs for the given inputs and a given mathematical model of the system (or rules governing the system). To a lesser extent, we will also consider the problem of design or synthesis--how to construct a system that will produce a desired set of outputs for the given inputs.\\n\\nData Needed to Compute System Response', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='To understand what data we need to compute a system response, consider a simple (RC) circuit with a current source (x(t)) as its input (Fig. 1.26).\\n\\nThe output voltage (y(t)) is given by\\n\\n[y(t)=Rx(t)+\\\\frac{1}{C}\\\\int_{-\\\\infty}^{t}x(\\\\tau)\\\\,d\\\\tau \\\\tag{1.18}]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='The limits of the integral on the right-hand side are from (-\\\\infty) to (t) because this integral represents the capacitor charge due to the current (x(t)) flowing in the capacitor, and this charge is the result of the current flowing in the capacitor from (-\\\\infty). Now, Eq. (1.18) can be expressed as\\n\\n[y(t)=Rx(t)+\\\\frac{1}{C}\\\\int_{-\\\\infty}^{0}x(\\\\tau)\\\\,d\\\\tau+\\\\frac{1}{C}\\\\int_{0}^{t} x(\\\\tau)\\\\,d\\\\tau]\\n\\nThe middle term on the right-hand side is (v_{C}(0)), the capacitor voltage at (t=0). Therefore,', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[y(t)=v_{C}(0)+Rx(t)+\\\\frac{1}{C}\\\\int_{0}^{t}x(\\\\tau)\\\\,d\\\\tau\\\\qquad t\\\\geq 0]\\n\\nThis equation can be readily generalized as\\n\\n[y(t)=v_{C}(t_{0})+Rx(t)+\\\\frac{1}{C}\\\\int_{t_{0}}^{t}x(\\\\tau)\\\\,d\\\\tau\\\\qquad t\\\\geq t _{0} \\\\tag{1.19}]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='From Eq. (1.18), the output voltage (y(t)) at an instant (t) can be computed if we know the input current flowing in the capacitor throughout its entire past ((-\\\\infty) to (t)). Alternatively, if we know the input current (x(t)) from some moment (t_{0}) onward, then, using Eq. (1.19), we can still calculate (y(t)) for (t\\\\geq t_{0}) from a knowledge of the input current, provided we know (v_{C}(t_{0})), the initial capacitor voltage (voltage at (t_{0})). Thus (v_{C}(t_{0})) contains all the', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content=\"relevant information about the circuit's entire\", metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Figure 1.26: Example of a simple electrical system.\\n\\npast ((-\\\\infty) to (t_{0})) that we need to compute (y(t)) for (t\\\\geq t_{0}). Therefore, the response of a system at (t\\\\geq t_{0}) can be determined from its input(s) during the interval (t_{0}) to (t) and from certain initial conditions at (t=t_{0}).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='In the preceding example, we needed only one initial condition. However, in more complex systems, several initial conditions may be necessary. We know, for example, that in passive RLC networks, the initial values of all inductor currents and all capacitor voltages1 are needed to determine the outputs at any instant (t\\\\geq 0) if the inputs are given over the interval ([0,t]).\\n\\nFootnote 1: Strictly speaking, this means independent inductor currents and capacitor voltages.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='7 Classification of Systems\\n\\nSystems may be classified broadly in the following categories:\\n\\nLinear and nonlinear systems\\n\\nConstant-parameter and time-varying-parameter systems\\n\\nInstantaneous (memoryless) and dynamic (with memory) systems\\n\\nCausal and noncausal systems\\n\\nContinuous-time and discrete-time systems\\n\\nAnalog and digital systems\\n\\nInvertible and noninvertible systems\\n\\nStable and unstable systems', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Other classifications, such as deterministic and probabilistic systems, are beyond the scope of this text and are not considered.\\n\\n7-1 Linear and Nonlinear Systems\\n\\nThe Concept of Linearity', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='A system whose output is proportional to its input is an example of a linear system. But linearity implies more than this; it also implies the additivity property: that is, if several inputs are acting on a system, then the total effect on the system due to all these inputs can be determined by considering one input at a time while assuming all the other inputs to be zero. The total effect is then the sum of all the component effects. This property may be expressed as follows: for a linear', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='system, if an input (x_{1}) acting alone has an effect (y_{1}), and if another input (x_{2}), also acting alone, has an effect (y_{2}), then, with both inputs acting on the system, the total effect will be (y_{1}+y_{2}). Thus, if', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[x_{1}\\\\longrightarrow y_{1}\\\\qquad\\\\mbox{and}\\\\qquad x_{2}\\\\longrightarrow y_{2}]\\n\\nthen for all (x_{1}) and (x_{2})\\n\\n[x_{1}+x_{2}\\\\longrightarrow y_{1}+y_{2} \\\\tag{20}]\\n\\nIn addition, a linear system must satisfy the homogeneity or scaling property, which states that for arbitrary real or imaginary number (k), if an input is increased (k)-fold, the effect also increases (k)-fold. Thus, if\\n\\n[x\\\\longrightarrow y]then for all real or imaginary (k)\\n\\n[kx\\\\longrightarrow ky \\\\tag{1.21}]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Thus, linearity implies two properties: homogeneity (scaling) and additivity.2 Both these properties can be combined into one property (superposition), which is expressed as follows: If\\n\\nFootnote 2: A linear system must also satisfy the additional condition of smoothness, where small changes in the system’s inputs must result in small changes in its outputs [3].\\n\\n[x_{1}\\\\longrightarrow y_{1}\\\\qquad\\\\mbox{and}\\\\qquad x_{2}\\\\longrightarrow y_{2}]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='then for all inputs (x_{1}) and (x_{2}) and all constants (k_{1}) and (k_{2}),\\n\\n[k_{1}x_{1}+k_{2}x_{2}\\\\longrightarrow k_{1}y_{1}+k_{2}y_{2} \\\\tag{1.22}]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='There is another useful way to view the linearity condition described in Eq. (1.22): the response of a linear system is unchanged whether the operations of summing and scaling precede the system (sum and scale act on inputs) or follow the system (sum and scale act on outputs). Thus, linearity implies commutability between a system and the operations of summing and scaling. It may appear that additivity implies homogeneity. Unfortunately, homogeneity does not always follow from additivity. Drill', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='1.11 demonstrates such a case.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Drill 1.11 Additivity but Not Homogeneity\\n\\nShow that a system with the input (x(t)) and the output (y(t)) related by (y(t)=\\\\mbox{Re}{x(t)}) satisfies the additivity property but violates the homogeneity property. Hence, such a system is not linear.\\n\\n[Hint: Show that Eq. (1.21) is not satisfied when (k) is complex.]\\n\\nResponse of a Linear System', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='For the sake of simplicity, we discuss only single-input, single-output (SISO) systems. But the discussion can be readily extended to multiple-input, multiple-output (MIMO) systems.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content=\"A system's output for (t\\\\geq 0) is the result of two independent causes: the initial conditions of the system (or the system state) at (t=0) and the input (x(t)) for (t\\\\geq 0). If a system is to be linear, the output must be the sum of the two components resulting from these two causes: first, the zero-input response (ZIR) that results only from the initial conditions at (t=0) with the input (x(t)=0) for (t\\\\geq 0), and then the zero-state response (ZSR) that results only from the input (x(t))\", metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='for (t\\\\geq 0) when the initial conditions (at (t=0)) are assumed to be zero. When all the appropriate initial conditions are zero, the system is said to be in zero state. The system output is zero when the input is zero only if the system is in zero state.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='In summary, a linear system response can be expressed as the sum of the zero-input and zero-state responses:\\n\\ntotal response = zero-input response + zero-state response This property of linear systems, which permits the separation of an output into components resulting from the initial conditions and from the input, is called the decomposition property. For the (RC) circuit of Fig. 1.26, the response (y(t)) was found to be [see Eq. (19) with (t_{0}=0)]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[y(t)=\\\\underbrace{v_{C}(0)}{\\\\text{ZIR}}+\\\\underbrace{Rx(t)+\\\\frac{1}{C}\\\\int{0}^ {t}x(\\\\tau)\\\\,d\\\\tau}_{\\\\text{ZSR}} \\\\tag{23}]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='From Eq. (23), it is clear that if the input (x(t)=0) for (t\\\\geq 0), the output (y(t)=v_{C}(0)). Hence (v_{C}(0)) is the zero-input response of the response (y(t)). Similarly, if the system state (the voltage (v_{C}) in this case) is zero at (t=0), the output is given by the second component on the right-hand side of Eq. (23). Clearly this is the zero-state response of the response (y(t)).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='In addition to the decomposition property, linearity implies that both the zero-input and zero-state components must obey the principle of superposition with respect to each of their respective causes. For example, if we increase the initial condition (k)-fold, the zero-input response must also increase (k)-fold. Similarly, if we increase the input (k)-fold, the zero-state response must also increase (k)-fold. These facts can be readily verified from Eq. (23) for the (RC) circuit in Fig. 1.26.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='For instance, if we double the initial condition (v_{C}(0)), the zero-input response doubles; if we double the input (x(t)), the zero-state response doubles.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Example 1.10: Linearity of Constant-Coefficient Linear Differential Equations\\n\\nShow that the system described by the equation\\n\\n[\\\\frac{dy(t)}{dt}+3y(t)=x(t) \\\\tag{24}]\\n\\nis linear.\\n\\nLet the system response to the inputs (x_{1}(t)) and (x_{2}(t)) be (y_{1}(t)) and (y_{2}(t)), respectively. Then\\n\\n[\\\\frac{dy_{1}(t)}{dt}+3y_{1}(t)=x_{1}(t)\\\\qquad\\\\text{and}\\\\qquad\\\\frac{dy_{2}(t)}{ dt}+3y_{2}(t)=x_{2}(t)]\\n\\nMultiplying the first equation by (k_{1}), the second by (k_{2}), and adding them yield', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[\\\\frac{d}{dt}[k_{1}y_{1}(t)+k_{2}y_{2}(t)]+3[k_{1}y_{1}(t)+k_{2}y_{2}(t)]=k_{1 }x_{1}(t)+k_{2}x_{2}(t)]\\n\\nBut this equation is the system equation [Eq. (24)] with\\n\\n[x(t)=k_{1}x_{1}(t)+k_{2}x_{2}(t)\\\\qquad\\\\text{and}\\\\qquad y(t)=k_{1}y_{1}(t)+k_{2 }y_{2}(t)]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Therefore, when the input is (k_{1}x_{1}(t)+k_{2}x_{2}(t)), the system response is (k_{1}y_{1}(t)+k_{2}y_{2}(t)). Consequently, the system is linear. Using this argument, we can readily generalize the result to show that a system described by a differential equation of the form\\n\\n[a_{0}\\\\frac{d^{N}y(t)}{dt^{N}}+a_{1}\\\\frac{d^{N-1}y(t)}{dt^{N-1}}+\\\\cdot\\\\cdot\\\\cdot+a {N}y(t)=b{N-M}\\\\frac{d^{M}x(t)}{dt^{M}}+\\\\cdot\\\\cdot\\\\cdot+b_{N-1}\\\\frac{dx(t)}{dt} +b_{N}x(t) \\\\tag{1.25}]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='is a linear system. The coefficients (a_{i}) and (b_{i}) in this equation can be constants or functions of time. Although here we proved only zero-state linearity, it can be shown that such systems are also zero-input linear and have the decomposition property.\\n\\nDrill 1.12: Linearity of a Differential Equation with Time-Varying Parameters\\n\\nShow that the system described by the following equation is linear:\\n\\n[\\\\frac{dy(t)}{dt}+t^{2}y(t)=(2t+3)x(t)]\\n\\nDrill 1.13: A Nonlinear Differential Equation', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Show that the system described by the following equation is nonlinear:\\n\\n[y(t)\\\\frac{dy(t)}{dt}+3y(t)=x(t)]\\n\\nMore Comments on Linear Systems', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Almost all systems observed in practice become nonlinear when large enough signals are applied to them. However, it is possible to approximate most of the nonlinear systems by linear systems for small-signal analysis. The analysis of nonlinear systems is generally difficult. Nonlinearities can arise in so many ways that describing them with a common mathematical form is impossible. Not only is each system a category in itself, but even for a given system, changes in initial conditions or input', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='amplitudes may change the nature of the problem. On the other hand, the superposition property of linear systems is a powerful unifying principle that allows for a general solution. The superposition property (linearity) greatly simplifies the analysis of linear systems. Because of the decomposition property, we can evaluate separately the two components of the output. The zero-input response can be computed by assuming the input to be zero, and the zero-state response can be computed by', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='assuming zero initial conditions. Moreover, if we express an input (x(t)) as a sum of simpler functions,', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[x(t)=a_{1}x_{1}(t)+a_{2}x_{2}(t)+\\\\cdot\\\\cdot\\\\cdot+a_{m}x_{m}(t)]\\n\\nthen, by virtue of linearity, the response (y(t)) is given by\\n\\n[y(t)=a_{1}y_{1}(t)+a_{2}y_{2}(t)+\\\\cdot\\\\cdot\\\\cdot+a_{m}y_{m}(t)]\\n\\nwhere (y_{k}(t)) is the zero-state response to an input (x_{k}(t)). This apparently trivial observation has profound implications. As we shall see repeatedly in later chapters, it proves extremely useful and opens new avenues for analyzing linear systems.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='For example, consider an arbitrary input (x(t)) such as the one shown in Fig. 1.27a. We can approximate (x(t)) with a sum of rectangular pulses of width (\\\\Delta t) and of varying heights. The approximation improves as (\\\\Delta t\\\\to 0), when the rectangular pulses become impulses spaced (\\\\Delta t) seconds apart (with (\\\\Delta t\\\\to 0)).2 Thus, an arbitrary input can be replaced by a weighted sum of impulses spaced (\\\\Delta t) ((\\\\Delta t\\\\to 0)) seconds apart. Therefore, if we know the system response', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='to a unit impulse, we can immediately determine the system response to an arbitrary input (x(t)) by adding the system response to each impulse component of (x(t)). A similar situation is depicted in Fig. 1.27b, where (x(t)) is approximated by a sum of step functions of varying magnitude and spaced (\\\\Delta t) seconds apart. The approximation improves as (\\\\Delta t) becomes smaller. Therefore, if we know the system response to a unit step input, we can compute the system response to any arbitrary', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='input (x(t)) with relative ease. Time-domain analysis of linear systems (discussed in Ch. 2) uses this approach.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Footnote 2: Here, the discussion of a rectangular pulse approaching an impulse at (\\\\Delta t\\\\to 0) is somewhat imprecise. It is explained in Sec. 2.4 with more rigor.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Chapters 4, 5, 6, and 7 employ the same approach but instead use sinusoids or exponentials as the basic signal components. We show that any arbitrary input signal can be expressed as a weighted sum of sinusoids (or exponentials) having various frequencies. Thus a knowledge of the system response to a sinusoid enables us to determine the system response to an arbitrary input (x(t)).\\n\\nFigure 1.27: Signal representation in terms of impulse and step components.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='1.7-2 Time-Invariant and Time-Varying Systems', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Systems whose parameters do not change with time are time-invariant (also constant-parameter) systems. For such a system, if the input is delayed by (T) seconds, the output is the same as before but delayed by (T) (assuming initial conditions are also delayed by (T)). This property is expressed graphically in Fig. 1.28. We can also illustrate this property, as shown in Fig. 1.29. We can delay the output (y(t)) of a system (\\\\mathcal{S}) by applying the output (y(t)) to a (T) second delay (Fig.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='1.29a). If the system is time invariant, then the delayed output (y(t-T)) can also be obtained by first delaying the input (x(t)) before applying it to the system, as shown in Fig. 1.29b. In other words, the system (\\\\mathcal{S}) and the time delay commute if the system (\\\\mathcal{S}) is time invariant. This would not be true for time-varying systems. Consider, for instance, a time-varying system specified by (y(t)=e^{-t}x(t)). The output for such a system in Fig. 1.29a is (e^{-(t-T)}x(t-T)). In', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='contrast, the output for the system in Fig. 1.29b is (e^{-t}x(t-T)).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Figure 1.28: Time-invariance property.\\n\\nFigure 1.29: Illustration of time-invariance property.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='It is possible to verify that the system in Fig. 26 is a time-invariant system. Networks composed of (RLC) elements and other commonly used active elements such as transistors are time-invariant systems. A system with an input-output relationship described by a linear differential equation of the form given in Ex. 10 [Eq. (25)] is a linear time-invariant (LTI) system when the coefficients (a_{i}) and (b_{i}) of such equation are constants. If these coefficients are functions of time, then the', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='system is a linear time-varying system.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='The system described in Drill 12 is linear time varying. Another familiar example of a time-varying system is the carbon microphone, in which the resistance (R) is a function of the mechanical pressure generated by sound waves on the carbon granules of the microphone. The output current from the microphone is thus modulated by the sound waves, as desired.\\n\\nEXAMPLE 1.11 Assessing System Time Invariance', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Determine the time invariance of the following systems: (a)(y(t)=x(t)u(t)) and (b)(y(t)=\\\\frac{d}{dt}x(t)).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='(a) In this case, the output equals the input for (t\\\\geq 0) and is otherwise zero. Clearly, the input is being modified by a time-dependent function, so the system is likely time variant. We can prove that the system is not time invariant through a counterexample. Letting (x_{1}(t)=\\\\delta(t+1)), we see that (y_{1}(t)=0). However, (x_{2}(t)=x_{1}(t-2)=\\\\delta(t-1)) produces an output of (y_{2}(t)=\\\\delta(t-1)), which does equal (y_{1}(t-2)=0) as time-invariance would require. Thus, (y(t)=x(t)u(t))', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='is a time variant system.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='(b) Although it appears that (x(t)) is being modified by a time-dependent function, this is not the case. The output of this system is simply the slope of the input. If the input is delayed, so too is the output. Applying input (x(t)) to the system produces output (y(t)=\\\\frac{d}{dt}x(t)); delaying this output by (T) produces (y(t-T)=\\\\frac{d}{dt(t-T)}x(t-T)=\\\\frac{d}{dt}x(t-T)). This is just the output of the system to a delayed input (x(t-T)). Since the (T)-delayed output of the system to input', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='(x(t)) equals the output of the system to the (T)-delayed input (x(t-T)), the system is time invariant.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='DRILL 1.14 A Time-Variant System\\n\\nShow that a system described by the following equation is a time-varying-parameter system:\\n\\n[y(t)=(\\\\sin t)x(t-2)]\\n\\n[Hint: Show that the system fails to satisfy the time-invariance property.]\\n\\n1.7-3 Instantaneous and Dynamic Systems', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content=\"As observed earlier, a system's output at any instant (t) generally depends on the entire past input. However, in a special class of systems, the output at any instant (t) depends only on its input at that instant. In resistive networks, for example, any output of the network at some instant (t) depends only on the input at the instant (t). In these systems, past history is irrelevant in determining the response. Such systems are said to be instantaneous or memoryless systems. More precisely, a\", metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='system is said to be instantaneous (or memoryless) if its output at any instant (t) depends, at most, on the strength of its input(s) at the same instant (t), and not on any past or future values of the input(s). Otherwise, the system is said to be dynamic (or a system with memory). A system whose response at (t) is completely determined by the input signals over the past (T) seconds [interval from ((t-T)) to (t)] is a finite-memory system with a memory of (T) seconds. Networks containing', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='inductive and capacitive elements generally have infinite memory because the response of such networks at any instant (t) is determined by their inputs over the entire past ((-\\\\infty,t)). This is true for the (RC) circuit of Fig. 1.26.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Example 1.12: Assessing System Memory\\n\\nDetermine whether the following systems are memoryless: (a)(y(t-1)=2x(t-1)), (b)(y(t)=\\\\frac{d}{dt}x(t)), and (c)(y(t)=(t-1)x(t)).\\n\\n(a) In this case, the output at time (t-1) is just twice the input at the same time (t-1). Since the output at a particular time depends only on the strength of the input at the same time, the system is memoryless.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='(b) Although it appears that the output (y(t)) at time (t) depends on the input (x(t)) at the same time (t), we know that the slope (derivative) of (x(t)) cannot be determined solely from a single point. There must be some memory, even if infinitesimally small, involved. This is confirmed by using the fundamental theorem of calculus to express the system as\\n\\n[y(t)=\\\\lim_{T\\\\to 0}\\\\frac{x(t)-x(t-T)}{T}]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Since the output at a particular time depends on more than just the input at the same time, the system is not memoryless.\\n\\n(c) The output (y(t)) at time (t) is just the input (x(t)) at the same time (t) multiplied by the (time-dependent) coefficient (t-1). Since the output at a particular time depends only on the strength of the input at the same time, the system is memoryless.\\n\\n1.7-4 Causal and Noncausal Systems', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='A causal (also known as a physical or nonanticipative) system is one for which the output at any instant (t_{0}) depends only on the value of the input (x(t)) for (t\\\\leq t_{0}). In other words, the value of the output at the present instant depends only on the past and present values of the input (x(t)), not on its future values. To put it simply, in a causal system the output cannot start before the input is applied. If the response starts before the input, it means that the system knows the', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='input in the future and acts on this knowledge before the input is applied. A system that violates the condition of causality is called a noncausal (or anticipative) system.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Any practical system that operates in real time+ must necessarily be causal. We do not yet know how to build a system that can respond to future inputs (inputs not yet applied). A noncausal system is a prophetic system that knows the future input and acts on it in the present. Thus, if we apply an input starting at (t=0) to a noncausal system, the output would begin even before (t=0). For example, consider the system specified by\\n\\nFootnote †: margin: (\\\\diamondsuit)', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[y(t)=x(t-2)+x(t+2) \\\\tag{26}]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='For the input (x(t)) illustrated in Fig. 30a, the output (y(t)), as computed from Eq. (26) (shown in Fig. 30b), starts even before the input is applied. Equation (26) shows that (y(t)), the output at (t), is given by the sum of the input values 2 seconds before and 2 seconds after (t) (at (t-2) and (t+2), respectively). But if we are operating the system in real time at (t), we do not know what the value of the input will be 2 seconds later. Thus it is impossible to implement this system in', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='real time. For this reason, noncausal systems are unrealizable in real time.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='13 Assessing System Causality\\n\\nDetermine whether the following systems are causal: (a)(y(t)=x(-t)), (b)(y(t)=x(t+1)), and (c)(y(t+1)=x(t)).\\n\\nFigure 30: Input–output of a noncausal system and the causal output achieved by delay.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='(a) Here, the output is a reflection of the input. We can easily use a counterexample to disprove the causality of this system. The input (x(t)=\\\\delta(t-1)), which is nonzero at (t=1), produces an output (y(t)=\\\\delta(t+1)), which is nonzero at (t=-1), a time (2) seconds earlier than the input! Clearly the system is not causal.\\n\\n(b) In this case, the output at time (t) depends on the input at future time of (t+1). Clearly the system is not causal.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='(c) In this case, the output at time (t+1) depends on the input one second in the past, at time (t). Since the output does not depend on future values of the input, the system is causal.\\n\\nWhy Study Noncausal Systems?', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='The foregoing discussion may suggest that noncausal systems have no practical purpose. This is not the case; they are valuable in the study of systems for several reasons. First, noncausal systems are realizable when the independent variable is other than \"time\" (e.g., space). Consider, for example, an electric charge of density (q(x)) placed along the (x) axis for (x\\\\geq 0). This charge density produces an electric field (E(x)) that is present at every point on the (x) axis from (x=-\\\\infty) to', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='(\\\\infty). In this case the input [i.e., the charge density (q(x))] starts at (x=0), but its output [the electric field (E(x))] begins before (x=0). Clearly, this space-charge system is noncausal. This discussion shows that only temporal systems (systems with time as independent variable) must be causal to be realizable. The terms \"before\" and \"after\" have a special connection to causality only when the independent variable is time. This connection is lost for variables other than time.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Nontemporal systems, such as those occurring in optics, can be noncausal and still realizable.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content=\"Moreover, even for temporal systems, such as those used for signal processing, the study of noncausal systems is important. In such systems we may have all input data prerecorded. This often happens with speech, geophysical, and meteorological signals, and with space probes. In such cases, the input's future values are available to us. For example, suppose we had a set of input signal records available for the system described by Eq. (26). We can then compute (y(t)) since, for any (t), we need\", metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content=\"only refer to the records to find the input's value (2) seconds before and (2) seconds after (t). Thus, noncausal systems can be realized, although not in real time. We may therefore be able to realize a noncausal system, provided we are willing to accept a time delay in the output. Consider a system whose output (\\\\hat{y}(t)) is the same as (y(t)) in Eq. (26) delayed by (2) seconds (Fig. 30c), so that\", metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[\\\\hat{y}(t)=y(t-2)=x(t-4)+x(t)]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Here the value of the output (\\\\hat{y}) at any instant (t) is the sum of the values of the input (x) at (t) and at the instant (4) seconds earlier [at ((t-4))]. In this case, the output at any instant (t) does not depend on future values of the input, and the system is causal. The output of this system, which is (\\\\hat{y}(t)), is identical to that in Eq. (26) or Fig. 30b except for a delay of (2) seconds. Thus, a noncausal system may be realized or satisfactorily approximated in real time by', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='using a causal system with a delay.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content=\"A third reason for studying noncausal systems is that they provide an upper bound on the performance of causal systems. For example, if we wish to design a filter for separating a signal from noise, then the optimum filter is invariably a noncausal system. Although unrealizable, thisnoncausal system's performance acts as the upper limit on what can be achieved and gives us a standard for evaluating the performance of causal filters.\", metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='At first glance, noncausal systems may seem to be inscrutable. Actually, there is nothing mysterious about these systems and their approximate realization through physical systems with delay. If we want to know what will happen one year from now, we have two choices: go to a prophet (an unrealizable person) who can give the answers instantly, or go to a wise man and allow him a delay of one year to give us the answer! If the wise man is truly wise, he may even be able, by studying trends, to', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='shrewdly guess the future very closely with a delay of less than a year. Such is the case with noncausal systems--nothing more and nothing less.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='DRIL 1.15: A Noncausal System\\n\\nShow that a system described by the following equation is noncausal:\\n\\n[y(t)=\\\\int_{t-5}^{t+5}x(\\\\tau)\\\\,d\\\\tau]\\n\\nShow that this system can be realized physically if we accept a delay of 5 seconds in the output.\\n\\n1.7-5 Continuous-Time and Discrete-Time Systems', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Signals defined or specified over a continuous range of time are continuous-time signals, denoted by symbols (x(t)), (y(t)), and so on. Systems whose inputs and outputs are continuous-time signals are continuous-time systems. On the other hand, signals defined only at discrete instants of time (t_{0},t_{1},t_{2},\\\\ldots,t_{n},\\\\ldots) are discrete-time signals, denoted by the symbols (x(t_{n})), (y(t_{n})), and so on, where (n) is some integer. Systems whose inputs and outputs are discrete-time', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='signals are discrete-time systems. A digital computer is a familiar example of this type of system. In practice, discrete-time signals can arise from sampling continuous-time signals. For example, when the sampling is uniform, the discrete instants (t_{0},t_{1},)(t_{2},)(\\\\ldots) are uniformly spaced so that', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[t_{k+1}-t_{k}=T\\\\qquad\\\\text{for all }k]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='In such case, the discrete-time signals represented by the samples of continuous-time signals (x(t),y(t),) and so on can be expressed as (x(nT),)(y(nT),) and so on; for convenience, we further simplify this notation to (x[n],)(y[n],)(\\\\ldots,) where it is understood that (x[n]=x(nT)) and that (n) is some integer. A typical discrete-time signal is shown in Fig. 1.31. A discrete-time signal may also be viewed as a sequence of numbers (\\\\ldots,)(x[-1],)(x[0],)(x[1],)(x[2],)(\\\\ldots.) Thus, a', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='discrete-time system may be seen as processing a sequence of numbers (x[n]) and yielding as an output another sequence of numbers (y[n].)', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Discrete-time signals arise naturally in situations that are inherently discrete time, such as population studies, amortization problems, national income models, and radar tracking. They may also arise as a result of sampling continuous-time signals in sampled data systems, digital filtering, and the like. Digital filtering is a particularly interesting application in which continuous-time signals are processed by using discrete-time systems, as shown in Fig. 1.32. A continuous-time signal', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='(x(t)) is first sampled to convert it into a discrete-time signal (x[n],) which then is processed by the discrete-time system to yield a discrete-time output (y[n].) A continuous-time signal (y(t)) is finally constructed from (y[n].) In this manner, we can process a continuous-time signal with an appropriate discrete-time system such as a digital computer. Because discrete-time systems have several significant advantages over continuous-time systems, there is an accelerating trend toward', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='processing continuous-time signals with discrete-time systems.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Figure 1.32: Processing continuous-time signals by discrete-time systems.\\n\\nFigure 1.31: A discrete-time signal.\\n\\n1.7-6 Analog and Digital Systems\\n\\nAnalog and digital signals are discussed in Sec. 1.3-2. A system whose input and output signals are analog is an analog system; a system whose input and output signals are digital is a digital system. A digital computer is an example of a digital (binary) system. Observe that a digital computer is a digital as well as a discrete-time system.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='1.7-7 Invertible and Noninvertible Systems', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='A system (\\\\mathcal{S}) performs certain operation(s) on input signal(s). If we can obtain the input (x(t)) back from the corresponding output (y(t)) by some operation, the system (\\\\mathcal{S}) is said to be invertible. When several different inputs result in the same output (as in a rectifier), it is impossible to obtain the input from the output, and the system is noninvertible. Therefore, for an invertible system, it is essential that every input have a unique output so that there is a', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='one-to-one mapping between an input and the corresponding output. The system that achieves the inverse operation [of obtaining (x(t)) from (y(t))] is the inverse system for (\\\\mathcal{S}). For instance, if (\\\\mathcal{S}) is an ideal integrator, then its inverse system is an ideal differentiator. Consider a system (\\\\mathcal{S}) connected in tandem with its inverse (\\\\mathcal{S}{i}), as shown in Fig. 1.33. The input (x(t)) to this tandem system results in signal (y(t)) at the output of', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='(\\\\mathcal{S}), and the signal (y(t)), which now acts as an input to (\\\\mathcal{S}{i}), yields back the signal (x(t)) at the output of (\\\\mathcal{S}{i}). Thus, (\\\\mathcal{S}{i}) undoes the operation of (\\\\mathcal{S}) on (x(t)), yielding back (x(t)). A system whose output is equal to the input (for all possible inputs) is an identity system. Cascading a system with its inverse system, as shown in Fig. 1.33, results in an identity system.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='In contrast, a rectifier, specified by an equation (y(t)=|x(t)|), is noninvertible because the rectification operation cannot be undone.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Inverse systems are very important in signal processing. In many applications, the signals are distorted during the processing, and it is necessary to undo the distortion. For instance, in transmission of data over a communication channel, the signals are distorted owing to non-ideal frequency response and finite bandwidth of a channel. It is necessary to restore the signal as closely as possible to its original shape. Such equalization is also used in audio systems and photographic systems.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='1.14 Assessing System Invertibility\\n\\nDetermine whether the following systems are invertible: (a)(y(t)=x(-t)), (b)(y(t)=tx(t)), and (c)(y(t)=\\\\frac{d}{dt}x(t)).\\n\\n(a) Here, the output is a reflection of the input, which does not cause any loss to the input. The input can, in fact, be exactly recovered by simply reflecting the output [(x(t)=y(-t))], which is to say that a reflecting system is its own inverse. Thus, (y(t)=x(-t)) is an invertible system.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Figure 1.33: A cascade of a system with its inverse results in an identity system.\\n\\n(b) In this case, one might be tempted to recover the input from the output as (x(t)=\\\\frac{1}{t}y(t)). This approach works almost everywhere, except at (t=0) where the input value (x(0)) cannot be recovered. Due to this single lost point, the system (y(t)=tx(t)) is not invertible.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='(c) Differentiation eliminates any dc component. For example, the inputs (x_{1}(t)=1) and (x_{2}(t)=2) both produce the same output (y(t)=0). Given only (y(t)=0), it is impossible to know if the original input was (x_{1}(t)=1), (x_{2}(t)=2), or something else entirely. Since unique inputs do produce unique outputs, we know that (y(t)=\\\\frac{d}{dt}x(t)) is not an invertible system.\\n\\n7-8 Stable and Unstable Systems', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Systems can also be classified as stable or unstable systems. Stability can be internal or external. If every bounded input applied at the input terminal results in a bounded output, the system is said to be stable externally. External stability can be ascertained by measurements at the external terminals (input and output) of the system. This type of stability is also known as the stability in the BIBO (bounded-input/bounded-output) sense. The concept of internal stability is postponed to Ch.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='2 because it requires some understanding of internal system behavior, introduced in that chapter.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='EXAMPLE 1.15 Assessing System BIBO Stability\\n\\nDetermine whether the following systems are BIBO-stable: (a)(y(t)=x^{2}(t)), (b)(y(t)=tx(t)), and (c)(y(t)=\\\\frac{d}{dt}x(t)).\\n\\n(a) This system squares an input to produce the output. If the input is bounded, which is to say that (|x(t)|\\\\leq M_{x}<\\\\infty) for all (t), then we see that\\n\\n[|y(t)|=|x^{2}(t)|=|x(t)|^{2}\\\\leq M_{x}^{2}<\\\\infty]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Since the output amplitude is guaranteed to be bounded for any bounded-amplitude input, the system (y(t)=x^{2}(t)) is BIBO-stable.\\n\\n(b) We can prove that (y(t)=tx(t)) is not BIBO-stable with a simple example. The bounded-amplitude input (x(t)=u(t)) produces the output (y(t)=tu(t)) whose amplitude grows to infinity as (t\\\\to\\\\infty). Thus, (y(t)=tx(t)) is a BIBO-unstable system.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='(c) We can prove that (y(t)=\\\\frac{d}{dt}x(t)) is not BIBO-stable with an example. The bounded-amplitude input (x(t)=u(t)) produces the output (y(t)=\\\\delta(t)) whose amplitude is infinite at (t=0). Thus, (y(t)=\\\\frac{d}{dt}x(t)) is a BIBO-unstable system.\\n\\nDDrill 1.16 A Noninvertible BIBO-Stable System\\n\\nShow that a system described by the equation (y(t)=x^{2}(t)) is noninvertible but BIBO-stable.\\n\\nChapter 1.8 System Model: Input-Output Description', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='A system description in terms of the measurements at the input and output terminals is called the input-output description. As mentioned earlier, systems theory encompasses a variety of systems, such as electrical, mechanical, hydraulic, acoustic, electromechanical, and chemical, as well as social, political, economic, and biological. The first step in analyzing any system is the construction of a system model, which is a mathematical expression or a rule that satisfactorily approximates the', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='dynamical behavior of the system. In this chapter we shall consider only continuous-time systems. Modeling of discrete-time systems is discussed in Ch. 3.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='1.8.1 Electrical Systems', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content=\"To construct a system model, we must study the relationships between different variables in the system. In electrical systems, for example, we must determine a satisfactory model for the voltage-current relationship of each element, such as Ohm's law for a resistor. In addition, we must determine the various constraints on voltages and currents when several electrical elements are interconnected. These are the laws of interconnection--the well-known Kirchhoff laws for voltage and current (KVL\", metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='and KCL). From all these equations, we eliminate unwanted variables to obtain equation(s) relating the desired output variable(s) to the input(s). The following examples demonstrate the procedure of deriving input-output relationships for some LTI electrical systems.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content=\"Application of Kirchhoff's voltage law around the loop yields\\n\\n[v_{L}(t)+v_{R}(t)+v_{C}(t)=x(t)]\\n\\nFigure 1.34: Circuit for Ex. 1.16.\\n\\nBy using the voltage-current laws of each element (inductor, resistor, and capacitor), we can express this equation as\\n\\n[\\\\frac{dy(t)}{dt}+3y(t)+2\\\\int_{-\\\\infty}^{t}y(\\\\tau)\\\\,d\\\\tau=x(t) \\\\tag{1.27}]\\n\\nDifferentiating both sides of this equation, we obtain\\n\\n[\\\\frac{d^{2}y(t)}{dt^{2}}+3\\\\frac{dy(t)}{dt}+2y(t)=\\\\frac{dx(t)}{dt} \\\\tag{1.28}]\", metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='This differential equation is the input-output relationship between the output (y(t)) and the input (x(t)).\\n\\nIt proves convenient to use a compact notation (D) for the differential operator (d/dt). This notation can be repeatedly applied. Thus,\\n\\n[\\\\frac{dy(t)}{dt}\\\\equiv Dy(t),\\\\qquad\\\\frac{d^{2}y(t)}{dt^{2}}\\\\equiv D^{2}y(t), \\\\qquad\\\\ldots,\\\\qquad\\\\frac{d^{N}y(t)}{dt^{N}}\\\\equiv D^{N}y(t)]\\n\\nWith this notation, Eq. (1.28) can be expressed as\\n\\n[(D^{2}+3D+2)y(t)=Dx(t) \\\\tag{1.29}]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='The differential operator is the inverse of the integral operator, so we can use the operator (1/D) to represent integration.2\\n\\nFootnote 2: Use of operator (1/D) for integration generates some subtle mathematical difficulties because the operators (D) and (1/D) do not commute. For instance, we know that (D(1/D)=1) because\\n\\n[\\\\frac{d}{dt}\\\\left[\\\\int_{-\\\\infty}^{t}y(\\\\tau)\\\\,d\\\\tau\\\\right]=y(t)]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content=\"However, ((1/D)D) is not necessarily unity. Use of Cramer's rule in solving simultaneous integro-differential equations will always result in cancellation of operators (1/D) and (D). This procedure may yield erroneous results when the factor (D) occurs in the numerator as well as in the denominator. This happens, for instance, in circuits with all-inductor loops or all-capacitor cut sets. To eliminate this problem, avoid the integral operation in system equations so that the resulting equations\", metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='are differential rather than integro-differential. In electrical circuits, this can be done by using charge (instead of current) variables in loops containing capacitors and choosing current variables for loops without capacitors. In the literature this problem of commutativity of (D) and (1/D) is largely ignored. As mentioned earlier, such a procedure gives erroneous results only in special systems, such as the circuits with all-inductor loops or all-capacitor cut sets. Fortunately such', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='systems constitute a very small fraction of the systems we deal with. For further discussion of this topic and a correct method of handling problems involving integrals, see [4].', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Consequently, Eq. (1.27) can be expressed as\\n\\n[\\\\left(D+3+\\\\frac{2}{D}\\\\right)y(t)=x(t)]\\n\\nMultiplying both sides by (D) to differentiate the expression, we obtain\\n\\n[(D^{2}+3D+2)y(t)=Dx(t)]\\n\\nwhich is identical to Eq. (1.29).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Recall that Eq. (1.29) is not an algebraic equation, and (D^{2}+3D+2) is not an algebraic term that multiplies (y(t)); it is an operator that operates on (y(t)). It means that we must perform the following operations on (y(t)): take the second derivative of (y(t)) and add to it 3 times the first derivative of (y(t)) and 2 times (y(t)). Clearly, a polynomial in (D) multiplied by (y(t)) represents a certain differential operation on (y(t)).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='EXAMPLE 1.17Input-Output Equation of a Series RC Circuit\\n\\nUsing operator notation, find the equation relating input to output for the series (RC) circuit of Fig. 1.35 if the input is the voltage (x(t)) and output is\\n\\n(a): the loop current (i(t))\\n(b): the capacitor voltage (y(t))\\n\\n(a) The loop equation for the circuit is\\n\\n[Ri(t)+\\\\frac{1}{C}\\\\int_{-\\\\infty}^{t}i(\\\\tau)\\\\,d\\\\tau=x(t)]\\n\\nor\\n\\n[15i(t)+5\\\\int_{-\\\\infty}^{t}i(\\\\tau)\\\\,d\\\\tau=x(t)]\\n\\nWith operator notation, this equation can be expressed as', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[15i(t)+\\\\frac{5}{D}i(t)=x(t) \\\\tag{1.30}]\\n\\nFigure 1.35: Circuit for Ex. 1.17\\n\\n(b) Multiplying both sides of Eq. (1.30) by (D) (i.e., differentiating the equation), we obtain\\n\\n[(15D+5)\\\\,i(t)=Dx(t)]\\n\\nUsing the fact that (i(t)=C\\\\,\\\\frac{dy(t)}{dt}=\\\\frac{1}{3}Dy(t)), simple substitution yields\\n\\n[(3D+1)y(t)=x(t) \\\\tag{1.31}]\\n\\nDrill 1.17Input-Output Equation of a Series (RLC) Circuit with\\n\\nInductor Voltage as Output', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='If the inductor voltage (v_{L}(t)) is taken as the output, show that the (RLC) circuit in Fig. 1.34 has an input-output equation of ((D^{2}+3D+2)v_{L}(t)=D^{2}x(t)).\\n\\nDrill 1.18Input-Output Equation of a Series (RC) Circuit with\\n\\nCapacitor Voltage as Output\\n\\nIf the capacitor voltage (v_{C}(t)) is taken as the output, show that the (RLC) circuit in Fig. 1.34 has an input-output equation of ((D^{2}+3D+2)v_{C}(t)=2x(t)).\\n\\n1.8-2 Mechanical Systems', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Planar motion can be resolved into translational (rectilinear) motion and rotational (torsional) motion. Translational motion will be considered first. We shall restrict ourselves to motions in one dimension.\\n\\nTranslational Systems\\n\\nThe basic elements used in modeling translational systems are ideal masses, linear springs, and dashpots providing viscous damping. The laws of various mechanical elements are now discussed.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content=\"For a (mass)(M) (Fig. 1.36a), a force (x(t)) causes a motion (y(t)) and acceleration (\\\\vec{y}(t)). From Newton's law of motion,\\n\\n[x(t)=M\\\\vec{y}(t)=M\\\\frac{d^{2}y(t)}{dt^{2}}=MD^{2}y(t)]\\n\\nThe force (x(t)) required to stretch (or compress) a linear spring (Fig. 1.36b) by an amount (y(t)) is given by\\n\\n[x(t)=Ky(t)]\\n\\nwhere (K) is the stiffness of the spring.\", metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='For a linear dashpot (Fig. 36c), which operates by virtue of viscous friction, the force moving the dashpot is proportional to the relative velocity (\\\\dot{y}(t)) of one surface with respect to the other. Thus\\n\\n[x(t)=B\\\\dot{y}(t)=B\\\\frac{dy(t)}{dt}=BDy(t)]\\n\\nwhere (B) is the damping coefficient of the dashpot or the viscous friction.\\n\\nExample 1.18: Input-Output Equation for a Translational Mechanical System', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Find the input-output relationship for the translational mechanical system shown in Fig. 37a or its equivalent in Fig. 37b. The input is the force (x(t)), and the output is the mass position (y(t)).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Figure 1.37: Mechanical system for Ex. 1.18In mechanical systems it is helpful to draw a free-body diagram of each junction, which is a point at which two or more elements are connected. In Fig. 1.37, the point representing the mass is a junction. The displacement of the mass is denoted by (y(t)). The spring is also stretched by the amount (y(t)), and therefore it exerts a force (-Ky(t)) on the mass. The dashpot exerts a force (-B\\\\dot{y}(t)) on the mass, as shown in the free-body diagram (Fig.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content=\"1.37c). By Newton's second law, the net force must be (M\\\\ddot{y}(t)). Therefore,\", metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[M\\\\ddot{y}(t)=-B\\\\dot{y}(t)-Ky(t)+x(t)]\\n\\nor\\n\\n[(MD^{2}+BD+K)y(t)=x(t)]\\n\\n1.3 Rotational Systems', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='In rotational systems, the motion of a body may be defined as its motion about a certain axis. The variables used to describe rotational motion are torque (in place of force), angular position (in place of linear position), angular velocity (in place of linear velocity), and angular acceleration (in place of linear acceleration). The system elements are rotational mass or moment of inertia (in place of mass) and torsional springs and torsional dashpots (in place of linear springs and dashpots).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='The terminal equations for these elements are analogous to the corresponding equations for translational elements. If (J) is the moment of inertia (or rotational mass) of a rotating body about a certain axis, then the external torque required for this motion is equal to (J) (rotational mass) times the angular acceleration. If (\\\\theta(t)) is the angular position of the body, (\\\\ddot{\\\\theta}(t)) is its angular acceleration, and', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[\\\\text{torque}=J\\\\ddot{\\\\theta}(t)=J\\\\frac{d^{2}\\\\theta(t)}{dt^{2}}=JD^{2}\\\\theta(t)]\\n\\nSimilarly, if (K) is the stiffness of a torsional spring (per unit angular twist), and (\\\\theta) is the angular displacement of one terminal of the spring with respect to the other, then\\n\\n[\\\\text{torque}=K\\\\theta(t)]\\n\\nFinally, the torque due to viscous damping of a torsional dashpot with damping coefficient (B) is\\n\\n[\\\\text{torque}=B\\\\dot{\\\\theta}(t)=BD\\\\theta(t)]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Example 1.19: Input-Output Equation for Aircraft Roll Angle', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='The attitude of an aircraft can be controlled by three sets of surfaces (shown shaded in Fig. 1.38): elevators, rudder, and ailerons. By manipulating these surfaces, one can set the aircraft on a desired flight path. The roll angle (\\\\varphi(t)) can be controlled by deflecting in the opposite direction the two aileron surfaces as shown in Fig. 1.38. Assuming only rolling motion, find the equation relating the roll angle (\\\\varphi(t)) to the input (deflection) (\\\\theta(t)).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='The aileron surfaces generate a torque about the roll axis proportional to the aileron deflection angle (\\\\theta(t)). Let this torque be (c\\\\theta(t)), where (c) is the constant of proportionality. Air friction dissipates the torque (B\\\\dot{\\\\varphi}(t)). The torque available for rolling motion is then (c\\\\theta(t)-B\\\\dot{\\\\varphi}(t)). If (J) is the moment of inertia of the plane about the (x) axis (roll axis), then\\n\\n[\\\\text{net torque}=J\\\\ddot{\\\\varphi}(t)=c\\\\theta(t)-B\\\\dot{\\\\varphi}(t)]\\n\\nand', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[J\\\\frac{d^{2}\\\\varphi(t)}{dt^{2}}+B\\\\frac{d\\\\varphi(t)}{dt}=c\\\\,\\\\theta(t)\\\\qquad \\\\text{or}\\\\qquad(JD^{2}+BD)\\\\varphi(t)=c\\\\theta(t)]\\n\\nThis is the desired equation relating the output (roll angle (\\\\varphi(t))) to the input (aileron angle (\\\\theta(t))).\\n\\nThe roll velocity (\\\\omega(t)) is (\\\\dot{\\\\varphi}(t)). If the desired output is the roll velocity (\\\\omega(t)) rather than the roll angle (\\\\varphi(t)), then the input-output equation would be', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content=\"[J\\\\frac{d\\\\omega(t)}{dt}+B\\\\omega(t)=c\\\\theta(t)\\\\qquad\\\\text{or}\\\\qquad(JD+B)\\\\omega (t)=c\\\\theta(t)]\\n\\n1.19 Input-Output Equation of a Rotational Mechanical System\\n\\nTorque (\\\\mathcal{T}(t)) is applied to the rotational mechanical system shown in Fig. 1.39a. The torsional spring stiffness is (K); the rotational mass (the cylinder's moment of inertia about the shaft) is (J); the viscous damping coefficient between the cylinder and the ground is (B). Find the equation\", metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Figure 1.38: Attitude control of an airplane.\\n\\n1.8.2 The (\\\\mathcal{T}(t))\\n\\nThe (\\\\mathcal{T}(t)) is a (\\\\mathcal{T}(t))-invariant function of the form (\\\\mathcal{T}(t)=\\\\mathcal{T}(t)). The (\\\\mathcal{T}(t)) is a (\\\\mathcal{T}(t))-invariant function of the form (\\\\mathcal{T}(t)=\\\\mathcal{T}(t)).\\n\\nChapter 1 Introduction\\n\\n1.9 Internal and External\\n\\nDescriptions of a System', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='The input-output relationship of a system is an external description of that system. We have found an external description (not the internal description) of systems in all the examples discussed so far. This may puzzle the reader because in each of these cases, we derived the input-output relationship by analyzing the internal structure of that system. Why is this not an internal description? What makes a description internal? Although it is true that we did find the input-output description by', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='internal analysis of the system, we did so strictly for convenience. We could have obtained the input-output description by making observations at the external (input and output) terminals, for example, by measuring the output for certain inputs, such as an impulse or a sinusoid. A description that can be obtained from measurements at the external terminals (even when the rest of the system is sealed inside an inaccessible black box) is an external description. Clearly, the input-output', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='description is an external description. What, then, is an internal description? An internal description is capable of providing complete information about all possible signals in the system. An external description may not give such complete information. An external description can always be found from an internal description, but the converse is not necessarily true. We shall now give an example to clarify the distinction between an external and an internal description.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Let the circuit in Fig. 1.41a with the input (x(t)) and the output (y(t)) be enclosed inside a \"black box\" with only the input and the output terminals accessible. To determine its external description, let us apply a known voltage (x(t)) at the input terminals and measure the resulting output voltage (y(t)).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Let us also assume that there is some initial charge (Q_{0}) present on the capacitor. The output voltage will generally depend on both, the input (x(t)) and the initial charge (Q_{0}). To compute the output resulting because of the charge (Q_{0}), assume the input (x(t)=0) (short across the input). In this case, the currents in the two 2 (\\\\Omega) resistors in the upper and the lower branches at the output terminals are equal and opposite because of the balanced nature of the circuit. Clearly,', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='the capacitor charge results in zero voltage at the output.1', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Footnote 1: The output voltage (y(t)) resulting because of the capacitor charge [assuming (x(t)=0)] is the zero-input response, which, as argued above, is zero. The output component due to the input (x(t)) (assuming zero initial capacitor charge) is the zero-state response. Complete analysis of this problem is given later in Ex. 1.21.\\n\\nFigure 1.40: Armature-controlled dc motor.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Now, to compute the output (y(t)) resulting from the input voltage (x(t)), we assume zero initial capacitor charge (short across the capacitor terminals). The current (i(t)) (Fig. 1.41a), in this case, divides equally between the two parallel branches because the circuit is balanced. Thus, the voltage across the capacitor continues to remain zero. Therefore, for the purpose of computing the current (i(t)), the capacitor may be removed or replaced by a short. The resulting circuit is equivalent', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='to that shown in Fig. 1.41b, which shows that the input (x(t)) sees a load of (5\\\\,\\\\Omega), and', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[i(t)=\\\\tfrac{1}{5}x(t)]\\n\\nAlso, because (y(t)=2\\\\,i(t)),\\n\\n[y(t)=\\\\tfrac{2}{5}x(t)]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='This is the total response. Clearly, for the external description, the capacitor does not exist. No external measurement or external observation can detect the presence of the capacitor. Furthermore, if the circuit is enclosed inside a \"black box\" so that only the external terminals are accessible, it is impossible to determine the currents (or voltages) inside the circuit from external measurements or observations. An internal description, however, can provide every possible signal inside the', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='system. In Ex. 1.21, we shall find the internal description of this system and show that it is capable of determining every possible signal in the system.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='For most systems, the external and internal descriptions are equivalent, but there are a few exceptions, as in the present case, where the external description gives an inadequate picture of the system. This happens when the system is uncontrollable and/or unobservable.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Figure 1.42 shows structural representations of simple uncontrollable and unobservable systems. In Fig. 1.42a, we note that part of the system (subsystem (\\\\mathcal{S}{2})) inside the box cannot be controlled by the input (x(t)). In Fig. 1.42b, some of the system outputs (those in subsystem (\\\\mathcal{S}{2})) cannot be observed from the output terminals. If we try to describe either of these systems by applying an external input (x(t)) and then measuring the output (y(t)), the measurement will', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='not characterize the complete system but only the part of the system (here (\\\\mathcal{S}_{1})) that is both controllable', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Figure 1.41: A system that cannot be described by external measurements.\\n\\nand observable (linked to both the input and output). Such systems are undesirable in practice and should be avoided in any system design. The system in Fig. 1.41a can be shown to be neither controllable nor observable. It can be represented structurally as a combination of the systems in Figs. 1.42a and 1.42b.\\n\\n1.10 Internal Description: The State-Space Description', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='We shall now introduce the state-space description of a linear system, which is an internal description of a system. In this approach, we identify certain key variables, called the state variables, of the system. These variables have the property that every possible signal in the system can be expressed as a linear combination of these state variables. For example, we can show that every possible signal in a passive RLC circuit can be expressed as a linear combination of independent capacitor', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='voltages and inductor currents, which, therefore, are state variables for the circuit.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='To illustrate this point, consider the network in Fig. 1.43. We identify two state variables: the capacitor voltage (q_{1}) and the inductor current (q_{2}). If the values of (q_{1}), (q_{2}), and the input (x(t)) are known at some instant (t), we can demonstrate that every possible signal (current or voltage) in the circuit can be determined at (t). For example, if (q_{1}=10), (q_{2}=1), and the input (x=20) at some instant, the remaining voltages and currents at that instant will be', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[i_{1}=(x-q_{1})/1=20-10=10\\\\,\\\\mathrm{A}] [v_{1}=x-q_{1}=20-10=10\\\\,\\\\mathrm{V}] [v_{2}=q_{1}=10\\\\,\\\\mathrm{V}] [i_{2}=q_{1}/2=5\\\\,\\\\mathrm{A}] [i_{C}=i_{1}-i_{2}-q_{2}=10-5-1=4\\\\,\\\\mathrm{A}] [i_{3}=q_{2}=1\\\\,\\\\mathrm{A}] [v_{3}=5q_{2}=5\\\\,\\\\mathrm{V}] [v_{L}=q_{1}-v_{3}=10-5=5\\\\,\\\\mathrm{V} \\\\tag{1.33}]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Thus all signals in this circuit are determined. Clearly, state variables consist of the key variables in a system; a knowledge of the state variables allows one to determine every possible output of the system. Note that the state-variable description is an internal description of a system because it is capable of describing all possible signals in the system.\\n\\nFigure 1.42: Structures of uncontrollable and unobservable systems.\\n\\nChapter 1 Signals and Systems\\n\\n1.1 Introduction', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='The signal-to-noise ratio (SNR) of a signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR),', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='(SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR),', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='(SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR),', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR), which is a function of the', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR),', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR), which is a function of the signal-to-noise ratio (SNR). The signal-to-noise ratio (SNR) is a function of the signal-to-noisefrom the output equations. In the input-output description, an (N)th-order system is described by an (N)th-order equation. In the state-variable approach, the same system is', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='described by (N) simultaneous first-order state equations.1', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Footnote 1: This assumes the system to be controllable and observable. If it is not, the input–output description equation will be of an order lower than the corresponding number of state equations.\\n\\nExample 1.21: Controllability and Observability\\n\\nInvestigate the nature of state equations and the issue of controllability and observability for the circuit in Fig. 1.41a.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='This circuit has only one capacitor and no inductors. Hence, there is only one state variable, the capacitor voltage (q(t)). Since (C=1) F, the capacitor current is (\\\\dot{q}). There are two sources in this circuit: the input (x(t)) and the capacitor voltage (q(t)). The response due to (x(t)), assuming (q(t)=0), is the zero-state response, which can be found from Fig. 1.44a, where we have shorted the capacitor [(q(t)=0)]. The response due to (q(t)) assuming (x(t)=0), is the zero-input response,', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='which can be found from Fig. 1.44b, where we have shorted (x(t)) to ensure (x(t)=0). It is now trivial to find both the components.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Figure 1.44a shows zero-state currents in every branch. It is clear that the input (x(t)) sees an effective resistance of 5 (\\\\Omega), and, hence, the current through (x(t)) is (x/5) A, which divides in the two parallel branches, resulting in the current (x/10) through each branch.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Examining the circuit in Fig. 1.44b for the zero-input response, we note that the capacitor voltage is (q) and the current is (\\\\dot{q}). We also observe that the capacitor sees two loops in parallel, each with resistance 4 (\\\\Omega) and current (\\\\dot{q}/2). Interestingly, the 3 (\\\\Omega) branch is effectively shorted because the circuit is balanced, and thus the voltage across the terminals (cd) is zero. The total current in any branch is the sum of the currents in that branch in Figs. 1.44a and', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='1.44b (principle of superposition).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[\\\\begin{array}{llll}\\\\mbox{Branch}&\\\\mbox{Current}&\\\\mbox{Voltage}\\\\ ca&\\\\frac{x}{10}+\\\\frac{\\\\dot{q}}{2}& 2\\\\biggl{(}\\\\frac{x}{10}+\\\\frac{\\\\dot{q}}{2} \\\\biggr{)}\\\\ cb&\\\\frac{x}{10}-\\\\frac{\\\\dot{q}}{2}& 2\\\\biggl{(}\\\\frac{x}{10}-\\\\frac{\\\\dot{q}}{2} \\\\biggr{)}\\\\ ad&\\\\frac{x}{10}-\\\\frac{\\\\dot{q}}{2}& 2\\\\biggl{(}\\\\frac{x}{10}-\\\\frac{\\\\dot{q}}{2} \\\\biggr{)}\\\\ bd&\\\\frac{x}{10}+\\\\frac{\\\\dot{q}}{2}& 2\\\\biggl{(}\\\\frac{x}{10}+\\\\frac{\\\\dot{q}}{2} \\\\biggr{)}\\\\ ec&\\\\frac{x}{5}& 3\\\\biggl{(}\\\\frac{x}{5}\\\\biggr{)}\\\\ ed&\\\\frac{x}{5}& x\\\\end{array}', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='\\\\tag{1.35}]To find the state equation, we note that the current in branch (ca) is ((x/10)+\\\\dot{q}/2) and the current in branch (cb) is ((x/10)-\\\\dot{q}/2). Hence, the equation around the loop (acba) is', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[q=2\\\\biggl{[}-\\\\frac{x}{10}-\\\\frac{\\\\dot{q}}{2}\\\\biggr{]}+2\\\\biggl{[}\\\\frac{x}{10}- \\\\frac{\\\\dot{q}}{2}\\\\biggr{]}=-2\\\\dot{q}]\\n\\nor\\n\\n[\\\\dot{q}=-0.5q \\\\tag{1.36}]\\n\\nThis is the desired state equation.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Substitution of (\\\\dot{q}=-0.5q) in Eq. (1.35) shows that every possible current and voltage in the circuit can be expressed in terms of the state variable (q) and the input (x), as desired. Hence, the set of Eq. (1.35) is the output equation for this circuit. Once we have solved the state equation [Eq. (1.36)] for (q), we can determine every possible output in the circuit.\\n\\nThe output (y(t)) is given by', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[y(t)=2\\\\biggl{[}\\\\frac{x}{10}-\\\\frac{\\\\dot{q}}{2}\\\\biggr{]}+2\\\\biggl{[}\\\\frac{x}{10} +\\\\frac{\\\\dot{q}}{2}\\\\biggr{]}=\\\\frac{2}{5}x(t) \\\\tag{1.37}]', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='A little examination of the state and the output equations indicates the nature of this system. Equation (1.36) shows that the state (q(t)) is independent of the input (x(t)); hence the system state (q) cannot be controlled by the input. Moreover, Eq. (1.37) shows that the output (y(t)) does not depend on the state (q(t)). Thus, the system state cannot be observed from the output terminals. Hence, the system is neither controllable nor observable. Such is not the case of other systems examined', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='earlier. Consider, for example, the circuit in Fig. 1.43. The state equations [Eq. (1.34)] show that the states are influenced by the input directly or indirectly. Hence, the system is controllable. Moreover, as Eq. (1.33) shows, every possible output is expressed in terms of the state variables and the input. Hence, the states are also observable.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Figure 1.44: Analysis of a system that is neither controllable nor observable.\\n\\nState-space techniques are useful not just because of their ability to provide internal system description, but for several other reasons, including the following.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='State equations of a system provide a mathematical model of great generality that can describe not just linear systems, but also nonlinear systems; not just time-invariant systems, but also time-varying parameter systems; not just SISO (single-input/single-output) systems, but also multiple-input/multiple-output (MIMO) systems. Indeed, state equations are ideally suited for the analysis, synthesis, and optimization of MIMO systems.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Compact matrix notation and the powerful techniques of linear algebra greatly facilitate complex manipulations. Without such features, many important results of the modern system theory would have been difficult to obtain. State equations can yield a great deal of information about a system even when they are not solved explicitly.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='State equations lend themselves readily to digital computer simulation of complex systems of high order, with or without nonlinearities, and with multiple inputs and outputs.\\n\\nFor second-order systems ((N=2)), a graphical method called phase-plane analysis can be used on state equations, whether they are linear or nonlinear.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='The real benefits of the state-space approach, however, are realized for highly complex systems of large order. Much of the book is devoted to introduction of the basic concepts of linear systems analysis, which must necessarily begin with simpler systems without using the state-space approach. Chapter 10 deals with the state-space analysis of linear, time-invariant, continuous-time, and discrete-time systems.\\n\\nDRILL 1.20 State Equations for a Series RLC Circuit', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Write the state equations for the series RLC circuit shown in Fig. 1.45, using the inductor current (q_{1}(t)) and the capacitor voltage (q_{2}(t)) as state variables. Express every voltage and current in this circuit as a linear combination of (q_{1}), (q_{2}), and (x).\\n\\nANSWERS\\n\\n(q_{1}=-3\\\\,q_{1}-q_{2}+x) and (q_{2}=2\\\\,q_{1}).\\n\\nFigure 1.45 Circuit for Drill 1.20.\\n\\n1.11 MATLAB: Working with Functions', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Working with functions is fundamental to signals and systems applications. MATLAB provides several methods of defining and evaluating functions. An understanding and proficient use of these methods are therefore necessary and beneficial.\\n\\n1.11-1 Anonymous Functions', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Many simple functions are most conveniently represented by using MATLAB anonymous functions. An anonymous function provides a symbolic representation of a function defined in terms of MATLAB operators, functions, or other anonymous functions. For example, consider defining the exponentially damped sinusoid(f(t)=e^{-t}\\\\cos(2\\\\pi t)).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content=\"f = @(t) exp(-t).cos(2pi*t); In this context, the @ symbol identifies the expression as an anonymous function, which is assigned a name of f. Parentheses following the @ symbol are used to identify the function's independent variables (input arguments), which in this case is the single time variable t. Input arguments, such as t, are local to the anonymous function and are not related to any workspace variables with the same names.\", metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Once defined, (f(t)) can be evaluated simply by passing the input values of interest. For example,\\n\\nt = 0; f(t) ans = 1 evaluates (f(t)) at (t=0), confirming the expected result of unity. The same result is obtained by passing (t=0) directly.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='f(0) ans = 1 Vector inputs allow the evaluation of multiple values simultaneously. Consider the task of plotting (f(t)) over the interval ((-2\\\\leq t\\\\leq 2)). Gross function behavior is clear: (f(t)) should oscillate four times with a decaying envelope. Since accurate hand sketches are cumbersome, MATLAB-generated plots are an attractive alternative. As the following example illustrates, care must be taken to ensure reliable results.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Suppose vector t is chosen to include only the integers contained in ((-2\\\\leq t\\\\leq 2)), namely, [(-2,-1,0,1,2)].\\n\\nt = (-2:2); This vector input is evaluated to form a vector output.\\n\\nf(t) ans = 7.3891 2.7183 1.0000 0.3679 0.1353The plot command graphs the result, which is shown in Fig. 1.46.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content=\"``` >>plot(t,f(t)); >>xlabel('t');ylabel('f(t)');grid; Gridlines,addedbyusingthegridcommand,aidfeatureidentification.Unfortunately,theplotdoesnotillustratetheexpectedoscillatorybehavior.Morepointsarerequiredtoadequatelyrepresent(f(t)).\", metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='The question, then, is how many points is enough?+ If too few points are chosen, information is lost. If too many points are chosen, memory and time are wasted. A balance is needed. For oscillatory functions, plotting 20 to 200 points per oscillation is normally adequate. For the present case, t is chosen to give 100 points per oscillation.\\n\\nFootnote †: margin: 100\\n\\nt=(-2:0.01:2); Again, thefunctionisevaluatedandplotted.\\n\\nFigure 1.46: (f(t)=e^{-t}\\\\cos{(2\\\\pi t)})fort=(-2:2).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content=\"Figure 1.47: (f(t)=e^{-t}\\\\cos{(2\\\\pi t)})fort=(-2:0.01:2).\\n\\nplot(t,f(t)); >>xlabel('t');ylabel('f(t)');grid; The result, shown in Fig. 1.47, is an accurate depiction of (f(t)).\\n\\n1.11-2 Relational Operators and the Unit Step Function\\n\\nThe unit step function (u(t)) arises naturally in many practical situations. For example, a unit step can model the act of turning on a system. With the help of relational operators, anonymous functions can represent the unit step function.\", metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='In MATLAB, a relational operator compares two items. If the comparison is true, a logical true (1) is returned. If the comparison is false, a logical false (0) is returned. Sometimes called indicator functions, relational operators indicates whether a condition is true. Six relational operators are available: <, >, <=, >=, ==, and ~=.\\n\\nThe unit step function is readily defined using the >= relational operator.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content=\"u = @(t) 1.0.*(t>=0); Any function with a jump discontinuity, such as the unit step, is difficult to plot. Consider plotting (u(t)) by using t = (-2:2).\\n\\nt = (-2:2);plot(t,u(t)); >>xlabel('t');ylabel('u(t)');\", metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Two significant problems are apparent in the resulting plot, shown in Fig. 1.48. First, MATLAB automatically scales plot axes to tightly bound the data. In this case, this normally desirable feature obscures most of the plot. Second, MATLAB connects plot data with lines, making a true jump discontinuity difficult to achieve. The coarse resolution of vector t emphasizes the effect by showing an erroneous sloping line between (t=-1) and (t=0).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content=\"The first problem is corrected by vertically enlarging the bounding box with the axis command. The second problem is reduced, but not eliminated, by adding points to vector t.\\n\\nFigure 1.48: (u(t)) for t = (-2:2).\\n\\nt = (-2:0.01:2); plot(t,u(t)); >> xlabel('t'); ylabel('u(t)'); >> axis([-2 2 -0.1 1.1]); The four-element vector argument of axis specifies (x) axis minimum, (x) axis maximum, (y) axis minimum, and (y) axis maximum, respectively. The improved results are shown in Fig. 1.49.\", metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Relational operators can be combined using logical AND, logical OR, and logical negation: &, 1, and ~, respectively. For example, (t>0)&(t<1) and ~((t<=0)|(t>=1)) both test if (0<t<1). To demonstrate, consider defining and plotting the unit pulse (p(t)=u(t)-u(t-1)), as shown in Fig. 1.50:', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content=\"p = @(t) 1.0.*((t>=0)&(t<1)); >> t = (-1:0.01:2); plot(t,p(t)); >> xlabel('t'); ylabel('p(t) = u(t)-u(t-1)'); >> axis([-1 2 -.1 1.1]); Since anonymous functions can be constructed using other anonymous functions, we could have used our previously defined unit step anonymous function to define (p(t)) as p = @(t) u(t)-u(t-1);.\\n\\nFigure 1.49: (u(t)) for t = (-2:0.01:2) with axis modification.\\n\\nFigure 1.50: (p(t)=u(t)-u(t-1)) over ((-1\\\\leq t\\\\leq 2)).\", metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='For scalar operands, MATLAB also supports two short-circuit logical constructs. A short-circuit logical AND is performed by using &&, and a short-circuit logical OR is performed by using (|!|!|). Short-circuit logical operators are often more efficient than traditional logical operators because they test the second portion of the expression only when necessary. That is, when scalar expression A is found false in (A&&B), scalar expression B is not evaluated, since a false result is already', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='guaranteed. Similarly, scalar expression B is not evaluated when scalar expression A is found true in (A|B), since a true result is already guaranteed.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content=\"1.11-3 Visualizing Operations on the Independent Variable\\n\\nTwo operations on a function's independent variable are commonly encountered: shifting and scaling. Anonymous functions are well suited to investigate both operations.\\n\\nConsider (g(t)=f(t)u(t)=e^{-t}\\\\cos{(2\\\\pi t)}u(t)), a causal version of (f(t)). MATLAB easily multiplies anonymous functions. Thus, we create (g(t)) by multiplying our anonymous functions for (f(t)) and (u(t)).1\", metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Footnote 1: Although we define g in terms of f and u, the function g will not change if we later change either f or u, unless we subsequently redefine g as well.\\n\\ng = @(t) f(t).*u(t);', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='A combined shifting and scaling operation is represented by (g(at+b)), where (a) and (b) are arbitrary real constants. As an example, consider plotting (g(2t+1)) over ((-2\\\\leq t\\\\leq 2)). With (a=2), the function is compressed by a factor of 2, resulting in twice the oscillations per unit (t). Adding the condition (b>0) shifts the waveform to the left. Given anonymous function g, an accurate plot is nearly trivial to obtain.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content=\"t = (-2:0.01:2); >> plot(t,g(2*t+1)); xlabel('t'); ylabel('g(2t+1)'); grid;\\n\\nFigure 1.51 confirms the expected waveform compression and left shift. As a final check, realize that function (g(\\\\cdot)) turns on when the input argument is zero. Therefore, (g(2t+1)) should turn on when (2t+1=0) or at (t=-0.5), a fact again confirmed by Fig. 1.51.\\n\\nFigure 1.51: (g(2t+1)) over ((-2\\\\leq t\\\\leq 2)).\", metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content=\"Next, consider plotting (g(-t+1)) over ((-2\\\\leq t\\\\leq 2)). Since (a<0), the waveform will be reflected. Adding the condition (b>0) shifts the final waveform to the right.\\n\\nplot(t,g(-t+1)); xlabel('t'); ylabel('g(-t+1)'); grid; Figure 1.52 confirms both the reflection and the right shift.\", metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content=\"Up to this point, Figs. 1.51 and 1.52 could be reasonably sketched by hand. Consider plotting the more complicated function (h(t)=g(2t+1)+g(-t+1)) over ((-2\\\\leq t\\\\leq 2)) (Fig. 1.53); an accurate hand sketch would be quite difficult. With MATLAB, the work is much less burdensome.\\n\\nplot(t,g(2*t+1)+g(-t+1)); xlabel('t'); ylabel('h(t)'); grid;\\n\\n1.11-4 Numerical Integration and Estimating Signal Energy\", metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Interesting signals often have nontrivial mathematical representations. Computing signal energy, which involves integrating the square of these expressions, can be a daunting task. Fortunately, many difficult integrals can be accurately estimated by means of numerical integration techniques.\\n\\nFigure 1.52: (g(-t+1)) over ((-2\\\\leq t\\\\leq 2)).\\n\\nFigure 1.53: (h(t)=g(2t+1)+g(-t+1)) over ((-2\\\\leq t\\\\leq 2)).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Even if the integration appears simple, numerical integration provides a good way to verify analytical results.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='To start, consider the simple signal (x(t)=e^{-t}(u(t)-u(t-1))). The energy of (x(t)) is expressed as (E_{x}=\\\\int_{-\\\\infty}^{\\\\infty}|x(t)|^{2}\\\\,dt=\\\\int_{0}^{1}e^{-2t}\\\\,dt). Integrating yields (E_{x}=0.5(1-e^{-2})\\\\approx 0.4323). The energy integral can also be evaluated numerically. Figure 1.27 helps illustrate the simple method of rectangular approximation: evaluate the integrand at points uniformly separated by (\\\\Delta t), multiply each by (\\\\Delta t) to compute rectangle areas, and then sum', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='over all rectangles. First, we create function (x(t)).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='x = @(t) exp(-t).*((t>=0)&(t<1)); With (\\\\Delta t=0.01), a suitable time vector is created.\\n\\nt = (0:0.01:1); The final result is computed by using the sum command.\\n\\nE_x = sum(x(t).x(t)0.01) E_x = 0.4367 The result is not perfect, but at 1% relative error it is close. By reducing (\\\\Delta t), the approximation is improved. For example, (\\\\Delta t=0.001) yields E_x = 0.4328, or 0.1% relative error.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Although simple to visualize, rectangular approximation is not the best numerical integration technique. The MATLAB function quad implements a better numerical integration technique called recursive adaptive Simpson quadrature.2 To operate, quad requires a function describing the integrand, the lower limit of integration, and the upper limit of integration. Notice that no (\\\\Delta t) needs to be specified.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Footnote 2: A comprehensive treatment of numerical integration is outside the scope of this text. Details of this particular method are not important for the current discussion; it is sufficient to say that it is better than the rectangular approximation.\\n\\nTo use quad to estimate (E_{x}), the integrand must first be described.\\n\\nx_squared = @(t) x(t).*x(t); Estimating (E_{x}) immediately follows.\\n\\nE_x = quad(x_squared,0,1) E_x = 0.4323 In this case, the relative error is (-0.0026\\\\%).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='The same techniques can be used to estimate the energy of more complex signals. Consider (g(t)), defined previously. Energy is expressed as (E_{g}=\\\\int_{0}^{\\\\infty}e^{-2t}\\\\cos^{2}\\\\left(2\\\\pi\\\\,t\\\\right)dt). A closed-form solution exists, but it takes some effort. MATLAB provides an answer more quickly.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='g_squared = @(t) g(t).*g(t);Although the upper limit of integration is infinity, the exponentially decaying envelope ensures (g(t)) is effectively zero well before (t=100). Thus, an upper limit of (t=100) is used along with (\\\\Delta t=0.001).\\n\\nt = (0:0.001:100); >> E_g = sum(g_squared(t)*0.001)  E_g = 0.2567 A slightly better approximation is obtained with the quad function.\\n\\nE_g = quad(g_squared,0,100)  E_g = 0.2562\\n\\nDRIL 1.21 Computing Signal Energy with MATLAB', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Use MATLAB to confirm that the energy of signal (h(t)), defined previously as (h(t)=g(2t+1)+g(-t+1)), is (E_{h}=0.3768).\\n\\n1.12 Summary\\n\\nA signal is a set of data or information. A system processes input signals to modify them or extract additional information from them to produce output signals (response). A system may be made up of physical components (hardware realization), or it may be an algorithm that computes an output signal from an input signal (software realization).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='A convenient measure of the size of a signal is its energy, if it is finite. If the signal energy is infinite, the appropriate measure is its power, if it exists. The signal power is the time average of its energy (averaged over the entire time interval from (-\\\\infty) to (\\\\infty)). For periodic signals, the time averaging need be performed over only one period in view of the periodic repetition of the signal. Signal power is also equal to the mean squared value of the signal (averaged over the', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='entire time interval from (t=-\\\\infty) to (\\\\infty)).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Signals can be classified in several ways.\\n\\nA continuous-time signal is specified for a continuum of values of the independent variable (such as time (t)). A discrete-time signal is specified only at a finite or a countable set of time instants.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='An analog signal is a signal whose amplitude can take on any value over a continuum. On the other hand, a signal whose amplitudes can take on only a finite number of values is a digital signal. The terms discrete-time and continuous-time qualify the nature of a signal along the time axis (horizontal axis). The terms analog and digital, on the other hand, qualify the nature of the signal amplitude (vertical axis).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='A periodic signal(x(t)) is defined by the fact that (x(t)=x(t+T_{0})) for some (T_{0}). The smallest positive value of (T_{0}) for which this relationship is satisfied is called the fundamental period. A periodic signal remains unchanged when shifted by an integer multiple of its period. A periodic signal (x(t)) can be generated by a periodic extension of any contiguous segment of (x(t)) of duration (T_{0}). Finally, a periodic signal, by definition, must exist over the entire time interval', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='(-\\\\infty<t<\\\\infty). A signal is aperiodic if it is not periodic.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='An everlasting signal starts at (t=-\\\\infty) and continues forever to (t=\\\\infty). Hence, periodic signals are everlasting signals. A causal signal is a signal that is zero for (t<0).\\n\\nA signal with finite energy is an energy signal. Similarly a signal with a finite and nonzero power (mean-square value) is a power signal. A signal can be either an energy signal or a power signal, but not both. However, there are signals that are neither energy nor power signals.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='A signal whose physical description is known completely in a mathematical or graphical form is a deterministic signal. A random signal is known only in terms of its probabilistic description such as mean value or mean-square value, rather than by its mathematical or graphical form.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='A signal (x(t)) delayed by (T) seconds (right-shifted) can be expressed as (x(t-T)); on the other hand, (x(t)) advanced by (T) (left-shifted) is (x(t+T)). A signal (x(t)) time-compressed by a factor (a\\\\left(a>1\\\\right)) is expressed as (x(at)); on the other hand, the same signal time-expanded by factor (a\\\\left(a>1\\\\right)) is (x(t/a)). The signal (x(t)) when time-reversed can be expressed as (x(-t)).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='The unit step function (u(t)) is very useful in representing causal signals and signals with different mathematical descriptions over different intervals.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='In the classical (Dirac) definition, the unit impulse function (\\\\delta(t)) is characterized by unit area and is concentrated at a single instant (t=0). The impulse function has a sampling (or sifting) property, which states that the area under the product of a function with a unit impulse is equal to the value of that function at the instant at which the impulse is located (assuming the function to be continuous at the impulse location). In the modern approach, the impulse function is viewed as', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='a generalized function and is defined by the sampling property.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='The exponential function (e^{st}), where (s) is complex, encompasses a large class of signals that includes a constant, a monotonic exponential, a sinusoid, and an exponentially varying sinusoid.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='A real signal that is symmetrical about the vertical axis ((t=0)) is an even function of time, and a real signal that is antisymmetrical about the vertical axis is an odd function of time. The product of an even function and an odd function is an odd function. However, the product of an even function and an even function or an odd function and an odd function is an even function. The area under an odd function from (t=-a) to (a) is always zero regardless of the value of (a). On the other hand,', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='the area under an even function from (t=-a) to (a) is two times the area under the same function from (t=0) to (a) (or from (t=-a) to (0)). Every signal can be expressed as a sum of odd and even functions of time.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='A system processes input signals to produce output signals (response). The input is the cause, and the output is its effect. In general, the output is affected by two causes: the internal conditions of the system (such as the initial conditions) and the external input.\\n\\nSystems can be classified in several ways.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Linear systems are characterized by the linearity property, which implies superposition; if several causes (such as various inputs and initial conditions) are acting on a linear system, the total output (response) is the sum of the responses from each cause, assuming that all the remaining causes are absent. A system is nonlinear if superposition does not hold.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='In time-invariant systems, system parameters do not change with time. The parameters of time-varying-parameter systems change with time.\\n\\nFor memoryless (or instantaneous) systems, the system response at any instant (t) depends only on the value of the input at (t). For systems with memory (also known as dynamic systems), the system response at any instant (t) depends not only on the present value of the input, but also on the past values of the input (values before (t)).', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='In contrast, if a system response at (t) also depends on the future values of the input (values of input beyond (t)), the system is noncausal. In causal systems, the response does not depend on the future values of the input. Because of the dependence of the response on the future values of input, the effect (response) of noncausal systems occurs before the cause. When the independent variable is time (temporal systems), the noncausal systems are prophetic systems, and therefore, unrealizable,', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='although close approximation is possible with some time delay in the response. Noncausal systems with independent variables other than time (e.g., space) are realizable.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Systems whose inputs and outputs are continuous-time signals are continuous-time systems; systems whose inputs and outputs are discrete-time signals are discrete-time systems. If a continuous-time signal is sampled, the resulting signal is a discrete-time signal. We can process a continuous-time signal by processing the samples of the signal with a discrete-time system.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Systems whose inputs and outputs are analog signals are analog systems; those whose inputs and outputs are digital signals are digital systems.\\n\\nIf we can obtain the input (x(t)) back from the output (y(t)) of a system (\\\\mathcal{S}) by some operation, the system (\\\\mathcal{S}) is said to be invertible. Otherwise the system is noninvertible.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='A system is stable if bounded input produces bounded output. This defines external stability because it can be ascertained from measurements at the external terminals of the system. External stability is also known as the stability in the BIBO (bounded-input/bounded-output) sense. Internal stability, discussed later in Ch. 2, is measured in terms of the internal behavior of the system.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='The system model derived from a knowledge of the internal structure of the system is its internal description. In contrast, an external description is a representation of a system as seen from its input and output terminals; it can be obtained by applying a known input and measuring the resulting output. In the majority of practical systems, an external description of a system so obtained is equivalent to its internal description. At times, however, the external description fails to describe', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='the system adequately. Such is the case with the so-called uncontrollable or unobservable systems.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='A system may also be described in terms of certain set of key variables called state variables. In this description, an (N)th-order system can be characterized by a set of (N) simultaneous first-order differential equations in (N) state variables. State equations of a system represent an internal description of that system.\\n\\nReferences\\n\\n[1] Papoulis, A., The Fourier Integral and Its Applications. McGraw-Hill, New York, 1962.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='[2] Mason, S. J., Electronic Circuits, Signals, and Systems. Wiley, New York, 1960.\\n\\n[3] Kailath, T., Linear Systems. Prentice-Hall, Englewood Cliffs, NJ, 1980.\\n\\n[4] Lathi, B. P., Signals and Systems. Berkeley-Cambridge Press, Carmichael, CA, 1987.', metadata={'source': '..\\\\mmd\\\\Chapter_01.mmd'}),\n",
       " Document(page_content='Chapter Time-Domain Analysis\\n\\nof Discrete-Time Systems\\n\\nIn this chapter we introduce the basic concepts of discrete-time signals and systems. Furthermore, we explore the time-domain analysis of linear, time-invariant, discrete-time (LTID) systems. We show how to compute the zero-input response, determine the unit impulse response, and use convolution to evaluate the zero-state response.\\n\\n17 Introduction', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='A discrete-time signal is basically a sequence of numbers. Such signals arise naturally in inherently discrete-time situations such as population studies, amortization problems, national income models, and radar tracking. They may also arise as a result of sampling continuous-time signals in sampled data systems and digital filtering. Such signals can be denoted by (x[n]), (y[n]), and so on, where the variable (n) takes integer values, and (x[n]) denotes the (n)th number in the sequence labeled', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='(x). In this notation, the discrete-time variable (n) is enclosed in square brackets instead of parentheses, which we have reserved for enclosing continuous-time variables, such as (t).', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Systems whose inputs and outputs are discrete-time signals are called discrete-time systems. A digital computer is a familiar example of this type of system. A discrete-time signal is a sequence of numbers, and a discrete-time system processes a sequence of numbers (x[n]) to yield another sequence (y[n]) as the output.1\\n\\nFootnote 1: There may be more than one input and more than one output.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='A discrete-time signal, when obtained by uniform sampling of a continuous-time signal (x(t)), can also be expressed as (x(nT)), where (T) is the sampling interval and (n), the discrete variable taking on integer values. Thus, (x(nT)) denotes the value of the signal (x(t)) at (t=nT). The signal (x(nT)) is a sequence of numbers (sample values), and hence, by definition, is a discrete-time signal. Such a signal can also be denoted by the customary discrete-time notation (x[n]), where (x[n]=x(nT)).', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='A typical discrete-time signal is depicted in Fig. 17, which shows both forms of notation. By way of an example, a continuous-time exponential (x(t)=e^{-t}), when sampled every (T=0.1) seconds, results in a discrete-time signal (x(nT)) given by', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[x(nT)=e^{-nT}=e^{-0.1n}]Clearly, this signal is a function of (n) and may be expressed as (x[n]). Such representation is more convenient and will be followed throughout this book, even for signals resulting from sampling continuous-time signals.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Digital filters can process continuous-time signals by discrete-time systems, using appropriate interfaces at the input and the output, as illustrated in Fig. 3.2. A continuous-time signal (x(t)) is first sampled to convert it into a discrete-time signal (x[n]), which is then processed by a discrete-time system to yield the output (y[n]). A continuous-time signal (y(t)) is finally constructed from (y[n]). We shall use the notations C/D and D/C for conversion from continuous to discrete time and', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='from discrete to continuous time. By using the interfaces in this manner, we can use an appropriate discrete-time system to process a continuous-time signal. As we shall see later in our discussion, discrete-time systems have several advantages over continuous-time systems. For this reason, there is an accelerating trend toward processing continuous-time signals with discrete-time systems.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Size of a Discrete-Time Signal\\n\\nArguing along the lines similar to those used for continuous-time signals, the size of a discrete-time signal (x[n]) will be measured by its energy (E_{x}), defined by\\n\\n[E_{x}=\\\\sum_{n=-\\\\infty}^{\\\\infty}|x[n]|^{2} \\\\tag{3.1}]\\n\\nFigure 3.2: Processing a continuous-time signal by means of a discrete-time system.\\n\\nFigure 3.1: A discrete-time signal.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='This definition is valid for real or complex (x[n]). For this measure to be meaningful, the energy of a signal must be finite. A necessary condition for the energy to be finite is that the signal amplitude must (\\\\to 0) as (|n|\\\\to\\\\infty). Otherwise the sum in Eq. (3.1) will not converge. If (E_{x}) is finite, the signal is called an energy signal.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='In some cases, for instance, when the amplitude of (x[n]) does not (\\\\to 0) as (|n|\\\\to\\\\infty), then the signal energy is infinite, and a more meaningful measure of the signal in such a case would be the time average of the energy (if it exists), which is the signal power (P_{x}), defined by\\n\\n[P_{x}=\\\\lim_{N\\\\to\\\\infty}\\\\frac{1}{2N+1}\\\\sum_{-N}^{N}|x[n]|^{2}]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='In this equation, the sum is divided by (2N+1) because there are (2N+1) samples in the interval from (-N) to (N). For periodic signals, the time averaging need be performed over only one period in view of the periodic repetition of the signal. If (P_{x}) is finite and nonzero, the signal is called a power signal. As in the continuous-time case, a discrete-time signal can either be an energy signal or a power signal, but cannot be both at the same time. Some signals are neither energy nor power', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='signals.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Example 3.1: Computing DT Energy and Power\\n\\nFind the energy of the signal (x[n]=n(u[n]-u[n-6])), shown in Fig. 3.3a and the power for the periodic signal (y[n]) in Fig. 3.3b.\\n\\nBy definition,\\n\\n[E_{x}=\\\\sum_{n=0}^{5}n^{2}=55]\\n\\nA periodic signal (x[n]) with period (N_{0}) is characterized by the fact that\\n\\n[x[n]=x[n+N_{0}]]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='The smallest value of (N_{0}) for which the preceding equation holds is the fundamental period. Such a signal is called (N_{0})periodic. Figure 3.3b shows an example of a periodic signal (y[n]) of period (N_{0}=6) because each period contains 6 samples. Note that if the first sample is taken at (n=0), the last sample is at (n=N_{0}-1=5), not at (n=N_{0}=6). Because the signal (y[n]) is periodic, its power (P_{y}) can be found by averaging its energy over one period. Averaging the energy over', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='one period, we obtain', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[P_{y}=\\\\frac{1}{6}\\\\sum_{n=0}^{5}n^{2}=\\\\frac{55}{6}]\\n\\n3.2 Useful Signal Operations\\n\\nSignal operations for shifting, and scaling, as discussed for continuous-time signals also apply, with some modifications, to discrete-time signals.\\n\\nShifting\\n\\nConsider a signal (x[n]) (Fig. 3.4a) and the same signal delayed (right-shifted) by 5 units (Fig. 3.4b), which we shall denote by (x_{s}[n]).1 Using the argument employed for a similar operation in continuous-time signals (Sec. 1.2), we obtain', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Footnote 1: The terms “delay” and “advance” are meaningful only when the independent variable is time. For other independent variables, such as frequency or distance, it is more appropriate to refer to the “right shift” and “left shift” of a sequence.\\n\\n[x_{s}[n]=x[n-5]]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Therefore, to shift a sequence by (M) units ((M) integer), we replace (n) with (n-M). Thus (x[n-M]) represents (x[n]) shifted by (M) units. If (M) is positive, the shift is to the right (delay). If (M) is negative, the shift is to the left (advance). Accordingly, (x[n-5]) is (x[n]) delayed (right-shifted) by 5 units, and (x[n+5]) is (x[n]) advanced (left-shifted) by 5 units.\\n\\nFigure 3.3: (a) Energy and (b) power computations for a signal.\\n\\n3.2 Left-Shift Operation', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Show that (x[n]) in Fig. 3.4a left-shifted by 3 units can be expressed as (0.729(0.9)^{n}) for (0\\\\leq n\\\\leq 7), and zero otherwise. Sketch the shifted signal.\\n\\nFigure 3.4: Shifting and time reversal of a signal.\\n\\nChapter 3 Time-Domain Analysis of Discrete-Time Systems\\n\\n3.1 Time Reversal\\n\\nSketch the signal (x[n]=e^{-0.5n}) for (-3\\\\leq n\\\\leq 2), and zero otherwise. Sketch the corresponding time-reversed signal and show that it can be expressed as (x_{r}[n]=e^{0.5n}) for (-2\\\\leq n\\\\leq 3).', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Sampling Rate Alteration: Downsampling, Upsampling, and Interpolation\\n\\nAlteration of the sampling rate is somewhat similar to time-scaling in continuous-time signals. Consider a signal (x[n]) compressed by factor (M). Compressing a signal (x[n]) by factor (M) yields (x_{d}[n]) given by\\n\\n[x_{d}[n]=x[Mn]]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Because of the restriction that discrete-time signals are defined only for integer values of the argument, we must restrict (M) to integer values. The values of (x[Mn]) at (n=0,1,2,3,\\\\ldots) are (x[0]), (x[M]), (x[2M]), (x[3M]), (\\\\ldots). This means (x[Mn]) selects every (M)th sample of (x[n]) and deletes all the samples in between. It reduces the number of samples by factor (M). If (x[n]) is obtained by sampling a continuous-time signal, this operation implies reducing the sampling rate by', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='factor (M). For this reason, this operation is commonly called downsampling. Figure 3.5a shows a signal (x[n]) and Fig. 3.5b shows the signal (x[2n]), which is obtained by deleting odd-numbered samples of (x[n]).1', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Footnote 1: Odd-numbered samples of (x[n]) can be retained (and even-numbered samples deleted) by using the transformation (x_{d}[n]=x[2n+1]).\\n\\nIn the continuous-time case, time compression merely speeds up the signal without loss of any data. In contrast, downsampling (x[n]) generally causes loss of data. Under certain conditions--for example, if (x[n]) is the result of oversampling some continuous-time signal--then (x_{d}[n]) may still retain the complete information about (x[n]).', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='An interpolated signal is generated in two steps; first, we expand (x[n]) by an integer factor (L) to obtain the expanded signal (x_{e}[n]), as\\n\\n[x_{e}[n]=\\\\begin{cases}x[n/L]&\\\\quad n=0,\\\\,\\\\pm L\\\\,\\\\pm 2L,\\\\ldots,\\\\ 0&\\\\quad\\\\text{otherwise}\\\\end{cases} \\\\tag{3.2}]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='To understand this expression, consider a simple case of expanding (x[n]) by a factor 2 ((L=2)). When (n) is odd, (n/2) is noninteger, and (x_{e}[n]=0). That is, (x_{e}[1]=x_{e}[3]=x_{e}[5],\\\\ldots) are all zero, as depicted in Fig. 3.5c. Moreover, (n/2) is integer for even (n), and the values of (x_{e}[n]=x[n/2]) for (n=0,2,4,6,\\\\ldots), are (x[0]), (x[1]), (x[2]), (x[3]), (\\\\ldots), as shown in Fig. 3.5c. In general, for (n=0,1,2,\\\\ldots), (x_{e}[n]) is given by the sequence', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[x[0],\\\\underbrace{0,0,\\\\ldots,0,0}{L-1\\\\text{ zeros}},x[1],\\\\underbrace{0,0, \\\\ldots,0,0}{L-1\\\\text{ zeros}},x[2],\\\\underbrace{0,0,\\\\ldots,0,0}_{L-1\\\\text{ zeros}},\\\\ldots]\\n\\nThus, the sampling rate of (x_{e}[n]) is (L) times that of (x[n]). Hence, this operation is commonly called upsampling. The upsampled signal (x_{e}[n]) contains all the data of (x[n]), although in an expanded form.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='In the expanded signal in Fig. 3.5c, the missing (zero-valued) odd-numbered samples can be reconstructed from the non-zero-valued samples by using some suitable interpolation formula. Figure 3.5d shows such an interpolated signal (x_{i}[n]), where the missing samples are constructed by using an interpolating filter. The optimum interpolating filter is usually an ideal lowpassfilter, which is realizable only approximately. In practice, we may use an interpolation that is nonoptimum but', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='realizable. The process of filtering to interpolate the zero-valued samples is called interpolation. Since the interpolated data are computed from the existing data, interpolation does not result in gain of information. While further discussion of interpolation is beyond our scope, Drill 3.5 and Prob. 3.11-10 introduce the idea of linear interpolation.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Figure 3.5: Compression (downsampling) and expansion (upsampling, interpolation) of a signal.\\n\\n3.3 Some Useful Discrete-Time Signal Models\\n\\nWe now discuss some important discrete-time signal models that are encountered frequently in the study of discrete-time signals and systems.\\n\\n3.3.1 Discrete-Time Impulse Function (\\\\delta[n])\\n\\nThe discrete-time counterpart of the continuous-time impulse function (\\\\delta(t)) is (\\\\delta[n]), a Kronecker delta function, defined by', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[\\\\delta[n]=\\\\begin{cases}1&n=0\\\\ 0&n\\\\neq 0\\\\end{cases}]\\n\\nThis function, also called the unit impulse sequence, is shown in Fig. 3.6a. The shifted impulse sequence (\\\\delta[n-m]) is depicted in Fig. 3.6b. Unlike its continuous-time counterpart (\\\\delta(t)) (the Dirac delta), the Kronecker delta is a very simple function, requiring no special esoteric knowledge of distribution theory.\\n\\nFigure 3.6: Discrete-time impulse function: (a) unit impulse sequence and (b) shifted impulse sequence.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='3.3-2 Discrete-Time Unit Step Function (u[n])\\n\\nThe discrete-time counterpart of the unit step function (u(t)) is (u[n]) (Fig. 3.7a), defined by\\n\\n[u[n]=\\\\begin{cases}1&\\\\quad\\\\text{for $n\\\\geq 0$}\\\\ 0&\\\\quad\\\\text{for $n<0$}\\\\end{cases}]\\n\\nIf we want a signal to start at (n=0) (so that it has a zero value for all (n<0)), we need only multiply the signal by (u[n]).\\n\\n3.3 Describing Signals with Unit Step and Unit Impulse Functions', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Describe the signal (x[n]) shown in Fig. 3.7b by a single expression valid for all (n).\\n\\nThere are many different ways of viewing (x[n]). Although each way of viewing yields a different expression, they are all equivalent. We shall consider here just one possible expression.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='The signal (x[n]) can be broken into three components: (1) a ramp component (x_{1}[n]) from (n=0) to (4), (2) a scaled step component (x_{2}[n]) from (n=5) to (10), and (3) an impulse component (x_{3}[n]) represented by the negative spike at (n=8). Let us consider each one separately.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='We express (x_{1}[n]=n(u[n]-u[n-5])) to account for the signal from (n=0) to (4). Assuming that the spike at (n=8) does not exist, we can express (x_{2}[n]=4\\\\left(u[n-5]-u[n-11]\\\\right)) to account for the signal from (n=5) to (10). Once these two components have been added, the only part that is unaccounted for is a spike of amplitude (-2) at (n=8), which can be represented by(x_{3}[n]=-2\\\\delta[n-8]). Hence,', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[x[n] =x_{1}[n]+x_{2}[n]+x_{3}[n]] [=n(u[n]-u[n-5])+4\\\\left(u[n-5]-u[n-11]\\\\right)-2\\\\delta[n-8]\\\\qquad \\\\text{for all $n$}]\\n\\nWe stress again that the expression is valid for all values of (n). The reader can find several other equivalent expressions for (x[n]). For example, one may consider a scaled step function from (n=0) to (10), subtract a ramp over the range (n=0) to (3), and subtract the spike. You can also play with breaking (n) into different ranges for your expression.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='3.3-3 Discrete-Time Exponential (\\\\gamma^{n})\\n\\nA continuous-time exponential (e^{\\\\lambda t}) can be expressed in an alternate form as\\n\\n[e^{\\\\lambda t}=\\\\gamma^{t}\\\\qquad(\\\\gamma=e^{\\\\lambda}\\\\;\\\\;\\\\text{or}\\\\;\\\\;\\\\lambda= \\\\ln\\\\gamma)]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='For example, (e^{-0.3t}=(0.7408)^{t}) because (e^{-0.3}=0.7408). Conversely, (4^{t}=e^{1.386t}) because (e^{1.386}=4), that is, (\\\\ln 4=1.386). In the study of continuous-time signals and systems, we prefer the form (e^{\\\\lambda t}) rather than (\\\\gamma^{t}). In contrast, the exponential form (\\\\gamma^{n}) is preferable in the study of discrete-time signals and systems, as will become apparent later. The discrete-time exponential (\\\\gamma^{n}) can also be expressed by using a natural base, as', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[e^{\\\\lambda n}=\\\\gamma^{n}\\\\qquad(\\\\gamma=e^{\\\\lambda}\\\\;\\\\;\\\\text{or}\\\\;\\\\;\\\\lambda= \\\\ln\\\\gamma)]\\n\\nBecause of unfamiliarity with exponentials with bases other than (e), exponentials of the form (\\\\gamma^{n}) may seem inconvenient and confusing at first. The reader is urged to plot some exponentials to acquire a sense of these functions. Also observe that (\\\\gamma^{-n}=\\\\left(\\\\frac{1}{\\\\gamma}\\\\right)^{n}).\\n\\n3.6 Equivalent Forms of DT Exponentials', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Show that (i) ((0.25)^{-n}=4^{n}), (ii) (4^{-n}=(0.25)^{n}), (iii) (e^{2t}=(7.389)^{t}), (iv) (e^{-2t}=(0.1353)^{t}=(7.389)^{-t}), (v) (e^{3n}=(20.086)^{n}), and (vi) (e^{-1.5n}=(0.2231)^{n}=(4.4817)^{-n}).\\n\\nShow that (i) (2^{n}=e^{0.693n}), (ii) ((0.5)^{n}=e^{-0.693n}), and (iii) ((0.8)^{-n}=e^{0.2231n}).\\n\\n3.6.1 Nature of (\\\\gamma^{n}).', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='The signal (e^{\\\\lambda n}) grows exponentially with (n) if (\\\\operatorname{Re}\\\\lambda>0) ((\\\\lambda) in the RHP), and decays exponentially if (\\\\operatorname{Re}\\\\lambda<0) ((\\\\lambda) in the LHP). It is constant or oscillates with constant amplitude if (\\\\operatorname{Re}\\\\lambda=0) ((\\\\lambda) on the imaginary axis). Clearly, the location of (\\\\lambda) in the complex plane indicates whether the signal (e^{\\\\lambda n}) will grow exponentially, decay exponentially, or oscillate with constant amplitude', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='(Fig. 3.8a). A constant signal ((\\\\lambda=0)) is also an oscillation with zero frequency. We now find a similar criterion for determining the nature of (\\\\gamma^{n}) from the location of (\\\\gamma) in the complex plane.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Figure 3.8a shows a complex plane ((\\\\lambda) plane). Consider a signal (e^{i\\\\Omega n}). In this case, (\\\\lambda=j\\\\Omega) lies on the imaginary axis (Fig. 3.8a), and therefore is a constant-amplitude oscillating signal. This signal (e^{i\\\\Omega n}) can be expressed as (\\\\gamma^{n}), where (\\\\gamma=e^{i\\\\Omega}). Because the magnitude of (e^{i\\\\Omega}) is unity, (|\\\\gamma|=1). Hence, when (\\\\lambda) lies on the imaginary axis, the corresponding (\\\\gamma) lies on a circle of unit radius, centered at the', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='origin (the unit circle illustrated in Fig. 3.8b). Therefore, a signal (\\\\gamma^{n}) oscillates with constant amplitude if (\\\\gamma) lies on the unit circle. Thus, the imaginary axis in the (\\\\lambda) plane maps into the unit circle in the (\\\\gamma) plane.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Next consider the signal (e^{i\\\\lambda n}), where (\\\\lambda) lies in the left half-plane in Fig. 3.8a. This means (\\\\lambda=a+jb), where (a) is negative ((a<0)). In this case, the signal decays exponentially. This signal can be expressed as (\\\\gamma^{n}), where\\n\\n[\\\\gamma=e^{\\\\lambda}=e^{a+jb}=e^{a}\\\\,e^{jb}]\\n\\nand\\n\\n[|\\\\gamma|=|e^{a}|\\\\,|e^{ib}|=e^{a}\\\\qquad\\\\text{because }|e^{ib}|=1]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Also, (a) is negative ((a<0)). Hence, (|\\\\gamma|=e^{a}<1). This result means that the corresponding (\\\\gamma) lies inside the unit circle. Therefore, a signal (\\\\gamma^{n}) decays exponentially if (\\\\gamma) lies within the unit circle (Fig. 3.8b). If, in the preceding case we select (a) to be positive ((\\\\lambda) in the right half-plane), then (|\\\\gamma|>1), and (\\\\gamma) lies outside the unit circle. Therefore, a signal (\\\\gamma^{n}) grows exponentially if (\\\\gamma) lies outside the unit circle (Fig.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='3.8b).', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='To summarize, the imaginary axis in the (\\\\lambda) plane maps into the unit circle in the (\\\\gamma) plane. The left half-plane in the (\\\\lambda) plane maps into the inside of the unit circle and the right half of the (\\\\lambda) plane maps into the outside of the unit circle in the (\\\\gamma) plane, as depicted in Fig. 3.8.\\n\\nFigure 3.8: The (\\\\lambda) plane, the (\\\\gamma) plane, and their mapping.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Plots of ((0.8)^{n}) and ((-0.8)^{n}) appear in Figs. 3.9a and 3.9b, respectively. Plots of ((0.5)^{n}) and ((1.1)^{n}) appear in Figs. 3.9c and 3.9d, respectively. These plots verify our earlier conclusions about the location of (\\\\gamma) and the nature of signal growth. Observe that a signal ((-|\\\\gamma|)^{n}) alternates sign successively (is positive for even values of (n) and negative for odd values of (n), as depicted in Fig. 3.9b). Also, the exponential ((0.5)^{n}) decays faster than', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='((0.8)^{n}) because (0.5) is closer to the origin than (0.8). The exponential ((0.5)^{n}) can also be expressed as (2^{-n}) because ((0.5)^{-1}=2).', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='3.7 Sketching DT Exponentials', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Sketch the following signals: (a)((1)^{n}), (b)((-1)^{n}), (c)((0.5)^{n}), (d)((-0.5)^{n}), (e)((0.5)^{-n}), (f)(2^{-n}), and (g)((-2)^{n}). Express these exponentials as (\\\\gamma^{n}), and plot (\\\\gamma) in the complex plane for each case. Verify that (\\\\gamma^{n}) decays exponentially with (n) if (\\\\gamma) lies inside the unit circle and that (\\\\gamma^{n}) grows with (n) if (\\\\gamma) is outside the unit circle. If (\\\\gamma) is on the unit circle, (\\\\gamma^{n}) is constant or oscillates with a', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='constant amplitude.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Figure 3.9: Discrete-time exponentials (\\\\gamma^{n}).\\n\\n[MISSING_PAGE_FAIL:14]\\n\\n3.3-4 Discrete-Time Sinusoid (\\\\cos\\\\left(\\\\Omega n+\\\\theta\\\\right))\\n\\nA general discrete-time sinusoid can be expressed as (C\\\\cos\\\\left(\\\\Omega n+\\\\theta\\\\right)), where (C) is the amplitude, and (\\\\theta) is the phase in radians. Also, (\\\\Omega n) is an angle in radians. Hence, the dimensions of the frequency (\\\\Omega) are radians per sample. This sinusoid may also be expressed as', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[C\\\\cos\\\\left(\\\\Omega n+\\\\theta\\\\right)=C\\\\cos\\\\left(2\\\\pi\\\\mathcal{F}n+\\\\theta\\\\right)]\\n\\nwhere (\\\\mathcal{F}=\\\\Omega/2\\\\pi). Therefore, the dimensions of the discrete-time frequency (\\\\mathcal{F}) are (radians(/2\\\\pi)) per sample, which is equal to cycles per sample. This means if (N_{0}) is the period (samples/cycle) of the sinusoid, then the frequency of the sinusoid (\\\\mathcal{F}=1/N_{0}) (samples/cycle).', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Figure 3.11 shows a discrete-time sinusoid (\\\\cos\\\\left(\\\\frac{\\\\pi}{12}n+\\\\frac{\\\\pi}{4}\\\\right)). For this case, the frequency is (\\\\Omega=\\\\pi/12) radians/sample. Alternately, the frequency is (\\\\mathcal{F}=1/24) cycles/sample. In other words, there are 24 samples in one cycle of the sinusoid.\\n\\nBecause (\\\\cos\\\\left(-x\\\\right)=\\\\cos\\\\left(x\\\\right)),\\n\\n[\\\\cos\\\\left(-\\\\Omega n+\\\\theta\\\\right)=\\\\cos\\\\left(\\\\Omega n-\\\\theta\\\\right)]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='This shows that both (\\\\cos\\\\left(\\\\Omega n+\\\\theta\\\\right)) and (\\\\cos\\\\left(-\\\\Omega n+\\\\theta\\\\right)) have the same frequency ((\\\\Omega)). Therefore, the frequency of (\\\\cos\\\\left(\\\\Omega n+\\\\theta\\\\right)) is (|\\\\Omega|).\\n\\n3.4 Discrete-Time Sinusoid\\n\\nA continuous-time sinusoid (\\\\cos\\\\omega t) sampled every (T) seconds yields a discrete-time sequence whose (n)th element (at (t=nT)) is (\\\\cos\\\\omega nT). Thus, the sampled signal (x[n]) is given by', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[x[n]=\\\\cos\\\\omega nT=\\\\cos\\\\Omega n\\\\qquad\\\\text{where }\\\\Omega=\\\\omega T]\\n\\nFigure 3.11: A discrete-time sinusoid (\\\\cos\\\\left(\\\\frac{\\\\pi}{12}n+\\\\frac{\\\\pi}{4}\\\\right)).\\n\\nThus, a continuous-time sinusoid (\\\\cos\\\\omega t) sampled every (T) seconds yields a discrete-time sinusoid (\\\\cos\\\\Omega n), where (\\\\Omega=\\\\omega T).+\\n\\nFootnote †: margin:\\n\\nFootnote †: margin:\\n\\n3.3-5 Discrete-Time Complex Exponential (e^{i\\\\Omega n})', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content=\"Using Euler's formula, we can express an exponential (e^{i\\\\Omega n}) in terms of sinusoids as\\n\\n[e^{i\\\\Omega n}=(\\\\cos\\\\Omega n+j\\\\sin\\\\Omega n)\\\\qquad\\\\text{and}\\\\qquad e^{-j\\\\Omega n }=(\\\\cos\\\\Omega n-j\\\\sin\\\\Omega n)]\\n\\nThese equations show that the frequency of both (e^{i\\\\Omega n}) and (e^{-j\\\\Omega n}) is (\\\\Omega) (radians/sample). Therefore, the frequency of (e^{i\\\\Omega n}) is (|\\\\Omega|).\\n\\nObserve that for (r=1) and (\\\\theta=n\\\\Omega),\\n\\n[e^{i\\\\Omega n}=re^{i\\\\theta}]\", metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='This equation shows that the magnitude and angle of (e^{i\\\\Omega n}) are (1) and (n\\\\Omega), respectively. In the complex plane, (e^{i\\\\Omega n}) is a point on a unit circle at an angle (n\\\\Omega).\\n\\nExample 3.5: Plotting a DT Sinusoid with MATLAB\\n\\nUsing MATLAB, plot the discrete-time sinusoid (x[n]=\\\\cos\\\\big{(}\\\\frac{\\\\pi}{12}n+\\\\frac{\\\\pi}{4}\\\\big{)}).', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content=\"We represent the desired sinusoid using an anonymous function. Next, we plot this function over the desired range of (n). The result, shown in Fig. 3.12, matches the plot of the same signal shown in Fig. 3.11.\\n\\nn = (-30:30); x = @(n) cos(n*pi/12+pi/4); >> clf; stem(n,x(n),'k'); ylabel('x[n]'); xlabel('n');\\n\\nExamples of Discrete-Time Systems\", metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='We shall give here four examples of discrete-time systems. In the first two examples, the signals are inherently of the discrete-time variety. In the third and fourth examples, a continuous-time signal is processed by a discrete-time system, as illustrated in Fig. 3.2, by discretizing the signal through sampling.\\n\\n3.6 Savings Account', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='A person makes a deposit (the input) in a bank regularly at an interval of (T) (say, 1 month). The bank pays a certain interest on the account balance during the period (T) and mails out a periodic statement of the account balance (the output) to the depositor. Find the equation relating the output (y[n]) (the balance) to the input (x[n]) (the deposit).\\n\\nIn this case, the signals are inherently discrete time. Let', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[x[n] =\\\\text{deposit made at the $n$th discrete instant}] [y[n] =\\\\text{account balance at the $n$th instant computed}] [\\\\qquad\\\\qquad\\\\text{immediately after receipt of the $n$th deposit}] [r =\\\\text{interest per dollar per period $T$}]\\n\\nThe balance (y[n]) is the sum of (i) the previous balance (y[n-1]), (ii) the interest on (y[n-1]) during the period (T), and (iii) the deposit (x[n])\\n\\n[y[n] =y[n-1]+ry[n-1]+x[n]] [=(1+r)y[n-1]+x[n]]\\n\\nor\\n\\n[y[n]-ay[n-1]=x[n]\\\\qquad a=1+r \\\\tag{3.3}]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='In this example the deposit (x[n]) is the input (cause) and the balance (y[n]) is the output (effect).\\n\\nFigure 3.12: Sinusoid plot for Ex. 3.5.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='A withdrawal from the account is a negative deposit. Therefore, this formulation can handle deposits as well as withdrawals. It also applies to a loan payment problem with the initial value (y[0]=-M), where (M) is the amount of the loan. A loan is an initial deposit with a negative value. Alternately, we may treat a loan of (M) dollars taken at (n=0) as an input of (-M) at (n=0) (see Prob. 3.8-23).', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='We can express Eq. (3.3) in an alternate form. The choice of index (n) in Eq. (3.3) is completely arbitrary, so we can substitute (n+1) for (n) to obtain\\n\\n[y[n+1]-ay[n]=x[n+1] \\\\tag{3.4}]\\n\\nWe also could have obtained Eq. (3.4) directly by realizing that (y[n+1]), the balance at instant ((n+1)), is the sum of (y[n]) plus (ry[n]) (the interest on (y[n])) plus the deposit (input) (x[n+1]) at instant ((n+1)).', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='The difference equation in Eq. (3.3) uses delays, whereas the form in Eq. (3.4) uses advances. Thus, Eq. (3.3) is said to be in delay form and Eq. (3.4) is said to be in advance form. The delay form is more natural because operation of delay is causal, hence realizable. In contrast, advance operation, being noncausal, is unrealizable. We use the advance form primarily for its mathematical convenience over the delay form.1', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Footnote 1: Use of the advance form results in discrete-time system equations that are identical in form to those for continuous-time systems. This will become apparent later. In transform analysis, advance form leads to the more convenient variable (z) instead of the clumsy (z^{-1}) that arises from delay form.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='We shall now represent this system in a block diagram form, which is basically a road map to a hardware (or software) realization of the system. For this purpose, the causal (realizable) delay form in Eq. (3.3) will be used. There are three basic operations in this equation: addition, scalar multiplication, and delay. Figure 3.13 shows their schematic representation. In addition, we also have a pickoff node (Fig. 3.13d), which is used to provide multiple copies of a signal at its input.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Figure 3.13: Schematic representations of basic operations on sequences.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Figure 3.14 shows in block diagram form a system represented by Eq. (3.3). To understand this realization, it is helpful to rewrite Eq. (3.3) as (y[n]=ay[n-1]+x[n]) ((a=1+r)). Now, assume that the output (y[n]) is available at the pickoff node (N). Unit delay of (y[n]) results in (y[n-1]), which is multiplied by a scalar of value (a) to yield (ay[n-1]). Next, we generate (y[n]) by adding the input (x[n]) and (ay[n-1]).2 Observe that node (N) is a pickoff node, from which two copies of the', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='output signal flow out: one as the feedback signal and the other as the output signal.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Footnote 2: A unit delay represents 1 unit of time delay. In this example, 1 unit of delay in the output corresponds to period (T) for the actual output.\\n\\nExample 3.7 Sales Estimate', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='During semester (n), (x[n]) students enroll in a course requiring a certain textbook while the publisher sells (y[n]) new copies of the same book. On the average, one-quarter of students with books in salable condition resell the texts at the end of the semester, and the book life is three semesters. Write the equation relating (y[n]), the new books sold by the publisher, to (x[n]), the number of students enrolled in the (n)th semester, assuming that every student buys a book.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='In the (n)th semester, the total books (x[n]) sold to students must be equal to (y[n]) (new books from the publisher) plus the used books from students enrolled in the preceding two semesters (because the book life is only three semesters). There are (y[n-1]) new books sold in semester ((n-1)), and one-quarter of these books, that is, ((1/4)y[n-1]), will be resold in the (n)th semester. Also, (y[n-2]) new books are sold in semester (n-2), and one-quarter of these, that is, ((1/4)y[n-2]), will', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='be resold in semester ((n-1)). Again, a quarter of these, that is, ((1/16)y[n-2]), will be resold in the (n)th semester. Therefore, (x[n]) must be equal to the sum of (y[n]), ((1/4)y[n-1]), and ((1/16)y[n-2]).', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[y[n]+\\\\tfrac{1}{4}y[n-1]+\\\\tfrac{1}{16}y[n-2]=x[n] \\\\tag{3.5}]\\n\\nEquation (3.5) can also be expressed in an alternative form by realizing that this equation is valid for any value of (n). Therefore, replacing (n) by (n+2), we obtain', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[y[n+2]+\\\\tfrac{1}{4}y[n+1]+\\\\tfrac{1}{16}y[n]=x[To facilitate a realization of a system with this input-output equation, we rewrite the delay-form Eq. (3.5) as (y[n]=-\\\\frac{1}{4}y[n-1]-\\\\frac{1}{16}y[n-2]+x[n]). Figure 3.15 shows a corresponding hardware realization using two unit delays in cascade.+', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Footnote †: The comments in the preceding footnote apply here also. Although 1 unit of delay in this example is one semester, we need not use this value in the hardware realization. Any value other than one semester results in a time-scaled output.\\n\\nExample 3.8 Digital Differentiator\\n\\nDesign a discrete-time system, like the one in Fig. 3.2, to differentiate continuous-time signals. This differentiator is used in an audio system having an input signal bandwidth below 20 kHz.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='In this case, the output (y(t)) is required to be the derivative of the input (x(t)). The discrete-time processor (system) (G) processes the samples of (x(t)) to produce the discrete-time output (y[n]). Let (x[n]) and (y[n]) represent the samples (T) seconds apart of the signals (x(t)) and (y(t)), respectively, that is,\\n\\n[x[n]=x(nT)\\\\qquad\\\\text{and}\\\\qquad y[n]=y(nT) \\\\tag{3.7}]\\n\\nThe signals (x[n]) and (y[n]) are the input and the output for the discrete-time system (G). Now, we require that', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[y(t)=\\\\frac{dx(t)}{dt}]\\n\\nTherefore, at (t=nT) (see Fig. 3.16a),\\n\\n[y(nT)=\\\\left.\\\\frac{dx(t)}{dt}\\\\right|{t=nT}=\\\\lim{T\\\\to 0}\\\\frac{1}{T}\\\\left[x(nT)-x[(n-1)T]\\\\right]]\\n\\nFigure 3.15: Realization of the system representing sales estimate in Ex. 3.7.\\n\\nBy using the notation in Eq. (3.7), the foregoing equation can be expressed as\\n\\n[y[n]=\\\\lim_{T\\\\to 0}\\\\frac{1}{T}{x[n]-x[n-1]}]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='This is the input-output relationship for (G) required to achieve our objective. In practice, the sampling interval (T) cannot be zero. Assuming (T) to be sufficiently small, the equation just given can be expressed as\\n\\n[y[n]=\\\\frac{1}{T}{x[n]-x[n-1]} \\\\tag{3.8}]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='The approximation improves as (T) approaches 0. A discrete-time processor (G) to realize Eq. (3.8) is shown inside the shaded box in Fig. 3.16b. The system in Fig. 3.16b acts as a differentiator. This example shows how a continuous-time signal can be processed by a\\n\\nFigure 3.16: Digital differentiator and its realization.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='discrete-time system. The considerations for determining the sampling interval (T) are discussed in Chs. 5 and 8, where it is shown that to process frequencies below 20 kHz, the proper choice is\\n\\n[T\\\\leq\\\\frac{1}{2\\\\times\\\\text{highest frequency}}=\\\\frac{1}{40,000}=25\\\\;\\\\mu\\\\text{s}]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='To see how well this method works, let us consider the differentiator in Fig. 3.16b with a ramp input (x(t)=t), depicted in Fig. 3.16c. If the system were to act as a differentiator, then the output (y(t)) of the system should be the unit step function (u(t)). Let us investigate how the system performs this particular operation and how well the system achieves the objective.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='The samples of the input (x(t)=t) at the interval of (T) seconds act as the input to the discrete-time system (G). These samples, denoted by a compact notation (x[n]), are, therefore,\\n\\n[x[n] =x(t)|{t=nT}=t|{t=nT}\\\\qquad t\\\\geq 0] [=nT\\\\qquad n\\\\geq 0]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Figure 3.16d shows the sampled signal (x[n]). This signal acts as an input to the discrete-time system (G). Figure 3.16b shows that the operation of (G) consists of subtracting a sample from the preceding (delayed) sample and then multiplying the difference with (1/T). From Fig. 3.16d, it is clear that the difference between the successive samples is a constant (nT-(n-1)T=T) for all samples, except for the sample at (n=0) (because there is no preceding sample at (n=0)). The output of (G) is', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='(1/T) times the difference (T), which is unity for all values of (n), except (n=0), where it is zero. Therefore, the output (y[n]) of (G) consists of samples of unit values for (n\\\\geq 1), as illustrated in Fig. 3.16e. The D/C (discrete-time to continuous-time) converter converts these samples into a continuous-time signal (y(t)), as shown in Fig. 3.16f. Ideally, the output should have been (y(t)=u(t)). This deviation from the ideal is due to our use of a nonzero sampling interval (T). As (T)', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='approaches zero, the output (y(t)) approaches the desired output (u(t)).', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='The digital differentiator in Eq. (3.8) is an example of what is known as the backward difference system. The reason for calling it so is obvious from Fig. 3.16a. To compute the derivative of (y(t)), we are using the difference between the present sample value and the preceding (backward) sample value. If we use the difference between the next (forward) sample at (t=(n+1)T) and the present sample at (t=nT), we obtain a forward difference form of differentiator as', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[y[n]=\\\\frac{1}{T}{x[n+1]-x[n]} \\\\tag{3.9}]\\n\\nFor an integrator, the input (x(t)) and the output (y(t)) are related by\\n\\n[y(t)=\\\\int_{-\\\\infty}^{t}x(\\\\tau)\\\\,d\\\\tau]Therefore, at (t=nT) (see Fig. 3.16a),\\n\\n[y(nT)=\\\\lim_{T\\\\to 0}\\\\sum_{k=-\\\\infty}^{n}x(kT)T]\\n\\nUsing the usual notation (x(kT)=x[k],y(nT)=y[n]), and so on, this equation can be expressed as\\n\\n[y[n]=\\\\lim_{T\\\\to 0}T\\\\sum_{k=-\\\\infty}^{n}x[k]]\\n\\nAssuming that (T) is small enough to justify the assumption (T\\\\to 0), we have', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[y[n]=T\\\\sum_{k=-\\\\infty}^{n}x[k] \\\\tag{3.10}]\\n\\nThis equation represents an example of accumulator system. This digital integrator equation can be expressed in an alternate form. From Eq. (3.10), it follows that\\n\\n[y[n]-y[n-1]=Tx[n] \\\\tag{3.11}]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='This is an alternate description for the digital integrator. Equations (3.10) and (3.11) are equivalent; the one can be derived from the other. Observe that the form of Eq. (3.11) is similar to that of Eq. (3.3). Hence, the block diagram representation of a digital integrator in the form of Eq. (3.11) is identical to that in Fig. 3.14 with (a=1) and the input multiplied by (T).\\n\\nRecursive and Nonrecursive Forms of\\n\\nDifference Equation', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='If Eq. (3.11) expresses Eq. (3.10) in another form, what is the difference between these two forms? Which form is preferable? To answer these questions, let us examine how the output is computed by each of these forms. In Eq. (3.10), the output (y[n]) at any instant (n) is computed by adding all the past input values till (n). This can mean a large number of additions. In contrast, Eq. (3.11) can be expressed as (y[n]=y[n-1]+Tx[n]). Hence, computation of (y[n]) involves addition of only two', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='values: the preceding output value (y[n-1]) and the present input value (x[n]). The computations are done recursively by using the preceding output values. For example, if the input starts at (n=0), we first compute (y[0]). Then we use the computed value (y[0]) to compute (y[1]). Knowing (y[1]), we compute (y[2]), and so on. The computations are recursive. This is why the form of Eq. (3.11) is called recursive form and the form of Eq. (3.10) is called nonrecursive form. Clearly, \"recursive\" and', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='\"nonrecursive\" describe two different ways of presenting the same information. Equations (3.3), (3.5), and (3.11) are examples of recursive form, and Eqs. (3.8) and (3.10) are examples of nonrecursive form.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[MISSING_PAGE_EMPTY:24]\\n\\n3.4 Examples of Discrete-Time Systems\\n\\n3.4.1 Digital Integrator Design\\n\\nDesign a digital integrator in Ex. 3.9 using the fact that for an integrator, the output (y(t)) and the input (x(t)) are related by (dy(t)/dt=x(t)). Approximation (similar to that in Ex. 3.8) of this equation at (t=nT) yields the recursive form in Eq. (3.11).\\n\\n3.4.2 Analog, Digital, Continuous-Time, and Discrete-Time Systems', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='The basic difference between continuous-time systems and analog systems, as also between discrete-time and digital systems, is fully explained in Secs. 1.7-5 and 1.7-6.1 Historically, discrete-time systems have been realized with digital computers, where continuous-time signals are processed through digitized samples rather than unquantized samples. Therefore, the terms digital filters and discrete-time systems are used synonymously in the literature. This distinction is irrelevant in the', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='analysis of discrete-time systems. For this reason, we follow this loose convention in this book, where the term digital filter implies a discrete-time system, and analog filter means continuous-time system. Moreover, the terms C/D (continuous-to-discrete-time ) and D/C will occasionally be used interchangeably with terms A/D (analog-to-digital) and D/A, respectively.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Footnote 1: The terms discrete-time and continuous-time qualify the nature of a signal along the time axis (horizontal axis). The terms analog and digital, in contrast, qualify the nature of the signal amplitude (vertical axis).\\n\\nAdvantages of Digital Signal Processing', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Digital systems operation can tolerate considerable variation in signal values, and hence are less sensitive to changes in the component parameter values due to temperature variation, aging, and other factors. This results in greater degree of precision and stability. Since digital systems are binary circuits, their accuracy can be increased by using more complex circuitry to increase word length, subject to cost limitations.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Digital systems do not require any factory adjustment and can be easily duplicated in volume without having to worry about precise component values. They can be fully integrated, and even highly complex systems can be placed on a single chip by using VLSI (very-large-scale integrated) circuits.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Digital filters are more flexible. Their characteristics can be easily altered simply by changing the program. Digital hardware implementation permits the use of microprocessors, miniprocessors, digital switching, and large-scale integrated circuits.\\n\\nA greater variety of filters can be realized by digital systems.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Digital signals can be stored easily and inexpensively on various media (e.g., magnetic, optical, and solid state) without deterioration of signal quality. It is also possible (and increasingly popular) to search and select information from distant electronic storehouses, such as the cloud.\\n\\nDigital signals can be coded to yield extremely low error rates and high fidelity, as well as privacy. Also, more sophisticated signal-processing algorithms can be used to process digital signals.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Digital filters can be easily time-shared and therefore can serve a number of inputs simultaneously. Moreover, it is easier and more efficient to multiplex several digital signals on the same channel.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Reproduction with digital messages is extremely reliable without deterioration. Analog messages such as photocopies and films, for example, lose quality at each successive stage of reproduction and have to be transported physically from one distant place to another, often at relatively high cost.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='One must weigh these advantages against such disadvantages as increased system complexity due to use of A/D and D/A interfaces, limited range of frequencies available in practice (affordable rates are gigahertz or less), and use of more power than is needed for the passive analog circuits. Digital systems use power-consuming active devices.\\n\\n3.4-1 Classification of Discrete-Time Systems', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Before examining the nature of discrete-time system equations, let us consider the concepts of linearity, time invariance (or shift invariance), and causality, which apply to discrete-time systems also.\\n\\nLinearity and Time Invariance\\n\\nFor discrete-time systems, the definition of linearity is identical to that for continuous-time systems, as given in Eq. (1.22). We can show that the systems in Exs. 3.6, 3.7, 3.8, and 3.9 are all linear.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Time invariance (or shift invariance) for discrete-time systems is also defined in a way similar to that for continuous-time systems. Systems whose parameters do not change with time (with (n)) are time-invariant or shift-invariant (also constant-parameter) systems. For such a system, if the input is delayed by (k) units or samples, the output is the same as before but delayed by (k) samples (assuming the initial conditions also are delayed by (k)). The systems in Exs. 3.6, 3.7, 3.8, and 3.9', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='are time-invariant because the coefficients in the system equations are constants (independent of (n)). If these coefficients were functions of (n) (time), then the systems would be linear time-varying systems. Consider, for example, a system described by', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[y[n]=e^{-n}x[n]]\\n\\nFor this system, let a signal (x_{1}[n]) yield the output (y_{1}[n]), and another input (x_{2}[n]) yield the output (y_{2}[n]). Then\\n\\n[y_{1}[n]=e^{-n}x_{1}[n]\\\\qquad\\\\text{and}\\\\qquad y_{2}[n]=e^{-n}x_{2}[n]]\\n\\nIf we let (x_{2}[n]=x_{1}[n-N_{0}]), then\\n\\n[y_{2}[n]=e^{-n}x_{2}[n]=e^{-n}x_{1}[n-N_{0}]\\\\neq y_{1}[n-N_{0}]]\\n\\nClearly, this is a time-varying parameter system.\\n\\nCausal and Noncausal Systems', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='A causal (also known as a physical or nonanticipative) system is one for which the output at any instant (n=k) depends only on the value of the input (x[n]) for (n\\\\leq k). In other words, the value of the output at the present instant depends only on the past and present values of the input (x[n]), not on its future values. As we shall see, the systems in Exs. 3.6, 3.7, 3.8, and 3.9 are all causal.\\n\\nInvertible and Noninvertible Systems', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='A discrete-time system (\\\\mathcal{S}) is invertible if an inverse system (\\\\mathcal{S}{i}) exists such that the cascade of (\\\\mathcal{S}) and (\\\\mathcal{S}{i}) results in an identity system. An identity system is defined as one whose output is identical to the input. In other words, for an invertible system, the input can be uniquely determined from the corresponding output. For every input there is a unique output. When a signal is processed through such a system, its input can be reconstructed', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='from the corresponding output. There is no loss of information when a signal is processed through an invertible system.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='A cascade of a unit delay with a unit advance results in an identity system because the output of such a cascaded system is identical to the input. Clearly, the inverse of an ideal unit delay is ideal unit advance, which is a noncausal (and unrealizable) system. In contrast, a compressor (y[n]=x[Mn]) is not invertible because this operation loses all but every (M)th sample of the input, and, generally, the input cannot be reconstructed. Similarly, operations, such as (y[n]=\\\\cos x[n]) or', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='(y[n]=|x[n]|), are not invertible.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[breakable]\\n\\nDRIL 3.9: Invertibility\\n\\nShow that a system specified by equation (y[n]=ax[n]+b) is invertible but that the system (y[n]=|x[n]|^{2}) is noninvertible.\\n\\nStable and Unstable Systems', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='The concept of stability is similar to that in continuous-time systems. Stability can be internal or external. If every bounded input applied at the input terminal results in a bounded output, the system is said to be stable externally. External stability can be ascertained by measurements at the external terminals of the system. This type of stability is also known as the stability in the BIBO (bounded-input/bounded-output) sense. Both internal and external stability are discussed in greater', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='detail in Sec. 3.9.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Memoryless Systems and Systems with Memory', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='The concepts of memoryless (or instantaneous) systems and those with memory (or dynamic) are identical to the corresponding concepts of the continuous-time case. A system is memoryless if its response at any instant (n) depends at most on the input at the same instant (n). The output at any instant of a system with memory generally depends on the past, present, and future values of the input. For example, (y[n]=\\\\sin x[n]) is an example of instantaneous system, and (y[n]-y[n-1]=x[n]) is an', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='example of a dynamic system or a system with memory.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Example 3.10: Investigating DT System Properties\\n\\nConsider a DT system described as (y[n+1]=x[n+1]x[n]). Determine whether the system is (a) linear, (b) time-invariant, (c) causal, (d) invertible, (e) BIBO-stable, and (f) memoryless.\\n\\nLet us delay the input-output equation by one to obtain the equivalent but more convenient representation of (y[n]=x[n]x[n-1]).', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='(a) Linearity requires both homogeneity and additivity. Let us first investigate homogeneity. Assuming (x[n]\\\\Longrightarrow y[n]), we see that\\n\\n[ax[n]\\\\Longrightarrow(ax[n])(ax[n-1])=a^{2}y[n]\\\\neq ay[n]]\\n\\nThus, the system does not satisfy the homogeneity property.\\n\\nThe system also does not satisfy the additivity property. Assuming (x_{1}[n]\\\\Longrightarrow y_{1}[n]) and (x_{2}[n]\\\\Longrightarrow y_{2}[n]), we see that input (x[n]=x_{1}[n]+x_{2}[n]) produces output (y[n]) as', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[y[n] =(x_{1}[n]+x_{2}[n])(x_{1}[n-1]+x_{2}[n-1])] [=x_{1}[n]x_{1}[n-1]+x_{2}[n]x_{2}[n-1]+x_{1}[n]x_{2}[n-1]+x_{2}[n ]x_{1}[n-1]] [=y_{1}[n]+y_{2}[n]+x_{1}[n]x_{2}[n-1]+x_{2}[n]x_{1}[n-1]] [\\\\neq y_{1}[n]+y_{2}[n]]\\n\\nClearly, additivity is not satisfied.\\n\\nSince the system does not satisfy both the homogeneity and additivity properties, we conclude that the system is not linear.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='(b) To be time-invariant, a shift in any input should cause a corresponding shift in respective output. Assume that (x[n]\\\\Longrightarrow y[n]). Applying a delay version of this input to the system yields\\n\\n[x[n-N]\\\\Longrightarrow x[n-N]x[n-1-N]=x[(n-N)]x[(n-N)-1]=y[n-N]]\\n\\nSince shifting an input causes a corresponding shift in the output, we conclude that the system is time-invariant.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='(c) To be causal, an output value cannot depend on any future input values. The output (y) at time (n) depends on the input (x) at present and past times (n) and (n-1). Since the current output does not depend on future input values, the system is causal.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='(d) For a system to be invertible, every input must generate a unique output, which allows exact recovery of the input from the output. Consider two inputs to this system: (x_{1}[n]=1) and (x_{2}[n]=-1). Both inputs generate the same output: (y_{1}[n]=y_{2}[n]=1). Since unique inputs do not always generate unique outputs, we conclude that the system is not invertible.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='(e) To be BIBO-stable, any bounded input must generate a bounded output. A bounded input satisfies (|x[n]|\\\\leq M_{x}<\\\\infty) for all (n). Given this condition, the system output magnitude behaves as\\n\\n[|y[n]|=|x[n]x[n-1]|=|x[n]|\\\\,|x[n-1]|\\\\leq M_{x}^{2}<\\\\infty]Since any bounded input is guaranteed to produce a bounded output, it follows that the system is BIBO-stable.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content=\"(f) To be memoryless, a system's output can only depend on the strength of the current input. Since the output (y) at time (n) depends on the input (x) not only at present time (n) but also on past time (n-1), we see that the system is not memoryless.\\n\\n3.5 Discrete-Time System Equations\\n\\nIn this section we discuss time-domain analysis of LTID (linear, time-invariant, discrete-time systems). With minor differences, the procedure is parallel to that for continuous-time systems.\", metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Difference Equations', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Equations (3.3), (3.5), (3.8), and (3.13) are examples of difference equations. Equations (3.3), (3.8), and (3.13) are first-order difference equations, and Eq. (3.5) is a second-order difference equation. All these equations are linear, with constant (not time-varying) coefficients.2 Before giving a general form of an (N)th-order linear difference equation, we recall that a difference equation can be written in two forms: the first form uses delay terms such as (y[n-1]), (y[n-2]), (x[n-1]),', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='(x[n-2]), and so on; and the alternate form uses advance terms such as (y[n+1]), (y[n+2]), and so on. Although the delay form is more natural, we shall often prefer the advance form, not just for the general notational convenience, but also for resulting notational uniformity with the operator form for differential equations. This facilitates the commonality of the solutions and concepts for continuous-time and discrete-time systems.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Footnote 2: Equations such as (3.3), (3.5), (3.8), and (3.13) are considered to be linear according to the classical definition of linearity. Some authors label such equations as incrementally linear. We prefer the classical definition. It is just a matter of individual choice and makes no difference in the final results.\\n\\nWe start here with a general difference equation, written in advance form as', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[y[n+N]+a_{1}y[n+N-1]+\\\\cdot\\\\cdot\\\\cdot+a_{N-1}y[n+1]+a_{N}y[n]=] [b_{N-M}x[n+M]+b_{N-M+1}x[n+M-1]+\\\\cdot\\\\cdot\\\\cdot+b_{N-1}x[n+1]+b_ {N}x[n] \\\\tag{3.14}]\\n\\nThis is a linear difference equation whose order is (\\\\max(N,M)). We have assumed the coefficient of (y[n+N]) to be unity ((a_{0}=1)) without loss of generality. If (a_{0}\\\\neq 1), we can divide the equation throughout by (a_{0}) to normalize the equation to have (a_{0}=1).\\n\\nCausality Condition', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='For a causal system, the output cannot depend on future input values. This means that when the system equation is in the advance form of Eq. (3.14), causality requires (M\\\\leq N). If (M) were to be greater than (N), then (y[n+N]), the output at (n+N) would depend on (x[n+M]), which is the input at the later instant (n+M). For a general causal case, (M=N), and Eq. (3.14) can be expressed as', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[y[n+N]+a_{1}y[n+N-1]+\\\\cdot\\\\cdot\\\\cdot+a_{N-1}y[n+1]+a_{N}y[n]=] [b_{0}x[n+N]+b_{1}x[n+N-1]+\\\\cdot\\\\cdot\\\\cdot+b_{N-1}x[n+1]+b_{N}x[n] \\\\tag{3.15}]where some of the coefficients on either side can be zero. In this (N)th-order equation, (a_{0}), the coefficient of (y[n+N]), is normalized to unity. Equation (3.15) is valid for all values of (n). Therefore, it is still valid if we replace (n) by (n-N) throughout the equation [see Eqs. (3.3) and (3.4)]. Such replacement yields a delay-form alternative:', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[y[n]+a_{1}y[n-1]+\\\\cdot\\\\cdot\\\\cdot+a_{N-1}y[n-N+1]+a_{N}y[n-N]=] [b_{0}x[n]+b_{1}x[n-1]+\\\\cdot\\\\cdot\\\\cdot+b_{N-1}x[n-N+1]+b_{N}x[n-N] \\\\tag{3.16}]\\n\\n3.5-1 Recursive (Iterative) Solution of Difference Equation\\n\\nEquation (3.16) can be expressed as\\n\\n[y[n]= -a_{1}y[n-1]-a_{2}y[n-2]-\\\\cdot\\\\cdot\\\\cdot-a_{N}y[n-N]] [+b_{0}x[n]+b_{1}x[n-1]+\\\\cdot\\\\cdot\\\\cdot+b_{N}x[n-N] \\\\tag{3.17}]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='In Eq. (3.17), (y[n]) is computed from (2N+1) pieces of information; the preceding (N) values of the output: (y[n-1]), (y[n-2]), (\\\\ldots), (y[n-N]), and the preceding (N) values of the input: (x[n-1]), (x[n-2]), (\\\\ldots), (x[n-N]), and the present value of the input (x[n]). Initially, to compute (y[0]), the (N) initial conditions (y[-1]), (y[-2]), (\\\\ldots), (y[-N]) serve as the preceding (N) output values. Hence, knowing the (N) initial conditions and the input, we can determine recursively the', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='entire output (y[0]), (y[1]), (y[2]), (y[3]), (\\\\ldots), one value at a time. For instance, to find (y[0]) we set (n=0) in Eq. (3.17). The left-hand side is (y[0]), and the right-hand side is expressed in terms of (N) initial conditions (y[-1]), (y[-2]), (\\\\ldots), (y[-N]) and the input (x[0]) if (x[n]) is causal (because of causality, other input terms (x[-n]=0)). Similarly, knowing (y[0]) and the input, we can compute (y[1]) by setting (n=1) in Eq. (3.17). Knowing (y[0]) and (y[1]), we find', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='(y[2]), and so on. Thus, we can use this recursive procedure to find the complete response (y[0]), (y[1]), (y[2]), (\\\\ldots). For this reason, this equation is classed as a recursive form. This method basically reflects the manner in which a computer would solve a recursive difference equation, given the input and initial conditions. Equation (3.17) [or Eq. (3.16)] is nonrecursive if all the (N-1) coefficients (a_{i}=0) ((i=1,2,\\\\ldots,N-1)). In this case, it can be seen that (y[n]) is computed', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='only from the input values and without using any previous outputs. Generally speaking, the recursive procedure applies only to equations in the recursive form. The recursive (iterative) procedure is demonstrated by the following examples.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Example 3.11: Iterative Solution to a First-Order Difference Equation\\n\\nSolve iteratively\\n\\n[y[n]-0.5y[n-1]=x[n]]\\n\\nwith initial condition (y[-1]=16) and causal input (x[n]=n^{2}u[n]). This equation can be expressed as', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[y[n]=0.5y[n-1]+x[n] \\\\tag{3.18}]If we set (n=0) in Eq. (3.18), we obtain [y[0] =0.5y[-1]+x[0]] [=0.5(16)+0=8] Now, setting (n=1) in Eq. (3.18) and using the value (y[0]=8) (computed in the first step) and (x[1]=(1)^{2}=1), we obtain [y[1]=0.5(8)+(1)^{2}=5] Next, setting (n=2) in Eq. (3.18) and using the value (y[1]=5) (computed in the previous step) and (x[2]=(2)^{2}), we obtain [y[2]=0.5(5)+(2)^{2}=6.5] Continuing in this way iteratively, we obtain [y[3] =0.5(6.5)+(3)^{2}=12.25] [y[4]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='=0.5(12.25)+(4)^{2}=22.125] [\\\\vdots] The output (y[n]) is depicted in Fig. 3.17.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='We now present one more example of iterative solution--this time for a second-order equation. The iterative method can be applied to a difference equation in delay form or advance form. In Ex. 3.11 we considered the former. Let us now apply the iterative method to the advance form.\\n\\nEXAMPLE3.12 Iterative Solution to a Second-Order Difference Equation', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Solve iteratively [y[n+2]-y[n+1]+0.24y[n]=x[n+2]-2x[n+1]]with initial conditions (y[-1]=2), (y[-2]=1) and a causal input (x[n]=nu[n]). The system equation can be expressed as\\n\\n[y[n+2]=y[n+1]-0.24y[n]+x[n+2]-2x[n+1] \\\\tag{3.19}]\\n\\nSetting (n=-2) in Eq. (3.19) and then substituting (y[-1]=2), (y[-2]=1), (x[0]=x[-1]=0), we obtain\\n\\n[y[0]=2-0.24(1)+0-0=1.76]\\n\\nSetting (n=-1) in Eq. (3.19) and then substituting (y[0]=1.76), (y[-1]=2), (x[1]=1), (x[0]=0), we obtain\\n\\n[y[1]=1.76-0.24(2)+1-0=2.28]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Setting (n=0) in Eq. (3.19) and then substituting (y[0]=1.76), (y[1]=2.28), (x[2]=2), and (x[1]=1) yield\\n\\n[y[2]=2.28-0.24(1.76)+2-2(1)=1.8576]\\n\\nand so on.\\n\\nWith MATLAB, we can readily verify and extend these recursive calculations.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='```\\n\\nn=-2:5;y=[1,2,zeros(1,length(n)-2)];x=[0,0,n(3:end)]; >>fork=1:length(n)-2, >>y(k+2)=y(k+1)-0.24y(k)+x(k+2)-2x(k+1); >>end >>n,y n=-2-101233455y=1.000020.0001.76002.28001.85760.3104-2.1354-5.2099', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='```', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Note carefully the recursive nature of the computations. From the (N) initial conditions (and the input), we obtained (y[0]) first. Then, using this value of (y[0]) and the preceding (N-1) initial conditions (along with the input), we find (y[1]). Next, using (y[0]), (y[1]) along with the past (N-2) initial conditions and input, we obtained (y[2]), and so on. This method is general and can be applied to a recursive difference equation of any order. It is interesting that the hardware', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='realization of Eq. (3.18) depicted in Fig. 3.14 (with (a=0.5)) generates the solution precisely in this (iterative) fashion.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='```', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='```\\n\\nDRILL 3.10Iterative Solution to a Difference Equation\\n\\nUsing the iterative method, find the first three terms of (y[n]) for\\n\\n[y[n+1]-2y[n]=x[n]]\\n\\nThe initial condition is (y[-1]=10) and the input (x[n]=2) starting at (n=0).\\n\\nANSWER', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='(y[0]=20), (y[1]=42), and (y[2]=86)We shall see in the future that the solution of a difference equation obtained in this direct (iterative) way is useful in many situations. Despite the many uses of this method, a closed-form solution of a difference equation is far more useful in the study of system behavior and its dependence on the input and various system parameters. For this reason we shall develop a systematic procedure to analyze discrete-time systems along lines similar to those used', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='for continuous-time systems.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Operator Notation\\n\\nIn difference equations, it is convenient to use operator notation similar to that used in differential equations for the sake of compactness. In continuous-time systems, we used the operator (D) to denote the operation of differentiation. For discrete-time systems, we shall use the operator (E) to denote the operation for advancing a sequence by one time unit. Thus,\\n\\n[Ex[n] \\\\equiv x[n+1]] [E^{2}x[n] \\\\equiv x[n+2]] [\\\\vdots] [E^{N}x[n] \\\\equiv x[n+N]]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Let us use this advance operator notation to represent several systems investigated earlier. The first-order difference equation of a savings account is [see Eq. (3.4)]\\n\\n[y[n+1]-ay[n]=x[n+1]]\\n\\nUsing the operator notation, we can express this equation as\\n\\n[Ey[n]-ay[n]=Ex[n]\\\\qquad\\\\text{or}\\\\qquad(E-a)y[n]=Ex[n]]\\n\\nSimilarly, the second-order book sales estimate described by Eq. (3.6) as\\n\\n[y[n+2]+\\\\tfrac{1}{4}y[n+1]+\\\\tfrac{1}{16}y[n]=x[n+2]]\\n\\ncan be expressed in operator notation as', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[\\\\big{(}E^{2}+\\\\tfrac{1}{4}E+\\\\tfrac{1}{16}\\\\big{)}y[n]=E^{2}x[n]]\\n\\nThe general (N)th-order advance-form difference equation of Eq. (3.15) can be expressed as\\n\\n[(E^{N}+a_{1}E^{N-1}+\\\\cdot\\\\cdot\\\\cdot+a_{N-1}E+a_{N})y[n]=(b_{0}E^{N}+b_{1}E^{N- 1}+\\\\cdot\\\\cdot\\\\cdot+b_{N-1}E+b_{N})x[n]]\\n\\nor\\n\\n[Q[E]y[n]=P[E]x[n] \\\\tag{3.20}]\\n\\nwhere (Q[E]) and (P[E]) are (N)th-order polynomial operators\\n\\n[Q[E] =E^{N}+a_{1}E^{N-1}+\\\\cdot\\\\cdot\\\\cdot+a_{N-1}E+a_{N}] [P[E] =b_{0}E^{N}+b_{1}E^{N-1}+\\\\cdot\\\\cdot\\\\cdot+b_{N-1}E+b_{N}]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='3.6 System Response to Internal Conditions:\\n\\nThe Zero-Input Response\\n\\nThe zero-input response (y_{0}[n]) is the solution of Eq. (3.20) with (x[n]=0); that is,\\n\\n[Q[E]y_{0}[n]=0]\\n\\nor\\n\\n[(E^{N}+a_{1}E^{N-1}+\\\\cdot\\\\cdot\\\\cdot+a_{N-1}E+a_{N})y_{0}[n]=0 \\\\tag{3.21}]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Although we can solve this equation systematically, even a cursory examination points to the solution. This equation states that a linear combination of (y_{0}[n]) and advanced (y_{0}[n]) is zero, not for some values of (n), but for all (n). Such a situation is possible if and only if (y{0}[n]) and advanced (y{0}[n]) have the same form. Only an exponential function (\\\\gamma^{n}) has this property, as the following equation indicates:\\n\\n[E^{k}{\\\\gamma^{n}}=\\\\gamma^{n+k}=\\\\gamma^{k}\\\\gamma^{n}]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='This expression shows that (\\\\gamma^{n}) advanced by (k) units is a constant ((\\\\gamma^{k})) times (\\\\gamma^{n}). Therefore, the solution of Eq. (3.21) must be of the form1\\n\\nFootnote 1: A signal of the form (n^{m}\\\\gamma^{n}) also satisfies this requirement under certain conditions (repeated roots), discussed later.\\n\\n[y_{0}[n]=c\\\\gamma^{n} \\\\tag{3.22}]\\n\\nTo determine (c) and (\\\\gamma), we substitute this solution in Eq. (3.21). Since (E^{k}y_{0}[n]=y_{0}[n+k]=c\\\\gamma^{n+k}), this produces', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[c(\\\\gamma^{N}+a_{1}\\\\gamma^{N-1}+\\\\cdot\\\\cdot\\\\cdot+a_{N-1}\\\\gamma+a_{N})\\\\,\\\\gamma^ {n}=0]\\n\\nFor a nontrivial solution of this equation,\\n\\n[\\\\gamma^{N}+a_{1}\\\\gamma^{n-1}+\\\\cdot\\\\cdot\\\\cdot+a_{N-1}\\\\gamma+a_{N}=0 \\\\tag{3.23}]\\n\\nor\\n\\n[Q[\\\\gamma]=0]\\n\\nOur solution (c\\\\gamma^{n}) [Eq. (3.22)] is correct, provided (\\\\gamma) satisfies Eq. (3.23). Now, (Q[\\\\gamma]) is an (N)th-order polynomial and can be expressed in the factored form (assuming all distinct roots):', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[(\\\\gamma-\\\\gamma_{1})(\\\\gamma-\\\\gamma_{2})\\\\cdot\\\\cdot\\\\cdot(\\\\gamma-\\\\gamma_{N})=0]\\n\\nClearly, (\\\\gamma) has (N) solutions (\\\\gamma_{1}), (\\\\gamma_{2}), (\\\\ldots), (\\\\gamma_{N}) and, therefore, Eq. (3.21) also has (N) solutions (c_{1}\\\\gamma_{1}^{n}), (c_{2}\\\\gamma_{2}^{n}), (\\\\ldots), (c_{n}\\\\gamma_{N}^{n}). In such a case, we have shown that the general solution is a linear combination\\n\\n[MISSING_PAGE_FAIL:35]\\n\\nTherefore,\\n\\n[y_{0}[n]=\\\\tfrac{1}{5}(-0.2)^{n}+\\\\tfrac{4}{5}(0.8)^{n}\\\\qquad n\\\\geq 0]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='The reader can verify this solution by computing the first few terms using the iterative method (see Exs. 3.11 and 3.12).\\n\\n3.11 Zero-Input Response of First-Order Systems\\n\\nFind and sketch the zero-input response for the systems described by the following equations:\\n\\n(y[n+1]-0.8y[n]=3x[n+1])\\n\\n(y[n+1]+0.8y[n]=3x[n+1])\\n\\nIn each case the initial condition is (y[-1]=10). Verify the solutions by computing the first three terms using the iterative method.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='3.12 Zero-Input Response of a Second-Order System with Real Roots\\n\\nFind the zero-input response of a system described by the equation\\n\\n[y[n]+0.3y[n-1]-0.1y[n-2]=x[n]+2x[n-1]]\\n\\nThe initial conditions are (y_{0}[-1]=1) and (y_{0}[-2]=33). Verify the solution by computing the first three terms iteratively.\\n\\n3.13 Verify the Solution by Computing the First-Order System with Real Roots\\n\\nThe second-order system with real roots is\\n\\n[y_{0}[n]=(0.2)^{n}+2(-0.5)^{n}]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Section 3.5-1 introduced the method of recursion to solve difference equations. As the next example illustrates, the zero-input response can likewise be found through recursion. Since it does not provide a closed-form solution, recursion is generally not the preferred method of solving difference equations.\\n\\nExample 3.14: Iterative Solution to Zero-Input Response', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Using the initial conditions (y[-1]=2) and (y[-2]=1), use MATLAB to iteratively compute and then plot the zero-input response for the system described by ((E^{2}-1.56E+0.81)y[n]=(E+3)x[n]).', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content=\"```\\n\\nn=(-2:20)';y=[1;2;zeros(length(n)-2,1)]; >>fork=1:length(n)-2, >>y(k+2)=1.56y(k+1)-0.81y(k); >>end; >>clf;stem(n,y,'k');xlabel('n');ylabel('y[n]');axis([-220-1.52.5]);\", metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='```\\n\\nRepeated Roots', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='So far we have assumed the system to have (N) distinct characteristic roots (\\\\gamma_{1}), (\\\\gamma_{2}), (\\\\ldots), (\\\\gamma_{N}) with corresponding characteristic modes (\\\\gamma_{1}^{n}), (\\\\gamma_{2}^{n}), (\\\\ldots), (\\\\gamma_{N}^{n}). If two or more roots coincide (repeated roots), the form of characteristic modes is modified. Direct substitution shows that if a root (\\\\gamma) repeats (r) times (root of multiplicity (r)), the corresponding characteristic modes for this root are (\\\\gamma^{n}),', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='(ny^{n}), (n^{2}\\\\gamma^{n}), (\\\\ldots), (n^{r-1}\\\\gamma^{n}). Thus, if the characteristic equation of a system is', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[Q[\\\\gamma]=(\\\\gamma-\\\\gamma_{1})^{r}(\\\\gamma-\\\\gamma_{r+1})(\\\\gamma-\\\\gamma_{r+2}) \\\\cdot\\\\cdot\\\\cdot(\\\\gamma-\\\\gamma_{N})]\\n\\nthen the zero-input response of the system is\\n\\n[y_{0}[n]=(c_{1}+c_{2}n+c_{3}n^{2}+\\\\cdot\\\\cdot\\\\cdot+c_{r}n^{r-1})\\\\gamma_{1}^{n}+ c_{r+1}\\\\gamma_{r+1}^{n}+c_{r+2}\\\\gamma_{r+2}^{n}+\\\\cdot\\\\cdot\\\\cdot+c_{n}\\\\gamma_{N}^{n}]\\n\\nFigure 3.18: Zero-input response for Ex. 3.14.\\n\\nExample 3.15: Zero-Input Response of a Second-Order System with Repeated Roots', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Consider a second-order difference equation with repeated roots:\\n\\n[(E^{2}+6E+9)y[n]=(2E^{2}+6E)x[n]]\\n\\nDetermine the zero-input response (y_{0}[n]) if the initial conditions are (y_{0}[-1]=-1/3) and (y_{0}[-2]=-2/9).\\n\\nThe characteristic polynomial is (\\\\gamma^{2}+6\\\\gamma+9=(\\\\gamma+3)^{2}), and we have a repeated characteristic root at (\\\\gamma=-3). The characteristic modes are ((-3)^{n}) and (n(-3)^{n}). Hence, the zero-input response is\\n\\n[y_{0}[n]=(c_{1}+c_{2}n)(-3)^{n}]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Although we can determine the constants (c_{1}) and (c_{2}) from the initial conditions following a procedure similar to Ex. 3.13, we instead use MATLAB to perform the needed calculations.\\n\\nc = inv([(-3)^(-1) -1(-3)^(-1);(-3)^(-2) -2(-3)^(-2)])*[-1/3;-2/9] c = 4  3 Thus, the zero-input response is\\n\\n[y_{0}[n]=(4+3n)(-3)^{n}\\\\qquad n\\\\geq 0]\\n\\n3.3 Complex Roots', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='As in the case of continuous-time systems, the complex roots of a discrete-time system will occur in pairs of conjugates if the system equation coefficients are real. Complex roots can be treated exactly as we would treat real roots. However, just as in the case of continuous-time systems, we can also use the real form of solution as an alternative.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='First we express the complex conjugate roots (\\\\gamma) and (\\\\gamma^{*}) in polar form. If (|\\\\gamma|) is the magnitude and (\\\\beta) is the angle of (\\\\gamma), then\\n\\n[\\\\gamma=|\\\\gamma\\\\,|e^{i\\\\beta}\\\\qquad\\\\mbox{and}\\\\qquad\\\\gamma^{*}=|\\\\gamma|e^{-j\\\\beta}]\\n\\nThe zero-input response is given by\\n\\n[y_{0}[n]=c_{1}\\\\gamma^{n}+c_{2}(\\\\gamma^{*})^{n}=c_{1}|\\\\gamma^{|n}e^{i\\\\beta n}+ c_{2}|\\\\gamma^{|n}e^{-j\\\\beta n}]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='For a real system, (c_{1}) and (c_{2}) must be conjugates so that (y_{0}[n]) is a real function of (n). Let\\n\\n[c_{1}=\\\\frac{c}{2}e^{j\\\\theta}\\\\qquad\\\\mbox{and}\\\\qquad c_{2}=\\\\frac{c}{2}e^{-j\\\\theta}]\\n\\nSystem Response to Internal Conditions: The Zero-Input Response\\n\\nThen\\n\\n[y_{0}[n]=\\\\frac{c}{2}|\\\\gamma|^{n}\\\\left[e^{i(\\\\beta n+\\\\theta)}+e^{-i(\\\\beta n+\\\\theta)} \\\\right]=c|\\\\gamma|^{n}\\\\cos\\\\left(\\\\beta n+\\\\theta\\\\right) \\\\tag{3.25}]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='where (c) and (\\\\theta) are arbitrary constants determined from the auxiliary conditions. This is the solution in real form, which avoids dealing with complex numbers.\\n\\nExample 3.16: Zero-Input Response of a Second-Order System with Complex Roots\\n\\nConsider a second-order difference equation with complex-conjugate roots:\\n\\n[(E^{2}-1.56E+0.81)y[n]=(E+3)x[n]]\\n\\nDetermine the zero-input response (y_{0}[n]) if the initial conditions are (y_{0}[-1]=2) and (y_{0}[-2]=1).', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='The characteristic polynomial is ((\\\\gamma^{2}-1.56\\\\gamma+0.81)=(\\\\gamma-0.78-j0.45)(\\\\gamma-0.78+j0.45)). The characteristic roots are (0.78\\\\pm j0.45); that is, (0.9e^{\\\\pm j(\\\\pi/6)}). We could immediately write the solution as\\n\\n[y_{0}[n]=c(0.9)^{n}e^{j\\\\pi n/6}+c^{*}(0.9)^{n}e^{-j\\\\pi n/6}]\\n\\nSetting (n=-1) and (-2) and using the initial conditions (y_{0}[-1]=2) and (y_{0}[-2]=1), we find (c=1.1550-j0.2025=1.1726\\\\,e^{-j0.1735}) and (c^{*}=1.1550+j0.2025=1.1726\\\\,e^{j0.1735}).', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='gamma = roots([1 -1.56 0.81]); >> c = inv([gamma(1)^-(1) gamma(2)^(-1);gamma(1)^(-2) gamma(2)^(-2)])*[2;1] c = 1.1550 - 0.2025i  1.1550 + 0.2025i\\n\\nAlternately, we could also find the unknown coefficient by using the real form of the solution, as given in Eq. (3.25). In the present case, the roots are (0.9e^{\\\\pm j(\\\\pi/6)}). Hence, (|\\\\gamma|=0.9) and (\\\\beta=\\\\pi/6), and the zero-input response, according to Eq. (3.25), is given by\\n\\n[y_{0}[n]=c(0.9)^{n}\\\\cos\\\\left(\\\\frac{\\\\pi}{6}n+\\\\theta\\\\right)]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='To determine the constants (c) and (\\\\theta), we set (n=-1) and (-2) in this equation and substitute the initial conditions (y_{0}[-1]=2) and (y_{0}[-2]=1) to obtain', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[2 =\\\\frac{c}{0.9}\\\\cos\\\\left(-\\\\frac{\\\\pi}{6}+\\\\theta\\\\right)=\\\\frac{c}{0.9 }\\\\left[\\\\frac{\\\\sqrt{3}}{2}\\\\cos\\\\theta+\\\\frac{1}{2}\\\\sin\\\\theta\\\\right]] [1 =\\\\frac{c}{(0.9)^{2}}\\\\cos\\\\left(-\\\\frac{\\\\pi}{3}+\\\\theta\\\\right)=\\\\frac{ c}{0.81}\\\\left[\\\\frac{1}{2}\\\\cos\\\\theta+\\\\frac{\\\\sqrt{3}}{2}\\\\sin\\\\theta\\\\right]][\\\\frac{\\\\sqrt{3}}{1.8}\\\\,c\\\\cos\\\\theta+\\\\frac{1}{1.8}\\\\,c\\\\sin\\\\theta=2] [\\\\frac{1}{1.62}\\\\,c\\\\cos\\\\theta+\\\\frac{\\\\sqrt{3}}{1.62}\\\\,c\\\\sin\\\\theta=1] These are two simultaneous equations in two unknowns (c\\\\cos\\\\theta) and (c\\\\sin\\\\theta).', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Solution of these equations yields [c\\\\cos\\\\theta=2.308] [c\\\\sin\\\\theta=-0.397] Dividing (c\\\\sin\\\\theta) by (c\\\\cos\\\\theta) yields [\\\\tan\\\\theta=\\\\frac{-0.397}{2.308}=\\\\frac{-0.172}{1}] [\\\\theta=\\\\tan^{-1}(-0.172)=-0.17\\\\text{ rad}] Substituting (\\\\theta=-0.17) radian in (c\\\\cos\\\\theta=2.308) yields (c=2.34) and [y_{0}[n]=2.34(0.9)^{n}\\\\cos\\\\left(\\\\frac{\\\\pi}{6}n-0.17\\\\right)\\\\quad n\\\\geq 0] Observe that here we have used radian units for both (\\\\beta) and (\\\\theta). We also could have used the degree unit, although this', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='practice is not recommended. The important consideration is to be consistent and to use the same units for both (\\\\beta) and (\\\\theta).', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='3.13 Zero-Input Response of a Second-Order System with Complex Roots\\n\\nFind the zero-input response of a system described by the equation\\n\\n[y[n]+4y[n-2]=2x[n]]\\n\\nThe initial conditions are (y_{0}[-1]=-1/(2\\\\sqrt{2})) and (y_{0}[-2]=1/(4\\\\sqrt{2})). Verify the solution by computing the first three terms iteratively.\\n\\n3.14 Answer\\n\\nThe Unit Impulse Response (h[n])\\n\\nConsider an (n)th-order system specified by the equation', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[(E^{N}+a_{1}E^{N-1}+\\\\cdot\\\\cdot\\\\cdot+a_{N-1}E+a_{N})y[n]=(b_{0}E^{N}+b_{1}E^{N-1}+ \\\\cdot\\\\cdot\\\\cdot+b_{N-1}E+b_{N})x[n]]\\n\\nor\\n\\n[Q[E]y[n]=P[E]x[n]]\\n\\nThe unit impulse response (h[n]) is the solution of this equation for the input (\\\\delta[n]) with all the initial conditions zero; that is,\\n\\n[Q[E]h[n]=P[E]\\\\delta[n] \\\\tag{3.26}]\\n\\nsubject to initial conditions\\n\\n[h[-1]=h[-2]=\\\\cdot\\\\cdot\\\\cdot=h[-N]=0]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content=\"Equation (3.26) can be solved to determine (h[n]) iteratively or in a closed form. The following example demonstrates the iterative solution.\\n\\nIterative Determination of the Impulse Response\\n\\nIteratively compute the first two values of the impulse response (h[n]) of a system described by the equation\\n\\n[y[n]-0.6y[n-1]-0.16y[n-2]=5x[n]]\\n\\nTo determine the unit impulse response, we let the input (x[n]=\\\\delta[n]) and the output (y[n]=h[n]) in the system's difference equation to obtain\", metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[h[n]-0.6h[n-1]-0.16h[n-2]=5\\\\delta[n]]\\n\\nsubject to zero initial state; that is, (h[-1]=h[-2]=0).\\n\\nSetting (n=0) in this equation yields\\n\\n[h[0]-0.6(0)-0.16(0)=5(1)\\\\quad\\\\Longrightarrow\\\\quad h[0]=5]\\n\\nSetting (n=1) in the same equation and using (h[0]=5), we obtain\\n\\n[h[1]-0.6(5)-0.16(0)=5(0)\\\\quad\\\\Longrightarrow\\\\quad h[1]=3]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Continuing this way, we can determine any number of terms of (h[n]). Unfortunately, such a solution does not yield a closed-form expression for (h[n]). Nevertheless, determining a few values of (h[n]) can be useful in determining the closed-form solution, as the following development shows.\\n\\n3.7-1 The Closed-Form Solution of (h[n])', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Recall that (h[n]) is the system response to input (\\\\delta[n]), which is zero for (n>0). We know that when the input is zero, only the characteristic modes can be sustained by the system. Therefore, (h[n]) must be made up of characteristic modes for (n>0). At (n=0), it may have some nonzero value (A_{0}) so that a general form of (h[n]) can be expressed as+', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Footnote †: ({}^{\\\\ddagger}) If (a_{N}=0), then (A_{0}) cannot be determined by Eq. (3.28). In such a case, we show in Sec. 3.12 that (h[n]) is of the form (A_{0}\\\\delta[n]+A_{1}\\\\delta[n-1]+y_{c}[n]u[n]). We have here (N+2) unknowns, which can be determined from (N+2) values (h[0],h[1],\\\\ldots,h[N+1]) found iteratively.\\n\\n[h[n]=A_{0}\\\\delta[n]+y_{c}[n]u[n] \\\\tag{3.27}]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='where (y_{c}[n]) is a linear combination of the characteristic modes. We now substitute Eq. (3.27) in Eq. (3.26) to obtain (Q[E]\\\\left(A_{0}\\\\delta[n]+y_{c}[n]u[n]\\\\right)=P[E]\\\\delta[n]). Because (y_{c}[n]) is made up of characteristic modes, (Q[E]y_{c}[n]u[n]=0), and we obtain (A_{0}Q[E]\\\\delta[n]=P[E]\\\\delta[n]), that is,\\n\\n[A_{0}\\\\left(\\\\delta[n+N]+a_{1}\\\\delta[n+N-1]+\\\\cdot\\\\cdot\\\\cdot+a_{N}\\\\delta[n] \\\\right)=b_{0}\\\\delta[n+N]+\\\\cdot\\\\cdot\\\\cdot+b_{N}\\\\delta[n]]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Setting (n=0) in this equation and using the fact that (\\\\delta[m]=0) for all (m\\\\neq 0), and (\\\\delta[0]=1), we obtain\\n\\n[A_{0}a_{N}=b_{N}\\\\quad\\\\Longrightarrow\\\\quad A_{0}=\\\\frac{b_{N}}{a_{N}} \\\\tag{3.28}]\\n\\nHence,+', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Footnote †: ({}^{\\\\ddagger}) If (a_{N}=0), then (A_{0}) cannot be determined by Eq. (3.28). In such a case, we show in Sec. 3.12 that (h[n]) is of the form (A_{0}\\\\delta[n]+A_{1}\\\\delta[n-1]+y_{c}[n]u[n]). We have here (N+2) unknowns, which can be determined from (N+2) values (h[0],h[1],\\\\ldots,h[N+1]) found iteratively.\\n\\n[h[n]=\\\\frac{b_{N}}{a_{N}}\\\\,\\\\delta[n]+y_{c}[n]u[n] \\\\tag{3.29}]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='The (N) unknown coefficients in (y_{c}[n]) (on the right-hand side) can be determined from a knowledge of (N) values of (h[n]). Fortunately, it is a straightforward task to determine values of (h[n]) iteratively, as demonstrated in Ex. 3.17. We compute (N) values (h[0]), (h[1]), (h[2]), (\\\\ldots), (h[N-1]) iteratively. Now, setting (n=0), (1), (2), (\\\\ldots), (N-1) in Eq. (3.29), we can determine the (N) unknowns in (y_{c}[n]). This point will become clear in the following example.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='3.1 Closed-Form Determination of the Impulse Response\\n\\nDetermine the unit impulse response (h[n]) for a system in Ex. 3.17 specified by the equation\\n\\n[y[n]-0.6y[n-1]-0.16y[n-2]=5x[n]]This equation can be expressed in the advance form as\\n\\n[y[n+2]-0.6y[n+1]-0.16y[n]=5x[n+2]]\\n\\nor in advance operator form as\\n\\n[(E^{2}-0.6E-0.16)y[n]=5E^{2}x[n]]\\n\\nThe characteristic polynomial is\\n\\n[\\\\gamma^{2}-0.6\\\\gamma-0.16=(\\\\gamma+0.2)(\\\\gamma-0.8)]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='The characteristic modes are ((-0.2)^{n}) and ((0.8)^{n}). Therefore,\\n\\n[y_{c}[n]=c_{1}(-0.2)^{n}+c_{2}(0.8)^{n}]\\n\\nInspecting the system difference equation, we see that (a_{N}=-0.16) and (b_{N}=0). Therefore, according to Eq. (3.29),\\n\\n[h[n]=[c_{1}(-0.2)^{n}+c_{2}(0.8)^{n}]u[n]]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='To determine (c_{1}) and (c_{2}), we need to find two values of (h[n]) iteratively. From Ex. 3.17, we know that (h[0]=5) and (h[1]=3). Setting (n=0) and (1) in our expression for (h[n]) and using the fact that (h[0]=5) and (h[1]=3), we obtain\\n\\n[\\\\left.\\\\begin{array}{l}5=c_{1}+c_{2}\\\\ 3=-0.2c_{1}+0.8c_{2}\\\\end{array}\\\\right}\\\\quad\\\\Longrightarrow\\\\quad\\\\begin{array} []{l}c_{1}=1\\\\ c_{2}=4\\\\end{array}]\\n\\nTherefore,\\n\\n[h[n]=\\\\left[(-0.2)^{n}+4(0.8)^{n}\\\\right]u[n]]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Drill 3.14 Closed-Form Determination of the Impulse Response\\n\\nFind (h[n]), the unit impulse response of the LTID systems specified by the following equations:\\n\\n(y[n+1]-y[n]=x[n])\\n\\n(y[n]-5y[n-1]+6y[n-2]=8x[n-1]-19x[n-2])\\n\\n(y[n+2]-4y[n+1]+4y[n]=2x[n+2]-2x[n+1])\\n\\n(y[n]=2x[n]-2x[n-1])\\n\\nAnswers\\n\\n(h[n]=u[n-1])\\n\\n(h[n]=-\\\\frac{19}{6}\\\\delta[n]+\\\\left[\\\\frac{3}{2}(2)^{n}+\\\\frac{5}{3}(3)^{n}\\\\right] u[n])\\n\\n(h[n]=(2+n)2^{n}u[n])\\n\\n(h[n]=2\\\\delta[n]-2\\\\delta[n-1])', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Example 3.19: Filtering Perspective of the Unit Impulse Response\\n\\nUse the MATLAB filter command to solve Ex. 3.18.\\n\\nThere are several ways to find the impulse response using MATLAB. In this method, we first specify the unit impulse function, which will serve as our input. Vectors a and b are created to specify the system. The filter command is then used to determine the impulse response. In fact, this method can be used to determine the zero-state response for any input.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content=\"n = (0:19); delta = @(n) 1.0.*(n==0); >> a = [1 -0.6 -0.16]; b = [5 0 0]; >> h = filter(b,a,delta(n)); >> clf; stem(n,h,'k'); xlabel('n'); ylabel('h[n]');\\n\\nComment. Although it is relatively simple to determine the impulse response (h[n]) by using the procedure in this section, in Ch. 5 we shall discuss the much simpler method of the (z)-transform.\\n\\n3.8 System Response to External Input:\\n\\nThe Zero-State Response\", metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='The zero-state response (y[n]) is the system response to an input (x[n]) when the system is in the zero state. In this section we shall assume that systems are in the zero state unless mentioned otherwise, so that the zero-state response will be the total response of the system. Here we follow the procedure parallel to that used in the continuous-time case by expressing an arbitrary input (x[n]) as a sum of impulse components. A signal (x[n]) in Fig. 3.20a can be expressed as a sum of impulse', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='components, such as those depicted in Figs. 3.20b-3.20f. The component of (x[n]) at (n=m) is (x[m]\\\\delta[n-m]), and (x[n]) is the sum of all these components summed from (m=-\\\\infty) to (\\\\infty)', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Figure 3.19: Impulse response for Ex. 3.19Therefore,\\n\\n[x[n] = x[0]\\\\delta[n]+x[1]\\\\delta[n-1]+x[2]\\\\delta[n-2]+\\\\cdot\\\\cdot\\\\cdot \\\\tag{3.30}] [+x[-1]\\\\delta[n+1]+x[-2]\\\\delta[n+2]+\\\\cdot\\\\cdot\\\\cdot] [= \\\\sum_{m=-\\\\infty}^{\\\\infty}x[m]\\\\delta[n-m]]\\n\\nFigure 3.20: Representation of an arbitrary signal (x[n]) in terms of impulse components.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='For a linear system, if we know the system response to impulse (\\\\delta[n]), we can obtain the system response to any arbitrary input by summing the system response to various impulse components. Let (h[n]) be the system response to impulse input (\\\\delta[n]). We shall use the notation\\n\\n[x[n]\\\\Longrightarrow y[n]]\\n\\nto indicate the input and the corresponding response of the system. Thus, if\\n\\n[\\\\delta[n]\\\\Longrightarrow h[n]]\\n\\nthen because of time invariance\\n\\n[\\\\delta[n-m]\\\\Longrightarrow h[n-m]]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='and because of linearity\\n\\n[x[m]\\\\delta[n-m]\\\\Longrightarrow x[m]h[n-m]]\\n\\nand again because of linearity\\n\\n[\\\\underbrace{\\\\sum_{m=-\\\\infty}^{\\\\infty}x[m]\\\\delta[n-m]}{x[n]}\\\\quad \\\\Longrightarrow\\\\quad\\\\underbrace{\\\\sum{m=-\\\\infty}^{\\\\infty}x[m]h[n-m]}_{y[n]}]\\n\\nThe left-hand side is (x[n]) [see Eq. (3.30)], and the right-hand side is the system response (y[n]) to input (x[n]). Therefore,+', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Footnote †: margin: ({}^{\\\\dagger}) In deriving this result, we have assumed a time-invariant system. The system response to input (\\\\delta[n-m]) for a time-varying system cannot be expressed as (h[n-m]); instead, it has the form (h[n,m]). Using this form, Eq. (3.31) is modified as follows:\\n\\n[y[n]=\\\\sum_{m=-\\\\infty}^{\\\\infty}x[m]h[n,m]]\\n\\nThe summation on the right-hand side is known as the convolution sum of (x[n]) and (h[n]), and is represented symbolically by (x[n]*h[n])', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[x[n]*h[n]=\\\\sum_{m=-\\\\infty}^{\\\\infty}x[m]h[n-m]]\\n\\nProperties of the Convolution Sum\\n\\nThe structure of the convolution sum is similar to that of the convolution integral. Moreover, the properties of the convolution sum are similar to those of the convolution integral. We shall enumerate these properties here without proof. The proofs are similar to those for the convolution integral and may be derived by the reader.\\n\\n[MISSING_PAGE_FAIL:47]\\n\\nExample 3.20: Convolution of Causal Signals', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Determine (c[n]=x[n]*g[n]) for\\n\\n[x[n]=(0.8)^{n}u[n]\\\\qquad\\\\mbox{and}\\\\qquad g[n]=(0.3)^{n}u[n]]\\n\\nWe have\\n\\n[c[n]=\\\\sum_{m=-\\\\infty}^{\\\\infty}x[m]g[n-m]]\\n\\nNote that\\n\\n[x[m]=(0.8)^{m}u[m]\\\\qquad\\\\mbox{and}\\\\qquad g[n-m]=(0.3)^{n-m}u[n-m]]\\n\\nBoth (x[n]) and (g[n]) are causal. Therefore [see Eq. (3.33)],\\n\\n[c[n]=\\\\sum_{m=0}^{n}x[m]g[n-m]=\\\\sum_{m=0}^{n}(0.8)^{m}u[m]\\\\,(0.3)^{n-m}u[n-m]]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='In this summation, (m) lies between (0) and (n) ((0\\\\leq m\\\\leq n)). Therefore, if (n\\\\geq 0), then both (m) and (n-m\\\\geq 0) so that (u[m]=u[n-m]=1). If (n<0), (m) is negative because (m) lies between (0) and (n), and (u[m]=0). Therefore,\\n\\n[c[n]=\\\\left{\\\\begin{array}{cc}\\\\sum_{m=0}^{n}(0.8)^{m}\\\\,(0.3)^{n-m}&n\\\\geq 0 \\\\ 0&n<0\\\\end{array}\\\\right.]\\n\\nor\\n\\n[c[n]=(0.3)^{n}\\\\sum_{m=0}^{n}\\\\left(\\\\frac{0.8}{0.3}\\\\right)^{m}u[n]]\\n\\nThis is a geometric progression with common ratio ((0.8/0.3)). From Sec. B.8-3 we have', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[c[n] =(0.3)^{n}\\\\frac{(0.8)^{n+1}-(0.3)^{n+1}}{(0.3)^{n}(0.8-0.3)}u[n]] [=2[(0.8)^{n+1}-(0.3)^{n+1}]u[n]]\\n\\nDrill 3.15: Convolution of Causal Signals\\n\\nShow that ((0.8)^{n}u[n]*u[n]=5[1-(0.8)^{n+1}]u[n]).\\n\\n[MISSING_PAGE_EMPTY:49]\\n\\nExample 3.21: Convolution by Tables\\n\\nUsing Table 3.1, find the (zero-state) response (y[n]) of an LTID system described by the equation\\n\\n[y[n+2]-0.6y[n+1]-0.16y[n]=5x[n+2]]\\n\\nif the input (x[n]=4^{-n}u[n]).', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='The input can be expressed as (x[n]=4^{-n}u[n]=(1/4)^{n}u[n]=(0.25)^{n}u[n]). The unit impulse response of this system, obtained in Ex. 3.18, is\\n\\n[h[n]=[(-0.2)^{n}+4(0.8)^{n}]u[n]]\\n\\nTherefore,\\n\\n[y[n] =x[n]h[n]] [=(0.25)^{n}u[n]\\\\left[(-0.2)^{n}u[n]+4(0.8)^{n}u[n]\\\\right]] [=(0.25)^{n}u[n](-0.2)^{n}u[n]+(0.25)^{n}u[n]4(0.8)^{n}u[n]]\\n\\nWe use pair 4 (Table 3.1) to find the foregoing convolution sums.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[y[n] =\\\\left[\\\\frac{(0.25)^{n+1}-(-0.2)^{n+1}}{0.25-(-0.2)}+4\\\\frac{(0.25 )^{n+1}-(0.8)^{n+1}}{0.25-0.8}\\\\right]u[n]] [=(2.22[(0.25)^{n+1}-(-0.2)^{n+1}]-7.27[(0.25)^{n+1}-(0.8)^{n+1}])u [n]] [=[-5.05(0.25)^{n+1}-2.22(-0.2)^{n+1}+7.27(0.8)^{n+1}]u[n]]\\n\\nRecognizing that\\n\\n[\\\\gamma^{n+1}=\\\\gamma\\\\left(\\\\gamma\\\\right)^{n}]\\n\\nwe can express (y[n]) as\\n\\n[y[n] =[-1.26(0.25)^{n}+0.444(-0.2)^{n}+5.81(0.8)^{n}]u[n]] [=[-1.26(4)^{-n}+0.444(-0.2)^{n}+5.81(0.8)^{n}]u[n]]\\n\\nDRILL 3.16: Convolution by Tables', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Use Table 3.1 to show that\\n\\n(a): ((0.8)^{n+1}u[n]u[n]=4[1-0.8(0.8)^{n}]u[n])\\n(b): (n3^{-n}u[n](0.2)^{n}u[n]=\\\\frac{15}{4}\\\\left[(0.2)^{n}-\\\\left(1-\\\\frac{2}{3}n \\\\right)3^{-n}\\\\right]u[n])\\n(c): (e^{-n}u[n]*2^{-n}u[n]=\\\\frac{2}{2-n}\\\\left[e^{-n}-\\\\frac{c}{2}2^{-n}\\\\right]u[n])\\n\\n3.22 Filtering Perspective of the Zero-State Response\\n\\nUse the MATLAB filter command to compute and sketch the zero-state response for the system described by ((E^{2}+0.5E-1)y[n]=(2E^{2}+6E)x[n]) and the input (x[n]=4^{-n}u[n]).', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content=\"We solve this problem using the same approach as Ex. 3.19. Although the input is bounded and quickly decays to zero, the system itself is unstable and an unbounded output results.\\n\\nn = (0:11); x = @(n) 4.^(-n).*(n>=0); >> a = [1 0.5 -1]; b = [2 6 0]; y = filter(b,a,x(n)); >> clf; stem(n,y,'k'); xlabel('n'); ylabel('y[n]'); axis([-0.5 11.5 -20 25]);\\n\\nResponse to Complex Inputs\", metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='As in the case of real continuous-time systems, we can show that for an LTID system with real (h[n]), if the input and the output are expressed in terms of their real and imaginary parts, then the real part of the input generates the real part of the response and the imaginary part of the input generates the imaginary part. Thus, if\\n\\n[x[n]=x_{r}[n]+jx_{i}[n]\\\\qquad\\\\text{and}\\\\qquad y[n]=y_{r}[n]+jy_{i}[n]]\\n\\nusing the right-directed arrow to indicate the input-output pair, we can show that', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='[x_{r}[n]\\\\Longrightarrow y_{r}[n]\\\\qquad\\\\text{and}\\\\qquad x_{i}[n] \\\\Longrightarrow y_{i}[n] \\\\tag{3.34}]\\n\\nThe proof is similar to that used to derive Eq. (2.31) for LTIC systems.\\n\\n3.2 Multiple Inputs\\n\\nMultiple inputs to LTI systems can be treated by applying the superposition principle. Each input is considered separately, with all other inputs assumed to be zero. The sum of all these individual system responses constitutes the total system output when all the inputs are applied simultaneously.', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " Document(page_content='Figure 3.21: Zero-state response for Ex. 3.22\\n\\n3.8.1 Graphical Procedure for the Convolution Sum\\n\\nThe steps in evaluating the convolution sum are parallel to those followed in evaluating the convolution integral. The convolution sum of causal signals (x[n]) and (g[n]) is given by\\n\\n[c[n]=\\\\sum_{m=0}^{n}x[m]g[n-m]]', metadata={'source': '..\\\\mmd\\\\Chapter_03.mmd'}),\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "187859cd-bb88-488f-a13a-a102e21c7591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_store = Chroma.from_documents(split_docs, embeddings, persist_directory=\"./all_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506ccf2b-52d6-490d-a68a-7159fc8fbfb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
